# KeemenaSubwords.jl - Add training module scaffolding and BPE training first

## Goal
Add tokenizer TRAINING support in a dedicated module layout, without changing pretrained-tokenizer workflows. Implement BPE training first. Add deterministic CI tests that validate end-to-end behavior including offsets contract, save/export, and reload parity.

## Constraints
- Do not refactor unrelated pretrained-tokenizer logic.
- Training code must live under `src/training/` and be namespaced (Julia submodule).
- Training tests must live under `test/training/` and be clearly grouped.
- Offsets contract must remain exactly:
  - UTF-8 codeunits
  - 1-based indices
  - half-open spans [start, stop)
  - sentinel (0,0) for no-span tokens
  - `assume_normalized=true` must skip intrinsic normalization and compute offsets against the provided text

## Tasks (in order)

### 1) Training module entry point and wiring
- [ ] Add `src/training/Training.jl`:
  - Define `module Training ... end`
  - `import ..KeemenaSubwords` symbols needed by trainers (BPETokenizer, build_vocab, TokenizerMetadata, etc.)
  - `include(...)` training source files from inside the module
- [ ] Update `src/KeemenaSubwords.jl`:
  - Replace `include("training/training.jl")` with `include("training/Training.jl")`
  - `using .Training: train_bpe, train_unigram, train_wordpiece, train_sentencepiece`
  - Keep exporting the same top-level names for backward compatibility

### 2) Training scaffolding types and shared helpers
- [ ] Add `src/training/training_types.jl`:
  - `AbstractTrainingConfig`, `AbstractTrainingArtifacts`
  - `TrainingResult{T,C,A}` struct
  - `BPETrainingConfig` struct with fields:
    - vocab_size::Int
    - min_frequency::Int
    - special_tokens::Dict{Symbol,String}
    - pretokenizer::Union{Nothing,Function}
    - end_of_word_marker::String
    - model_name::String
  - `BPETrainingArtifacts` struct with:
    - vocab_tokens::Vector{String}
    - merge_pairs::Vector{Tuple{String,String}} (rank order)
- [ ] Add `src/training/training_common.jl`:
  - normalize special token keys (aliases :UNK => :unk, etc)
  - deterministic ordering for special tokens
  - `_collect_word_counts(corpus; pretokenizer)`
  - shared validation helpers (vocab_size bounds, required tokens present)

### 3) Public training API surface
- [ ] Update `src/training/training_api.jl` (or split into `api.jl`) to expose:
  - `train_bpe(corpus; vocab_size, min_frequency=2, special_tokens=..., pretokenizer=nothing, end_of_word_marker="</w>", model_name="trained_bpe")::BPETokenizer`
  - `train_bpe_result(...)::TrainingResult{BPETokenizer,BPETrainingConfig,BPETrainingArtifacts}`
  - Keep `train_unigram`, `train_wordpiece`, `train_sentencepiece` (stubs or existing implementation), but ensure they live under `KeemenaSubwords.Training`

### 4) BPE training implementation (first real trainer)
- [ ] Add `src/training/bpe_trainer.jl` (or adapt existing `bpe_train.jl`) implementing:
  - Build word counts from corpus
  - Initialize symbols = characters + end-of-word marker `</w>`
  - Iteratively merge most frequent adjacent pair:
    - weighted by word frequency
    - deterministic tie-break for equal counts
    - stop at vocab_size or min_frequency threshold
  - Build vocab with special tokens first, ensure required tokens are present:
    - :unk token must exist
    - end-of-word marker must exist
    - if vocab_size is too small to keep required tokens, throw ArgumentError
  - Build `pair_ranks` based on merge order (rank 1..N)
  - Return BPETokenizer with correct metadata and marker
  - Provide artifacts (vocab_tokens, merge_pairs)

### 5) Tests (deterministic, end-to-end, no network)
- [ ] Create folder `test/training/`
- [ ] Add `test/training/runtests_training.jl` that includes the training test files
- [ ] Add `test/training/test_bpe_training_e2e.jl`:
  - Train BPE on a fixed small corpus
  - Assert `decode(encode(text)) == text` for multiple samples
  - Save tokenizer -> reload -> assert identical tokenize/encode/decode behavior
  - (Optional) Train twice and assert identical artifacts for determinism
- [ ] Add `test/training/test_bpe_training_offsets.jl`:
  - Use the OffsetContract flow:
    - tokenization_text = tokenization_view(tokenizer, clean_text)
    - encode_result(...; assume_normalized=true, return_offsets=true, return_masks=true)
  - Assert `assert_offsets_contract(tokenization_text, offsets; require_string_boundaries=true)`
  - Assert `offsets_are_nonoverlapping(offsets)`
  - Include BOS/EOS as special tokens and assert sentinel (0,0) for inserted BOS/EOS offsets
- [ ] Add `test/training/test_training_stubs.jl`:
  - Assert `train_wordpiece` and `train_sentencepiece` throw ArgumentError with clear messages
- [ ] Update `test/runtests.jl`:
  - Replace `include("training_runtests.jl")` with `include("training/runtests_training.jl")`

## Notes for implementation
- Keep pretrained loading/tokenization untouched.
- Ensure offsets remain 1-based UTF-8 codeunit half-open spans and respect sentinel rules.
- Prefer deterministic ordering everywhere (sorted word iteration, tie-breaking on pairs) to keep CI stable.
