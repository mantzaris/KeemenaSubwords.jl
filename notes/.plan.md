# KeemenaSubwords.jl high-level implementation plan

Context:
- KeemenaSubwords.jl is intended as a downstream companion to KeemenaPreprocessing.jl, providing subword tokenization methods for the Keemena* ecosystem. 
- KeemenaPreprocessing (https://github.com/mantzaris/KeemenaPreprocessing.jl/) accepts tokenizers either as built-in symbols or as a custom callable with the shape `tokenizer(text::AbstractString) -> Vector{<:AbstractString}`.
- When `tokenizer_name` is a callable, KeemenaPreprocessing’s level key (the `Symbol` under which the resulting level is stored) is inferred from `Symbol(typeof(fn))`. This matters for how users retrieve the subword stream from a `PreprocessBundle`.

---

## 1) Scope: goals and non-goals

### Goals
- Provide a clean, Julia-native, extensible subword tokenization toolkit that covers the major families:
  - BPE (classic + GPT-2 style byte-level BPE)
  - WordPiece (BERT-style)
  - Unigram LM (SentencePiece-style)
  - SentencePiece format compatibility (load and run models; both BPE and Unigram)
- Make it easy to:
  - load and use existing trained tokenizers (from common file formats)
  - ship a small set of built-in “core” models with the package (no manual downloads by users)
  - let users supply their own tokenizer model files via paths
- Provide a stable callable interface so KeemenaPreprocessing can use these tokenizers directly without tight coupling (aligning with KeemenaPreprocessing’s callable tokenizer contract). 

### Non-goals (for first releases)
- Full parity with every feature of Hugging Face `tokenizers` (e.g., all normalizers, pre-tokenizers, special decoding tricks).
- Re-implementing “tiktoken” binary formats or every LLM tokenizer variant immediately.
- Advanced features like dropout/sampling tokenization in the first milestone (but design for later extension).

---

## 2) Algorithms to implement (what KeemenaSubwords will provide)

This list is intentionally “major algorithms + compatibility” rather than “everything”.

### A. BPE family
1) **Classic BPE (Sennrich-style)**
- Tokenization: apply merges to a pre-tokenized word stream (or optionally on raw text with a default pretokenizer).
- Model assets: `merges` + `vocab` (or internal unified format).
- Use cases: general subword segmentation.

2) **Byte-level BPE (GPT-2 style)**
- Tokenization: byte-to-unicode mapping, pretokenization, then BPE merges.
- Model assets: `vocab.json` + `merges.txt` (common GPT-2 style) or equivalent.
- Use cases: robust handling of arbitrary bytes / unknown unicode, common LLM tokenizers.

### B. WordPiece family
3) **WordPiece (BERT-style greedy longest-match)**
- Tokenization: whitespace pretokenize -> per-word greedy longest-match with continuation prefix (e.g., "##").
- Model assets: `vocab.txt` (one token per line) plus special tokens and continuation marker rules.
- Use cases: BERT-style models and many downstream pipelines.

(Training for WordPiece can be later; tokenization + compatibility is the priority.)

### C. Unigram LM family (SentencePiece-style)
4) **Unigram LM tokenizer**
- Tokenization: Viterbi/DP segmentation using log-probabilities for tokens.
- Model assets: token -> score/prob + special tokens; optionally byte fallback.
- Use cases: SentencePiece unigram models (common in many transformer families).

5) **SentencePiece compatibility layer**
- SentencePiece is both:
  - a model format (proto)
  - and a family (BPE or Unigram)
- Plan: implement loaders for SentencePiece `.model` and expose them as either:
  - `SentencePieceBPE` (internally using your BPE engine)
  - `SentencePieceUnigram` (internally using your Unigram engine)
- Minimal normalization hooks:
  - support basic whitespace marker behavior (the “▁” style boundary marker)
  - optionally allow an external normalizer callable (advanced normalization can be incremental)

---

## 3) Built-in models and model storage strategy

Requirement: “store within the package the mapping files so users do not have to fetch/supply them manually; ship only core models; allow custom file paths”.

### Recommended approach: Julia Artifacts + small in-repo defaults
- Add `Artifacts.toml` to KeemenaSubwords (currently not present in repo root listing) to manage model files in a reproducible way.
- Ship:
  - at least 1 small-but-realistic model per algorithm family:
    - `:core_bpe_en` (BPE merges+vocab)
    - `:core_wordpiece_en` (WordPiece vocab)
    - `:core_sentencepiece_unigram_en` (SentencePiece unigram `.model` OR internal unigram format)
  - keep these “core” artifacts modest in size to avoid bloating installs
- Allow user-supplied models:
  - `load_tokenizer("/path/to/model_dir")`
  - `load_tokenizer("/path/to/spm.model")`
  - `load_tokenizer((vocab_path, merges_path))` for BPE-style formats

### Model registry API concept
- A small registry mapping `Symbol => (format, artifact_name or relative path, metadata)`.
- Users can do:
  - `available_models()`
  - `describe_model(:core_bpe_en)`
  - `load_tokenizer(:core_bpe_en)`

---

## 4) API surface overview

You want two “faces”:
1) API that KeemenaPreprocessing uses
2) Direct API that KeemenaSubwords users use

### 4.1 Integration surface for KeemenaPreprocessing

KeemenaPreprocessing expects:
- either a built-in symbol (`:whitespace`, `:unicode`, `:byte`, `:char`)
- or a callable `tokenizer(text::AbstractString) -> Vector{<:AbstractString}`.

So KeemenaSubwords should provide tokenizers that are directly callable.

Key integration decision:
- Implement tokenizer types as `struct MyTokenizer <: Function ... end`
- Provide `(tok::MyTokenizer)(text::AbstractString)::Vector{String}`

This ensures:
- `tok isa Function` (robust for config validation)
- `typeof(tok)` is a stable named type, so KeemenaPreprocessing stores the level key as `Symbol(typeof(tok))`. 

#### Integration helper functions (KeemenaSubwords)
- `keemena_callable(tokenizer)::Function`
  - returns a `Function` that matches KeemenaPreprocessing contract
  - mostly a no-op if tokenizer is already `<: Function`, but helpful for wrapping configured tokenizers or pipelines
- `level_key(tokenizer)::Symbol`
  - returns `Symbol(typeof(tokenizer))` so users can retrieve the correct level from `PreprocessBundle`

Example usage pattern (document this clearly):
```julia
using KeemenaPreprocessing
using KeemenaSubwords

tokenizer = load_tokenizer(:core_bpe_en)
cfg = PreprocessConfiguration(tokenizer_name = tokenizer)

bundle = preprocess_corpus(docs; config = cfg)

lvl = level_key(tokenizer)  # e.g. :BPETokenizer
subword_corpus = get_corpus(bundle, lvl)
```

Note:
- This aligns with KeemenaPreprocessing’s “callables” philosophy. 
- You should explicitly document the level key behavior, since KeemenaPreprocessing infers it for callables. 

Optional future integration (keep out of v0.1 to reduce complexity):
- Provide a tiny helper in KeemenaPreprocessing docs: “Using KeemenaSubwords tokenizers”
- Consider an optional extension module (Julia package extensions) only if you later want `tokenizer_name = :keemena_bpe` style symbols.

### 4.2 Direct user API (KeemenaSubwords)

Design principles:
- Separate “tokenization into pieces” from “encoding into ids”.
- Keep types explicit and easy to dispatch on.
- Keep file format support behind a small I/O layer.

User-facing API proposal:

#### Core types
- `abstract type AbstractSubwordTokenizer <: Function end`
- Concrete tokenizers:
  - `struct BPETokenizer <: AbstractSubwordTokenizer ... end`
  - `struct ByteBPETokenizer <: AbstractSubwordTokenizer ... end`
  - `struct WordPieceTokenizer <: AbstractSubwordTokenizer ... end`
  - `struct UnigramTokenizer <: AbstractSubwordTokenizer ... end`
  - `struct SentencePieceTokenizer <: AbstractSubwordTokenizer ... end` (wrapper around BPE/Unigram or direct parsed model)

#### Construction / loading
- `load_tokenizer(name::Symbol; kwargs...) -> AbstractSubwordTokenizer`
- `load_tokenizer(path::AbstractString; format::Symbol = :auto, kwargs...) -> AbstractSubwordTokenizer`
- `load_tokenizer(spec::NamedTuple; kwargs...) -> AbstractSubwordTokenizer`
  - e.g. `(format=:bpe_gpt2, vocab="...", merges="...")`

#### Tokenization
- `tokenize(tokenizer::AbstractSubwordTokenizer, text::AbstractString)::Vector{String}`
- Call overload: `(tokenizer)(text::AbstractString)::Vector{String}` delegates to `tokenize`

#### Encoding/decoding (IDs)
- `encode(tokenizer::AbstractSubwordTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}`
- `decode(tokenizer::AbstractSubwordTokenizer, token_ids::AbstractVector{Int})::String`
- `token_to_id(tokenizer, token::AbstractString)::Int`
- `id_to_token(tokenizer, id::Int)::String`

#### Vocabulary / metadata
- `vocab_size(tokenizer)::Int`
- `special_tokens(tokenizer)::Dict{Symbol,String}` or `Dict{Symbol,Int}` depending on design
- `unk_id(tokenizer)::Int`
- `model_info(tokenizer)::NamedTuple` (format, version, normalizer info, etc.)

#### Saving/export
- `save_tokenizer(tokenizer, outdir::AbstractString; format::Symbol = :internal)`
- `export_tokenizer(tokenizer, outdir; format=:bpe_gpt2 | :wordpiece_vocab | :sentencepiece_model | ...)`

#### Training (future-facing but plan now)
- `train_bpe(corpus; vocab_size::Int, min_frequency::Int=2, specials=..., ...) -> BPETokenizer`
- `train_unigram(corpus; vocab_size::Int, seed_size::Int, num_iters::Int, ...) -> UnigramTokenizer`
- `train_wordpiece(...)` (optional later)

---

## 5) Changes to current KeemenaSubwords package layout

From the repo root, you already have a standard skeleton:
- `.github/`, `docs/`, `src/`, `test/`, `Project.toml`, etc. 

Recommended additions/changes:
1) Add `Artifacts.toml` at repo root for built-in models.
2) Add `src/models/` (or `src/registry.jl`) to centralize built-in model registry + artifact resolution.
3) Split `src` into multiple included files so each algorithm is isolated but the package remains simple.
4) Keep one public module `KeemenaSubwords` (no need for deep submodules unless you want strict namespaces).
5) Add `test/golden/` or `test/data/` with small fixtures for deterministic tests (tiny vocab/merges files).

Avoid over-engineering:
- Prefer “flat includes” over many nested modules unless you truly need separate namespaces.

---

## 6) Proposed file/module tree + function signature contracts

Below is a pragmatic structure that stays simple but keeps algorithms isolated.

### 6.1 Tree
```
KeemenaSubwords.jl/
  Artifacts.toml                     # built-in models (core)
  src/
    KeemenaSubwords.jl               # main module, exports, includes
    types.jl                         # AbstractSubwordTokenizer + shared structs
    vocab.jl                         # vocab + special tokens utilities
    normalization.jl                 # optional normalizer hooks (minimal)
    models.jl                        # built-in model registry + artifact resolution
    io.jl                            # auto-detect formats + load/save dispatch
    bpe.jl                           # BPE tokenizer + helpers
    bytebpe.jl                       # GPT-2 style byte-level BPE specifics
    wordpiece.jl                     # WordPiece tokenizer
    unigram.jl                       # Unigram tokenizer (DP/Viterbi)
    sentencepiece.jl                 # SentencePiece model loading + wrappers
    training.jl                      # shared training front-ends (calls into *_train.jl)
    bpe_train.jl                     # BPE training
    unigram_train.jl                 # Unigram training
  test/
    runtests.jl
    fixtures/
      bpe/
      wordpiece/
      sentencepiece/
      unigram/
```

### 6.2 `src/KeemenaSubwords.jl`
```julia
module KeemenaSubwords

# include files in dependency order
include("types.jl")
include("vocab.jl")
include("normalization.jl")
include("models.jl")
include("io.jl")
include("bpe.jl")
include("bytebpe.jl")
include("wordpiece.jl")
include("unigram.jl")
include("sentencepiece.jl")
include("training.jl")
include("bpe_train.jl")
include("unigram_train.jl")

# Exports (keep minimal, stable)
export AbstractSubwordTokenizer,
       BPETokenizer, ByteBPETokenizer, WordPieceTokenizer, UnigramTokenizer, SentencePieceTokenizer,
       load_tokenizer, save_tokenizer, export_tokenizer,
       tokenize, encode, decode,
       level_key, available_models, describe_model

end
```

### 6.3 `src/types.jl`
```julia
"""
Abstract parent type for all tokenizers.

Design requirement:
- Must be callable to satisfy KeemenaPreprocessing's tokenizer contract:
  tokenizer(text::AbstractString) -> Vector{String}
"""
abstract type AbstractSubwordTokenizer <: Function end

"""
Shared vocabulary container.

Suggested invariants:
- ids are 1-based (Julia-friendly)
- special tokens are present (at least :unk)
"""
struct SubwordVocabulary
    id_to_token::Vector{String}
    token_to_id::Dict{String,Int}
    special_token_ids::Dict{Symbol,Int}
end

"""
Standard metadata about a tokenizer model.
"""
struct TokenizerMetadata
    format::Symbol              # :bpe, :bytebpe, :wordpiece, :unigram, :sentencepiece
    model_name::String          # human readable
    version::VersionNumber
    normalizer::Symbol          # :none, :nfkc, etc (minimal for now)
end

"""
Return the level key KeemenaPreprocessing will store when using this tokenizer callable.

KeemenaPreprocessing uses Symbol(typeof(fn)) for Function tokenizers. 
"""
level_key(tokenizer::AbstractSubwordTokenizer)::Symbol = Symbol(typeof(tokenizer))
```

### 6.4 `src/vocab.jl`
```julia
# Contracts:
# - Provide fast token_to_id lookups
# - Provide stable handling of unknown tokens

unk_id(vocab::SubwordVocabulary)::Int
pad_id(vocab::SubwordVocabulary)::Union{Int,Nothing}
bos_id(vocab::SubwordVocabulary)::Union{Int,Nothing}
eos_id(vocab::SubwordVocabulary)::Union{Int,Nothing}

token_to_id(vocab::SubwordVocabulary, token::AbstractString)::Int
id_to_token(vocab::SubwordVocabulary, id::Int)::String

vocab_size(vocab::SubwordVocabulary)::Int

"""
Construct vocabulary from a list of tokens.

- tokens[1] is id 1, etc.
- insert specials first if requested (stable ordering).
"""
build_vocab(tokens::Vector{String};
            special_tokens::Dict{Symbol,String} = Dict(:unk => "<UNK>"))::SubwordVocabulary
```

### 6.5 `src/normalization.jl`
```julia
"""
Normalization hook.
Keep minimal at first: allow user to pass a custom callable.

Return:
- normalized text
"""
normalize_text(text::AbstractString; normalizer::Union{Nothing,Function}=nothing)::String
```

### 6.6 `src/models.jl`
```julia
"""
Return list of built-in models shipped via Artifacts/fixtures.
"""
available_models()::Vector{Symbol}

"""
Return metadata about a built-in model.
"""
describe_model(name::Symbol)::NamedTuple

"""
Resolve a built-in model to an on-disk path (artifact path).
"""
model_path(name::Symbol)::String
```

### 6.7 `src/io.jl`
```julia
"""
Load tokenizer from a built-in model name.
"""
load_tokenizer(name::Symbol; kwargs...)::AbstractSubwordTokenizer

"""
Load tokenizer from file path(s).

format = :auto attempts to detect based on extension or directory contents:
- SentencePiece .model
- WordPiece vocab.txt
- GPT2 BPE vocab.json + merges.txt
- internal KeemenaSubwords format (e.g. tokenizer.json)
"""
load_tokenizer(path::AbstractString; format::Symbol = :auto, kwargs...)::AbstractSubwordTokenizer

"""
Save tokenizer to an internal canonical format for round-tripping.
"""
save_tokenizer(tokenizer::AbstractSubwordTokenizer, outdir::AbstractString)::Nothing

"""
Export tokenizer to external formats.
"""
export_tokenizer(tokenizer::AbstractSubwordTokenizer, outdir::AbstractString; format::Symbol)::Nothing
```

### 6.8 `src/bpe.jl`
```julia
struct BPETokenizer <: AbstractSubwordTokenizer
    vocab::SubwordVocabulary
    # pair_ranks maps (a,b) => merge rank (lower is earlier)
    pair_ranks::Dict{Tuple{String,String},Int}
    metadata::TokenizerMetadata
    # configuration knobs
    end_of_word_marker::Union{Nothing,String}   # optional, for classic BPE variants
end

"""
Tokenize a string into subword pieces (strings).

Contract:
- Return Vector{String}
- Deterministic given same tokenizer + input
"""
tokenize(tokenizer::BPETokenizer, text::AbstractString)::Vector{String}

# Make it callable for KeemenaPreprocessing integration.
(tokenizer::BPETokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::BPETokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::BPETokenizer, ids::AbstractVector{Int})::String
```

### 6.9 `src/bytebpe.jl`
```julia
struct ByteBPETokenizer <: AbstractSubwordTokenizer
    base::BPETokenizer
    # byte encoder/decoder tables
    byte_to_unicode::Vector{Char}     # length 256
    unicode_to_byte::Dict{Char,UInt8}
    metadata::TokenizerMetadata
end

tokenize(tokenizer::ByteBPETokenizer, text::AbstractString)::Vector{String}
(tokenizer::ByteBPETokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::ByteBPETokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::ByteBPETokenizer, ids::AbstractVector{Int})::String
```

### 6.10 `src/wordpiece.jl`
```julia
struct WordPieceTokenizer <: AbstractSubwordTokenizer
    vocab::SubwordVocabulary
    continuation_prefix::String   # usually "##"
    metadata::TokenizerMetadata
end

"""
Greedy longest-match WordPiece tokenization.

Contract:
- If a word cannot be segmented, emit unk token
- Keep continuation_prefix rules consistent
"""
tokenize(tokenizer::WordPieceTokenizer, text::AbstractString)::Vector{String}
(tokenizer::WordPieceTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::WordPieceTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::WordPieceTokenizer, ids::AbstractVector{Int})::String
```

### 6.11 `src/unigram.jl`
```julia
struct UnigramTokenizer <: AbstractSubwordTokenizer
    vocab::SubwordVocabulary
    # token log-probabilities (aligned with vocab ids)
    logprobs::Vector{Float64}
    metadata::TokenizerMetadata
end

"""
Viterbi segmentation using log-probs.

Contract:
- Deterministic best-path segmentation
- If no path exists, fall back to <UNK> (or optional byte fallback later)
"""
tokenize(tokenizer::UnigramTokenizer, text::AbstractString)::Vector{String}
(tokenizer::UnigramTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::UnigramTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::UnigramTokenizer, ids::AbstractVector{Int})::String
```

### 6.12 `src/sentencepiece.jl`
```julia
"""
SentencePiece wrapper.

Design:
- parse SentencePiece .model (proto) into either:
  - UnigramTokenizer
  - BPETokenizer
and provide SentencePiece-specific whitespace marker behavior as needed.
"""
struct SentencePieceTokenizer <: AbstractSubwordTokenizer
    inner::AbstractSubwordTokenizer   # BPETokenizer or UnigramTokenizer
    whitespace_marker::String         # typically "▁"
    metadata::TokenizerMetadata
end

tokenize(tokenizer::SentencePieceTokenizer, text::AbstractString)::Vector{String}
(tokenizer::SentencePieceTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::SentencePieceTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::SentencePieceTokenizer, ids::AbstractVector{Int})::String
```

### 6.13 `src/training.jl`
```julia
"""
High-level training entry points.

These should accept either:
- Vector{String} documents
- iterator of strings
- (future) file paths

Keep memory predictable where possible.
"""

train_bpe(corpus;
          vocab_size::Int,
          min_frequency::Int = 2,
          special_tokens::Dict{Symbol,String} = Dict(:unk=>"<UNK>", :pad=>"<PAD>"),
          pretokenizer::Union{Nothing,Function} = nothing)::BPETokenizer

train_unigram(corpus;
              vocab_size::Int,
              seed_size::Int = 200_000,
              num_iters::Int = 5,
              special_tokens::Dict{Symbol,String} = Dict(:unk=>"<UNK>", :pad=>"<PAD>"),
              pretokenizer::Union{Nothing,Function} = nothing)::UnigramTokenizer
```

### 6.14 `src/bpe_train.jl`
```julia
"""
Implement BPE training.

Contract:
- Build initial symbol vocabulary (characters or byte symbols)
- Iteratively merge most frequent pairs until vocab_size reached
- Output merges + vocab

Implementation note:
- Use a priority queue for pair counts OR a batched recomputation strategy.
- Keep it correct first; optimize later.
"""
```

### 6.15 `src/unigram_train.jl`
```julia
"""
Implement Unigram LM training (SentencePiece-style).

Contract:
- Build seed vocab (frequent substrings)
- Run EM iterations to estimate token probabilities
- Prune to target vocab_size
- Output vocab + logprobs
"""
```

---

## 7) Implementation order (recommended)

This is the shortest path to a usable package with good ergonomics.

### Phase 0: groundwork (small, fast)
1) `types.jl` + `vocab.jl`
- Define `AbstractSubwordTokenizer <: Function`
- Define `SubwordVocabulary`
- Define `level_key(tokenizer)` helper

2) `models.jl` + `Artifacts.toml`
- Add a minimal registry and stub built-in model hooks (even if models come later)

3) `io.jl` (skeleton)
- Define `load_tokenizer(::Symbol)` and `load_tokenizer(::String)` API
- Implement format dispatch stubs with informative errors

### Phase 1: tokenization (deliver value early)
4) WordPiece tokenization + loader (`wordpiece.jl`)
- Load `vocab.txt`
- Implement greedy longest-match tokenization
- Tests with tiny vocab fixture

5) BPE tokenization + loader (`bpe.jl`)
- Support a simple merges+vocab format first (internal)
- Add GPT-2 style loader later if desired

6) Byte-level BPE (`bytebpe.jl`)
- Implement byte<->unicode mapping
- Connect to BPE engine
- Tests with tiny fixture

7) Unigram tokenization (`unigram.jl`)
- Implement DP/Viterbi using token logprobs
- Tests with tiny fixture

8) SentencePiece loading wrapper (`sentencepiece.jl`)
- Parse `.model` (ProtoBuf) OR initially support exporting/importing a simplified JSON representation
- Wrap into `SentencePieceTokenizer(inner=...)`

### Phase 2: built-in core models (user experience)
9) Add 2-3 built-in “core” models via artifacts
- Ensure `load_tokenizer(:core_...)` works out of the box

10) Add documentation pages:
- “Using KeemenaSubwords with KeemenaPreprocessing”
- “Loading tokenizers from files”
- “Built-in models”

### Phase 3: training (bigger effort, but now on stable foundation)
11) BPE training (`bpe_train.jl`)
- Start with a straightforward implementation
- Add streaming-friendly variants later

12) Unigram training (`unigram_train.jl`)
- Implement seed vocab generation + EM + pruning

13) Save/export formats
- Ensure `save_tokenizer` and `export_tokenizer` produce files users can load later

### Phase 4: polish and extensibility
14) Performance passes (without changing API)
- caching, faster pair-rank lookup, tries for WordPiece, optimized DP for Unigram

15) Optional advanced features
- dropout / sampling
- byte fallback for unigram
- additional format adapters

---

## 8) Minimal “contract” summary for extensibility

If you want to add a new tokenizer later, the only hard contract should be:

- Define `struct NewTokenizer <: AbstractSubwordTokenizer ... end`
- Implement:
  - `tokenize(tokenizer::NewTokenizer, text::AbstractString)::Vector{String}`
  - `encode(tokenizer::NewTokenizer, text::AbstractString; add_special_tokens=false)::Vector{Int}` (optional early)
  - `decode(tokenizer::NewTokenizer, ids)::String` (optional early)
- Make it callable:
  - `(tokenizer::NewTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)`
- Provide I/O:
  - `load_tokenizer(path; format=:new_format)` and/or registry entries
- Provide `level_key(tokenizer)` (already generic) so KeemenaPreprocessing users can retrieve the right level.

This keeps the package simple while remaining extensible.

---

## 9) Add downloadable pretrained tokenizer assets (tiktoken + public baselines) and register them as built-in models

### 9.1 Goal
Add a small set of well-known pretrained tokenizer files (from reputable upstream sources) so users can do:

- `load_tokenizer(:tiktoken_o200k_base)` / `load_tokenizer(:tiktoken_cl100k_base)`
- `load_tokenizer(:openai_gpt2_bpe)`
- `load_tokenizer(:bert_base_uncased_wordpiece)`
- `load_tokenizer(:t5_small_sentencepiece_unigram)`

without the user manually downloading vocab/merges/model files.

This should integrate cleanly with the existing model registry UX:
- `available_models()` lists them
- `describe_model(:key)` explains them
- `load_tokenizer(:key)` resolves files and constructs the correct tokenizer instance

### 9.2 Where to place the files (layout decision)
Do NOT commit large tokenizer assets directly into `models/` in the git repo.

Instead:
- Keep only metadata and registry code in the repo.
- Store external pretrained files as Julia Artifacts (lazy downloads) or in a user cache directory outside the package source tree.

Rationale:
- Keeps the package lightweight
- Avoids modifying the installed package directory
- Allows optional prefetch for offline environments

### 9.3 Directory structure for built-in assets (inside the artifact or cache root)
Organize by parser/format first, then by model key:

- tiktoken/
  - o200k_base/
    - o200k_base.tiktoken
  - cl100k_base/
    - cl100k_base.tiktoken
  - (optional) r50k_base/
    - r50k_base.tiktoken
  - (optional) p50k_base/
    - p50k_base.tiktoken

- bpe/
  - openai_gpt2/
    - encoder.json
    - vocab.bpe

- wordpiece/
  - bert_base_uncased/
    - vocab.txt

- sentencepiece/
  - t5_small/
    - spiece.model

This mirrors your parser types and makes it easy to add more models later without changing code structure.

### 9.4 Built-in model keys and descriptions (Symbols as stable user-facing IDs)
Add these additional built-in keys:

Tiktoken (OpenAI hosted encodings):
- `:tiktoken_o200k_base`  - OpenAI tiktoken encoding file (o200k_base.tiktoken)
- `:tiktoken_cl100k_base` - OpenAI tiktoken encoding file (cl100k_base.tiktoken)
- Optional legacy encodings:
  - `:tiktoken_r50k_base`
  - `:tiktoken_p50k_base`

Classic / GPT-2 style byte-level BPE:
- `:openai_gpt2_bpe` - GPT-2 style BPE (encoder.json + vocab.bpe)

WordPiece:
- `:bert_base_uncased_wordpiece` - BERT base uncased WordPiece vocab.txt

SentencePiece (Unigram LM):
- `:t5_small_sentencepiece_unigram` - T5-small SentencePiece model (spiece.model)

Each key should have:
- a one-line human description
- a format tag (`:tiktoken`, `:bpe_gpt2`, `:wordpiece_vocab`, `:sentencepiece_model`)
- resolved absolute paths to the required files
- (optional but recommended) provenance info: upstream URL(s), license string, version/date

### 9.5 Reputable upstream download sources (authoritative URLs)
OpenAI public tiktoken encodings:
- https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken
- https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken
- (optional) https://openaipublic.blob.core.windows.net/encodings/r50k_base.tiktoken
- (optional) https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken

OpenAI hosted GPT-2 BPE assets:
- https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json
- https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe

Hugging Face hosted WordPiece vocab:
- https://huggingface.co/bert-base-uncased/raw/main/vocab.txt

Hugging Face hosted T5 SentencePiece model:
- https://huggingface.co/google-t5/t5-small/resolve/main/spiece.model

### 9.6 Registry constants and public API behavior
Implementation intent (keep it simple, match existing style):

- Extend the existing model registry so `available_models()` includes the new keys.
- Extend `describe_model(key)` so it prints:
  - format/type
  - file list (relative + resolved absolute)
  - a short description + provenance URL(s)

- Extend `load_tokenizer(key::Symbol)` so it:
  1) recognizes these new keys
  2) ensures files exist (artifact resolve / cached download)
  3) chooses the correct parser based on `format`
  4) returns a tokenizer instance that matches your existing `tokenize(...)` API

Add a convenience function:
- `prefetch_models(keys=available_models(); force=false)` so users can download all built-ins once (useful for offline environments later).

### 9.7 Minimal acceptance checks
- For each new built-in key, add a small smoke test:
  - load it
  - run `tokenize` on a simple string
  - ensure result is non-empty and stable
  - if decode exists, check roundtrip on a safe sample
Gate any network downloads behind an env var if needed, but prefer artifacts so CI can be deterministic.


## 10) LLM-oriented built-in model coverage review and next recommendations

### What is already good (keep it)
- The public API surface is already aligned with what users expect:
  - `load_tokenizer(::Symbol)` for built-ins
  - `load_tokenizer(path_or_spec; format=...)` for user-supplied assets
  - `available_models()`, `describe_model(key)`, `model_path(key)`
  - `prefetch_models(keys)` so users can fetch/cache what they need up front
- The built-in selection already covers the major file families used in practice:
  - tiktoken encodings (OpenAI-style)
  - GPT-2 / RoBERTa style BPE (vocab.json + merges.txt, sometimes encoder.json + vocab.bpe)
  - WordPiece (vocab.txt)
  - SentencePiece Unigram (spiece.model)
  - SentencePiece BPE (sentencepiece.bpe.model, tokenizer.model(.v3), etc)
- The KeemenaPreprocessing integration story is already clean: "tokenizer is callable", and the corpus is stored under a level key.

### What is incomplete or risky for this stage
- README vs docs drift:
  - The GitHub README still reads like the older scope (core toy models and "training not implemented"),
    while the docs show a broader set of built-ins and prefetching.
  - Action: update README to match the docs and the true built-in set.
- Built-in model positioning:
  - Users will judge "coverage" mostly by whether the package includes tokenizers for the LLM families they actually use.
  - Right now, the biggest missing family perception-wise is Llama and Qwen.
- License/gating pitfalls:
  - Some popular tokenizers (notably some Llama assets) may be gated or have redistribution limits.
  - Action: draw a strict line between:
    - "shipped built-ins" (redistribution-safe)
    - "supported but not shipped" (user must provide files)

### Recommended additions to improve "one stop shop" perception (without becoming exhaustive)

#### A) Add as shipped built-ins (high value, common in LLM work)
1) Qwen2.5 tokenizer (BPE)
   - Files: vocab.json + merges.txt (+ tokenizer.json if you want fast loading / exact parity)
   - Add one representative key (example):
     - `:qwen2_5_bpe` or `:qwen2_5_7b_instruct_bpe`
   - This single addition covers a large share of modern open LLM usage patterns.

2) Mistral Tekken tokenizer (tiktoken-based), if redistributable
   - Add one key (example):
     - `:mistral_tekken_tiktoken`
   - This matters because newer Mistral tokenization can be tiktoken-based (separate from SentencePiece).

#### B) Support explicitly, but do not ship assets (to avoid license/gating trouble)
3) Llama 2 style SentencePiece (tokenizer.model)
   - Add a documented "supported external spec" path:
     - `load_tokenizer("/path/to/tokenizer.model")` or `load_tokenizer("/path/to/llama2_dir")`
   - Provide a helper doc page: "How to point KeemenaSubwords at Llama tokenizers".

4) Llama 3 style tiktoken-model file
   - Add a documented external spec:
     - `load_tokenizer("/path/to/llama3_*.tiktoken"; format=:tiktoken)`
   - Do NOT ship by default unless you have a redistribution-safe source.

### Make the built-in registry feel professional (small changes, big UX win)
- Ensure every built-in has metadata fields exposed via `describe_model`:
  - `key::Symbol`
  - `family` (openai, mistral, qwen, llama, bert, t5, roberta, etc)
  - `format` (tiktoken, bpe_gpt2, bytebpe, wordpiece, sentencepiece_unigram, sentencepiece_bpe)
  - `files` (expected filenames)
  - `upstream_source` (URL string)
  - `license` (short string)
  - `description` (1-2 lines)
- Add simple filters:
  - `available_models(; format=nothing, family=nothing)`
- Add a "recommended_defaults_for_llms()" convenience list:
  - returns a short list of keys a user can prefetch for typical LLM usage.

### Directory layout recommendation (keep it consistent and predictable)
- Prefer:
  - `models/tiktoken/<model_key>/...`
  - `models/bpe/<model_key>/...`
  - `models/wordpiece/<model_key>/...`
  - `models/sentencepiece/<model_key>/...`
- Keep "in-repo fallback" files minimal (only tiny demo/core models).
- Use Artifacts for real model assets to avoid bloating the git repo.
- Document the mapping from model key -> artifact name -> files.

### Test expectations (minimal but confidence-building)
- Add one test per built-in model key:
  - load succeeds
  - encode/decode round-trip for a small string does not throw
  - (optional) token ids match a small golden vector for 1-2 flagship models

### Implementation order (suggested)
1) Update README to reflect current API + built-ins (fast win).
2) Add Qwen2.5 built-in (BPE) + tests + docs.
3) Add Mistral Tekken built-in (tiktoken) if redistributable + tests + docs.
4) Add explicit "supported external specs" docs for Llama 2 and Llama 3 (no shipping).
5) Add `available_models` filters + "recommended_defaults_for_llms()".


## 11) Close the remaining "no Python needed" gaps for LLM tokenization

### 11.1 Objective
Make KeemenaSubwords feel like a one-stop Julia solution for most LLM tokenization work by addressing the last big gaps:
- Support Hugging Face `tokenizer.json` (json-only tokenizers) in pure Julia so users do not need Python.
- Provide a convenient path for LLaMA-family tokenizers (without bundling or redistributing gated assets).
- Add one more flagship BPE LLM tokenizer as a built-in to make the coverage feel real.
- Optionally add 1 more WordPiece tokenizer for multilingual coverage (small cost, improves perception).

### 11.2 Deliverables
- A new loader: `load_tokenizer(path; format=:hf_tokenizer_json)` that loads Hugging Face `tokenizer.json` directly.
- Directory autodetection: `load_tokenizer(dir)` prefers `tokenizer.json` when present (unless the user forces another format).
- A "Hugging Face hub download" utility (opt-in) to fetch tokenizer assets into the user cache using public URLs and optionally an auth token.
- New built-in registry keys:
  - `:qwen2_5_bpe` (flagship BPE LLM)
  - `:bert_base_multilingual_cased_wordpiece` (extra WordPiece)
- Documentation pages explaining:
  - what is shipped vs artifact-downloaded vs user-supplied / gated
  - how to use tokenizer.json and HF downloads
  - how to use LLaMA tokenizers via user-supplied paths or optional download
- Tests covering:
  - tokenizer.json loader
  - the new built-in keys
  - the HF download helper (network-gated or mocked)

### 11.3 Work plan

#### 11.3.1 Add Hugging Face tokenizer.json support (no Python)
Goal: Parse and execute the common Hugging Face tokenization pipeline stored in `tokenizer.json`.

Steps:
1) Add a new internal module, example:
   - `src/huggingface_json/`
     - `hf_json_types.jl` (structs for parsed JSON)
     - `hf_json_parse.jl` (JSON3 parsing and validation)
     - `hf_json_pipeline.jl` (normalizer, pretokenizer, postprocessor, decoder)
     - `hf_json_loader.jl` (public load function + integration with `load_tokenizer`)
2) Implement a minimal but high-value supported matrix first, with clear errors for unsupported parts.

Minimum supported "model" types:
- BPE (most common for LLMs in tokenizer.json)
- WordPiece (for BERT ecosystem)
- Unigram (less common in tokenizer.json, but support if feasible)

Minimum supported pipeline components:
- Normalizers:
  - Lowercase
  - NFKC (or a general unicode normalization wrapper)
  - Sequence (compose multiple)
- Pre-tokenizers:
  - ByteLevel
  - Whitespace / WhitespaceSplit
  - Metaspace
  - Sequence (compose multiple)
  - Split with regex (common in many LLM tokenizer.json files)
- Post-processors:
  - TemplateProcessing (special tokens insertion)
  - Sequence (compose multiple)
- Decoders:
  - ByteLevel
  - WordPiece
  - Metaspace
  - Sequence (compose multiple)

3) Make unsupported JSON components fail fast with a good message:
- "Unsupported pre_tokenizer type: X"
- "Unsupported post_processor type: X"
- Include the json path to the failing block and suggest a workaround:
  - "Use vocab.json + merges.txt if present"
  - "Export a simplified tokenizer.json"

4) Ensure the resulting tokenizer integrates with the existing KeemenaSubwords interface:
- returns `AbstractSubwordTokenizer <: Function`
- supports `tokenize`, `encode`, and `decode`
- preserves special tokens and added tokens from tokenizer.json

5) Add `export_tokenizer(tokenizer; format=:hf_tokenizer_json)` as a later optional step.
For stage 11, prioritize loading (inbound compatibility) over exporting.

Acceptance tests:
- Load tokenizer.json from a known repo fixture and run:
  - tokenize("Hello world")
  - encode/decode round-trip smoke tests
  - verify special tokens insertion works for at least one TemplateProcessing example

#### 11.3.2 Add flagship BPE LLM built-in: Qwen2.5
Goal: Provide a modern, popular BPE LLM tokenizer out of the box.

Steps:
1) Choose one Qwen2.5 model repo that is permissively licensed and stable.
Recommended: Qwen2.5-1.5B (Apache-2.0) since the tokenizer files are small and widely representative.
2) Artifact contents for `:qwen2_5_bpe` should include:
- `vocab.json`
- `merges.txt`
- `tokenizer.json`
- `tokenizer_config.json`
- `special_tokens_map.json` (if present and useful)
3) Add registry entry:
- key: `:qwen2_5_bpe`
- format: `:hf_tokenizer_json` (preferred), with fallback `:bpe_gpt2` using merges/vocab
- description: "Qwen2.5 BPE tokenizer (Hugging Face)."
- include upstream repo id and pinned revision in metadata
4) Add tests:
- ensure `load_tokenizer(:qwen2_5_bpe)` works after `prefetch_models([:qwen2_5_bpe])`
- ensure stable tokenization for 2-3 short strings (golden outputs)

#### 11.3.3 LLaMA family convenience without redistribution
Goal: Make LLaMA tokenizers easy to use, but do not ship or redistribute gated assets.

Key idea:
- Provide "supported external specs" and an opt-in download helper for users who already have access and accept the license.

Steps:
1) Document supported file types:
- LLaMA 2 style: SentencePiece BPE `.model` file (user supplies path)
- LLaMA 3 style: tiktoken model file (user supplies path)
2) Add an opt-in HF hub download helper (not a built-in artifact):
- `download_hf_files(repo_id, filenames; revision, outdir, token=nothing)`
- If `token` is provided, pass it as Authorization header to support gated repos
- Store into KeemenaSubwords cache path (not into the package source tree)
3) Add `register_local_model!(key::Symbol, path_or_dir; format, description)`:
- persists a small local registry file in the user cache (not in git)
- lets users do:
  - register_local_model!(:llama3, "/path/to/tokenizer.model"; format=:tiktoken)
  - load_tokenizer(:llama3)
4) Add docs:
- "We do not distribute LLaMA tokenizer files; you must have access yourself."
- show a simple workflow:
  - user downloads from their authorized source
  - user registers local model
  - user loads it by Symbol key

Acceptance tests:
- Unit test for `register_local_model!` and `load_tokenizer(:custom_key)` using a small local fixture model (not LLaMA).

#### 11.3.4 Add one extra WordPiece built-in (optional but improves completeness)
Goal: Provide a second WordPiece option that is genuinely useful.

Recommendation:
- `:bert_base_multilingual_cased_wordpiece`
- Artifact includes only `vocab.txt`

Notes:
- WordPiece is less common for LLMs, but it is still important for classic NLP and multilingual pipelines.
- Adding one multilingual WordPiece model improves coverage perception at low cost.

#### 11.3.5 UX and documentation polish for "one stop shop"
- Add `available_models(; format=nothing, family=nothing, shipped=nothing)` filters.
- Add `recommended_defaults_for_llms()` returning a short list:
  - OpenAI tiktoken encodings
  - Mistral sentencepiece
  - Qwen2.5 BPE
  - Phi-2 BPE
  - RoBERTa BPE
- Ensure `describe_model(key)` prints:
  - format
  - whether it is artifact-backed or repo-shipped or user-registered
  - expected filenames
  - short description

### 11.4 Suggested implementation order
1) Hugging Face tokenizer.json loader (core minimal matrix)
2) Add `:qwen2_5_bpe` built-in artifact + tests
3) Add local model registry + HF download helper (opt-in)
4) Add multilingual WordPiece built-in
5) Docs + UX filters + recommended defaults


## 12) Eliminate README/docs drift, finish WordPiece artifact coverage, and add LLaMA "one command install" convenience (without redistributing gated files)

### 12.1 Objective
Make the package feel finished and trustworthy by:
- Ensuring README and Documenter docs are generated from the same source of truth (no drift).
- Ensuring WordPiece coverage is fully real (artifact is actually downloadable and prefetchable), not just present in docs.
- Providing LLaMA-family convenience that feels like built-ins (automatic download + key-based loading) while respecting gated licensing and not redistributing files.

### 12.2 What "done" looks like
- The README built-in key list exactly matches `available_models()` output and includes WordPiece multilingual key.
- Docs examples use the correct function name(s) for local registry (`register_local_model!` if that is the canonical API).
- WordPiece built-ins:
  - `:bert_base_uncased_wordpiece`
  - `:bert_base_multilingual_cased_wordpiece`
  are both artifact-backed and `prefetch_models([...])` succeeds.
- LLaMA convenience:
  - Users can run one command like `install_model!(:llama3_8b_instruct; token=ENV["HF_TOKEN"])`
  - After install: `load_tokenizer(:llama3_8b_instruct)` works without specifying file paths.
  - The package does not ship LLaMA files in Artifacts.toml and does not redistribute them.

### 12.3 Tasks

#### A) Single source of truth for model inventory (stop drift)
1) Create one canonical registry table in code (if not already):
- Keep all model keys and metadata in one place (example file: `src/model_registry.jl`).
- Include fields used everywhere:
  - `key`, `format`, `family`, `license`, `description`
  - `expected_files`
  - `distribution` (one of: `:shipped`, `:artifact_public`, `:installable_gated`, `:user_local`)
  - `upstream_repo`, `upstream_ref`, `upstream_files`

2) Auto-generate README models section
- Add markers to README.md:
  - `<!-- KEEMENA_MODELS_START -->`
  - `<!-- KEEMENA_MODELS_END -->`
- Add `tools/sync_readme_models.jl` that:
  - loads KeemenaSubwords
  - queries registry and renders a markdown table grouped by format/family
  - replaces the marked section
- Add a CI check (or pre-commit script) that fails if the README is out of date.

3) Auto-generate docs "Built-In Models" content
- Either:
  - render the same table in Documenter by calling a function that returns markdown, or
  - include a generated markdown file produced by a doc build step.
Goal: docs and README are always consistent.

#### B) Fix naming drift in external/local registry API
1) Decide canonical API name:
- If `register_local_model!` is the intended public API, keep it and:
  - add `register_external_model!` as a deprecated alias (with a deprecation warning)
  - update docs examples to use `register_local_model!`
- If `register_external_model!` is the intended name, then:
  - update notes/docs to stop mentioning `register_local_model!`
  - ensure it persists to cache if that is implemented

2) Update docs and README examples to use the canonical name.

#### C) Ensure WordPiece artifact coverage is actually complete
1) Make `:bert_base_multilingual_cased_wordpiece` a first-class built-in everywhere:
- Add it to the README built-in list (via the auto-generation above).
- Ensure `describe_model(:bert_base_multilingual_cased_wordpiece)` works and prints correct provenance.

2) Ensure artifact binding exists and is stable
- Confirm `Artifacts.toml` includes the multilingual vocab artifact with:
  - pinned upstream revision (avoid floating "main" if possible)
  - sha256 verification
- Ensure `prefetch_models([:bert_base_multilingual_cased_wordpiece])` succeeds.

3) Optional: add one more WordPiece for completeness (only if you want)
- `:bert_base_cased_wordpiece` (English cased) as a small, cheap addition.
- Do not add more WordPiece beyond this; it becomes redundant noise.

4) Tests
- Add a test that prefetches multilingual WordPiece when network is allowed (ENV gated),
  and always tests that the registry entry exists and expected_files are correct.

#### D) LLaMA convenience without redistributing gated assets
Important constraint:
- Do not include Meta LLaMA tokenizer files as public package artifacts, and do not host them yourself.
- Provide an install workflow that uses the user's credentials or signed URL, stores into cache, and registers locally.

1) Add "installable gated models" registry entries
- Add entries with `distribution=:installable_gated`, for example:
  - `:llama2_tokenizer` (SentencePiece)
  - `:llama3_8b_tokenizer` (prefer tokenizer.json if available, otherwise documented alternatives)
- These are not "shipped" and not in Artifacts.toml, but they appear in:
  - `available_models(distribution=:installable_gated)` or similar.

2) Implement installer API
- `install_model!(key::Symbol; token=nothing, revision="main", force=false)`
  - Looks up registry entry
  - Calls `download_hf_files(...)` (already exists) for the listed filenames
  - Stores them under `KEEMENA_SUBWORDS_CACHE_DIR/install/<key>/...`
  - Calls `register_local_model!(key, installed_dir; format=..., family=:llama, description=...)`
- Optional: add `install_llama2_tokenizer!` and `install_llama3_tokenizer!` wrappers for discoverability.

3) Loading behavior after install
- `load_tokenizer(:llama3_8b_tokenizer)` should:
  - first check user local registry key (installed)
  - then fall back to regular built-ins
  - never silently attempt gated downloads without explicit `install_model!`

4) Docs
- Add a clear docs section:
  - "LLaMA tokenizers are gated; you must accept Meta license and have access"
  - "Use install_model! with HF token or use manual path loading"

5) Tests
- Add a unit test for install flow without network:
  - when token is missing, ensure a clear error message that explains how to proceed
- Add optional network test (ENV gated):
  - attempts to download only tokenizer files for a gated model when token is provided

### 12.4 Suggested implementation order
1) Auto-generate README and docs models tables from registry (kills drift fast).
2) Fix registry helper naming drift (register_local_model! vs register_external_model!).
3) Make multilingual WordPiece artifact fully solid + tests.
4) Add LLaMA install_model! workflow + docs.
5) Optional: add :bert_base_cased_wordpiece.


## 13) Documentation consolidation and path-based loader contracts for all tokenizer families

### 13.1 Objective
Make KeemenaSubwords feel "complete and professional" by:
1) Moving detailed inventories and usage guidance out of README into docs pages (README becomes a clean quickstart).
2) Making "bring your own tokenizer files" a first-class, clearly documented workflow across all families:
   - BPE (GPT-2/RoBERTa style)
   - Byte-level BPE
   - WordPiece
   - SentencePiece (Unigram and BPE models)
   - tiktoken (including LLaMA-3 style files that may be named .model)
   - Hugging Face tokenizer.json
3) Ensuring every loader has explicit, predictable function signatures and strong validation:
   - correct file expectations
   - clear errors when files are missing or the wrong format
   - robust auto-detection that can be overridden

### 13.2 Deliverables
- README reduced to: install, quickstart, 4-6 "featured" built-ins, links to docs.
- New or expanded docs pages:
  - "Tokenizer formats and required files" (single reference page)
  - "Loading tokenizers from local paths" (examples for each family)
  - "LLM cookbook" (OpenAI, Mistral, Qwen, LLaMA workflows)
  - "Installable gated models" (how install_model! works and why)
  - "Troubleshooting" (most common file/layout mistakes)
- Explicit loader contracts with convenience constructors for each format.
- Strong format detection and content sniffing for ambiguous cases (notably .model being SentencePiece binary vs tiktoken text).
- Tests that enforce the contracts and catch regressions in detection and error messages.

### 13.3 Tasks

#### A) Move depth from README to docs (reduce drift risk and improve clarity)
1) README.md should contain only:
   - 1 paragraph overview
   - install instructions
   - minimal quickstart examples:
     - load built-in and tokenize
     - prefetch_models
     - load from local path (one example)
   - a short "Featured models" list (not the full table)
   - links to docs pages: Models, Loading, Formats, LLaMA install, Troubleshooting

2) Docs pages to add or expand:
   - docs/src/formats.md
     - a table: format -> accepted inputs -> required files -> typical sources -> recommended loader call
   - docs/src/loading_local.md
     - "Bring your own files" recipes by family (BPE, WordPiece, SentencePiece, tiktoken, tokenizer.json)
   - docs/src/llm_cookbook.md
     - OpenAI tiktoken, Mistral SentencePiece, Qwen tokenizer.json, LLaMA install + local usage
   - docs/src/gated_models.md
     - explain install_model! and what it does/does not do (no redistribution)
   - docs/src/troubleshooting.md
     - "why did auto-detect pick the wrong format?"
     - "missing merges.txt"
     - "tokenizer.model is not SentencePiece, it is tiktoken text"

3) Keep the full model inventory table in docs only.
   - README should link to it.
   - Keep the auto-generation pipeline so tables remain consistent with registry.

#### B) Standardize and document path-based loader contracts (per family)
Add explicit public convenience constructors (in addition to load_tokenizer auto-detect):

BPE (GPT-2/RoBERTa style):
- load_bpe_gpt2(vocab_json::AbstractString, merges_txt::AbstractString; byte_level::Bool=true, kwargs...) -> AbstractSubwordTokenizer

BPE (encoder.json + vocab.bpe variant):
- load_bpe_encoder(encoder_json::AbstractString, vocab_bpe::AbstractString; kwargs...) -> AbstractSubwordTokenizer

WordPiece:
- load_wordpiece(vocab_txt::AbstractString; continuation_prefix::String="##", kwargs...) -> AbstractSubwordTokenizer

SentencePiece:
- load_sentencepiece(model_file::AbstractString; kind::Symbol=:auto, kwargs...) -> AbstractSubwordTokenizer
  - kind=:auto chooses unigram vs bpe based on model metadata

Unigram (non-SentencePiece internal, if supported):
- load_unigram(vocab_file::AbstractString, scores_file::AbstractString; kwargs...) -> AbstractSubwordTokenizer
  - if not supported, do not add this signature; instead document that Unigram is loaded via SentencePiece .model or tokenizer.json

tiktoken:
- load_tiktoken(encoding_file::AbstractString; kwargs...) -> AbstractSubwordTokenizer

Hugging Face tokenizer.json:
- load_hf_tokenizer_json(tokenizer_json::AbstractString; kwargs...) -> HuggingFaceJSONTokenizer

For each function:
- Validate file existence
- Validate expected extension OR content signature (see section C)
- Produce errors that tell the user exactly which files are missing and show an example call

#### C) Improve auto-detection for ambiguous local files (especially LLaMA)
1) Implement a single internal detector:
- detect_tokenizer_format(path::AbstractString)::Symbol
- detect_tokenizer_files(dir::AbstractString)::NamedTuple or FilesSpec

2) Detection rules (in order):
- If directory contains tokenizer.json: choose :hf_tokenizer_json
- If directory contains vocab.json + merges.txt: choose :bpe_gpt2 (byte-level default)
- If directory contains encoder.json + vocab.bpe: choose :bpe_encoder
- If directory contains any of:
  - tokenizer.model
  - spiece.model
  - sentencepiece.bpe.model
  choose :sentencepiece_model
- If file extension is .tiktoken: choose :tiktoken
- If file extension is .model:
  - sniff first bytes:
    - if it looks like SentencePiece protobuf (binary): choose :sentencepiece_model
    - else if it looks like tiktoken text (lines like "<base64> <int>"): choose :tiktoken
  - document this explicitly (LLaMA 3 commonly ships a tiktoken text file named tokenizer.model)

3) Always allow user override:
- load_tokenizer(path; format=:tiktoken) should bypass auto-detect.

#### D) Make "user supplied local models" ergonomic and format-safe
1) Ensure register_local_model! accepts:
- register_local_model!(key::Symbol, path::AbstractString; format=:auto, description="", family=nothing)
- register_local_model!(key::Symbol, spec; description="", family=nothing)
  where spec is a FilesSpec or NamedTuple for explicit file sets.

2) Stored metadata should include:
- format
- expected_files
- resolved paths
- optional notes

3) load_tokenizer(:key) should:
- check local registry first (installed or user-registered)
- then check built-in registry (artifact/shipped)

#### E) LLaMA convenience: keep it opt-in, but make it feel built-in
1) Keep LLaMA out of Artifacts.toml if access is gated.
2) Treat LLaMA as distribution=:installable_gated:
- appears in docs under "Installable gated"
- install_model!(key; token=...) downloads into cache and then registers locally
3) Ensure docs give 2 workflows:
- install_model! (best UX if user has HF token)
- manual local path (if user already downloaded files elsewhere)

#### F) Tests that enforce contracts and prevent regressions
Add tests for:
- each explicit loader function:
  - missing file error messages mention expected files and show example usage
- detect_tokenizer_format and detect_tokenizer_files:
  - directory with tokenizer.json prefers hf_tokenizer_json
  - .model binary chooses sentencepiece
  - .model text chooses tiktoken
- register_local_model! works with:
  - directory path
  - explicit file tuple/spec
- load_tokenizer(path) with format overrides behaves as expected

### 13.4 Suggested implementation order
1) Docs restructure: create formats.md, loading_local.md, troubleshooting.md; slim README.
2) Add explicit loader functions and validation helpers (keep old paths working).
3) Implement robust .model sniffing and detection tests.
4) Expand register_local_model! to accept explicit specs and improve metadata.
5) Final docs polish with concrete examples for each family.


## 14) README slimming + documentation polish + API discoverability (final "user trust" pass)

### 14.1 Objective
Make KeemenaSubwords feel finished and easy to adopt by:
- Keeping README short (quickstart + links), pushing detail into Documenter docs.
- Eliminating remaining docs/README drift and example inconsistencies.
- Making all path-based loaders and file contracts obvious and discoverable in both docs narrative pages and the API reference.

### 14.2 What "done" looks like
- README is a quickstart only:
  - install
  - 2 to 3 minimal examples
  - a short "Featured models" list (not the full inventory)
  - links to docs pages for Models, Formats, Local Loading, LLM Cookbook, Gated Models, Troubleshooting, API
- Docs are the authoritative reference:
  - consistent example calls and filenames everywhere
  - consistent naming for local registry function (no deprecated name in primary examples)
- API reference page lists the explicit loader functions clearly (and they are exported).
- A CI check prevents drift:
  - generated README "Featured models" stays in sync with registry
  - docs models table stays in sync with registry
  - doctests or example checks catch wrong file names (vocab.txt vs vocab.json, etc.)

### 14.3 Tasks

#### A) Reduce README and make docs the default
1) Rewrite README to include only:
- Install instructions
- Quickstart examples:
  - built-in load + tokenize
  - local path load + format override example
  - gated install example (install_model! + load_tokenizer)
- Small "Featured models" bullet list (8 to 12 keys)
- Link list to docs:
  - Models
  - Formats and required files
  - Loading local tokenizers
  - LLM cookbook
  - Gated models (install_model!)
  - Troubleshooting
  - API reference

2) Remove any manual full key inventory from README (no large tables).

#### B) Keep inventory generated, but split output: README summary vs docs full table
1) Update the generator tool so it can render:
- A short featured list for README (grouped by format)
- A full table for docs models page
2) Add marker blocks:
- README: <!-- KEEMENA_FEATURED_MODELS_START --> ... <!-- KEEMENA_FEATURED_MODELS_END -->
- docs/src/models.md: keep full table markers
3) Add CI check that fails if generated blocks are stale.

#### C) Documentation consistency fixes (examples and naming)
1) Canonicalize named-spec fields and filenames across all docs:
- GPT2/RoBERTa BPE must use:
  - vocab.json + merges.txt
  - named spec keys: vocab_json, merges_txt
- encoder.json + vocab.bpe variant must use:
  - named spec keys: encoder_json, vocab_bpe
- WordPiece must use:
  - vocab.txt
  - named spec key: vocab_txt
- SentencePiece must use:
  - *.model / *.model.v3 / sentencepiece.bpe.model
  - named spec key: model_file (or model_path), but pick one and use it everywhere
- tiktoken must use:
  - *.tiktoken
  - also document the LLaMA3 case where the tiktoken text file may be named tokenizer.model and requires format=:tiktoken
- HF tokenizer.json must use:
  - tokenizer.json
  - named spec key: tokenizer_json

2) Fix any incorrect examples (especially any mention of vocab.txt for GPT2 BPE).

3) Ensure all docs pages use the canonical public API names:
- Prefer register_local_model! everywhere
- If register_external_model! exists, mention it only in one short deprecation note (not in examples)

#### D) API reference discoverability (explicit loaders)
1) Ensure the explicit loader functions are:
- exported (or clearly documented if intentionally not exported)
- documented with docstrings that include required files and example calls
2) Update docs API page to explicitly list:
- load_bpe_gpt2
- load_bpe_encoder
- load_wordpiece
- load_sentencepiece
- load_tiktoken
- load_hf_tokenizer_json
- load_tokenizer
- detect_tokenizer_format / detect_tokenizer_files (if public)
- register_local_model! / install_model! / prefetch_models / available_models / describe_model

#### E) Add doc correctness guardrails
1) Add doctests (or doc snippet tests) for the examples in:
- Formats page
- Loading Local page
- Gated Models page
2) Add a lightweight CI check that scans docs markdown for common wrong patterns, for example:
- "format=:bpe_gpt2" together with "vocab.txt" should fail
(This is optional but extremely effective at preventing regressions.)

### 14.4 Suggested implementation order
1) README rewrite + links to docs
2) Update generator to produce README featured list + docs full table
3) Fix docs examples and canonicalize naming
4) Ensure API reference lists the explicit loaders
5) Add doc guardrails in CI
