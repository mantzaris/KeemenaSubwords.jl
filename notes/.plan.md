## 24) Offset boundary validity guarantees by tokenizer family (tests + docs)

### 24.1 Context
KeemenaSubwords offsets are:
- UTF-8 codeunit indices
- 1-based
- half-open spans [start, stop)
- sentinel (0,0) means "no span"
Downstream consumers (KeemenaPreprocessing) can safely consume offsets using:
- `has_nonempty_span`
- `span_codeunits`
- `try_span_substring`
- `is_valid_string_boundary`
- `offsets_are_nonoverlapping`

However:
- Byte-level tokenizers can legitimately return spans that are not valid Julia string boundaries on multibyte Unicode (expected).
- Non-byte-level tokenizers should generally return spans that ARE valid Julia string boundaries (if they do not, it is likely a bug).

### 24.2 Goal
Make offset robustness stronger by:
1) Explicitly documenting the expected "boundary validity" behavior by tokenizer family.
2) Adding regression tests that enforce boundary validity for non-byte-level tokenizers across multibyte inputs.
3) Adding regression tests that enforce boundary validity for byte-level tokenizers on ASCII-only inputs (where boundaries should still be valid).
4) Keeping all tests offline/deterministic and fast (fixtures + corpus subsets only).

### 24.3 Tasks

#### A) Docs: state the boundary validity expectation by family
Update the canonical contract (`notes/OffsetContract.md`) with a short section:

- For non-byte-level tokenizers (WordPiece, SentencePiece, Unigram TSV, classic BPE without byte-fallback), offsets for spanful tokens are expected to land on valid Julia string boundaries.
- For byte-level tokenizers (ByteBPE, HF ByteLevel), offsets may land on non-boundaries for multibyte Unicode; downstream should use `span_codeunits` and treat `try_span_substring == nothing` as "unsafe to slice".

Then run the existing sync tool so `docs/src/normalization_offsets_contract.md` stays in sync.

Acceptance:
- A downstream user understands when `try_span_substring` is expected to succeed or may return `nothing`.

#### B) Tests: enforce boundary validity for non-byte-level tokenizers
Add a new testset, for example:
"Section 24 boundary-valid offsets for string-level tokenizers"

Pick representative tokenizers that are NOT byte-level:
- core WordPiece fixture
- SentencePiece unigram fixture
- Unigram TSV fixture
- HF tokenizer.json WordPiece fixture (non-bytelevel pretokenizer)
- HF tokenizer.json Unigram fixture (if present)
- (optional) classic BPE fixture if it is not byte fallback

For each tokenizer and for each multibyte text case:
- `"cafe\u0301"` (combining mark)
- `"Ã©"`
- `"ðŸ™‚"`
- `"aðŸ™‚b"`

Workflow:
- `normalized = normalize(tok, text)`
- `result = encode_result(tok, normalized; assume_normalized=true, add_special_tokens=false, return_offsets=true, return_masks=true)`
- For each `offset` where `has_nonempty_span(offset)`:
  - assert `is_valid_string_boundary(normalized, start)`
  - assert `is_valid_string_boundary(normalized, stop)`
  - assert `try_span_substring(normalized, offset)` returns a `String` (not `nothing`)
  - assert `Vector{UInt8}(codeunits(sub)) == span_codeunits(normalized, offset)`

Acceptance:
- If a string-level tokenizer ever starts emitting non-boundary spans, tests fail loudly.

#### C) Tests: enforce boundary validity for byte-level tokenizers on ASCII inputs
Byte-level tokenizers may produce non-boundary spans on multibyte Unicode, but on ASCII inputs they should still be boundary-safe.

Add a testset:
"Section 24 boundary-valid offsets for byte-level tokenizers on ASCII"

Representative byte-level tokenizers:
- ByteBPE tokenizer fixture
- HF ByteLevel BPE fixture

For a small ASCII corpus subset (no unicode):
- `"hello"`
- `"hello world"`
- `" hello world"`
- `"hello  world"`
- `"hello\tworld"`
- `"hello\nworld"`

For each offset where `has_nonempty_span(offset)`:
- assert `try_span_substring` returns `String` (not `nothing`)
- assert codeunits match `span_codeunits`

Acceptance:
- Byte-level implementations remain sane on ASCII and regressions are caught.

#### D) Keep tooling checks intact
- After updating `notes/OffsetContract.md`, run:
  - `julia --project=. tools/sync_offset_contract.jl`
  - and ensure CI continues to run `--check`.

### 24.4 Suggested implementation order
1) Update OffsetContract.md text (small doc addition).
2) Add string-level boundary validity testset.
3) Add byte-level ASCII boundary validity testset.
4) Sync docs and run tests + docs build.

