# Goal: Improve BPE training quality and add optional Byte-level BPE training (deterministic, tested)

We already have training scaffolding + BPE training + training tests. Next we will:
1) Improve BPE training quality by making vocab construction merge-consistent (no dropping learned merges).
2) Add Byte-level BPE training that returns ByteBPETokenizer (full byte alphabet by default).
3) Add deterministic tests for bytebpe training, including offsets contract (byte-level allows non-string-boundary spans).
4) Keep pretrained-tokenizer logic unchanged; only additive changes.

## Phase 0: Sanity + small fixes
- [ ] Fix any syntax issue in `test/training/test_bpe_training_offsets.jl` (e.g., `:eos = >` must be `:eos =>`).
- [ ] Run/ensure existing training tests still pass after later modifications.

## Phase 1: BPE trainer quality pass (merge-consistent vocab)
Files: `src/training/bpe_trainer.jl`

- [ ] Change vocabulary construction so it is built in a deterministic merge-driven order:
  - Start with special tokens in canonical order (existing `_SPECIAL_TOKEN_ORDER`).
  - Add required non-special tokens (at minimum: end_of_word_marker).
  - Add base alphabet symbols (characters) deterministically (e.g., sorted).
  - Each time we accept a merge (left,right)->merged, append `merged` to vocab_tokens if not present.
  - Stop merging exactly when `length(vocab_tokens) == vocab_size` or no pair meets `min_frequency`.
- [ ] Ensure artifacts and tokenizer are consistent:
  - `artifacts.vocab_tokens == tokenizer.vocab.id_to_token`
  - `artifacts.merge_pairs` is exactly the merges actually used to build pair_ranks
  - `pair_ranks` is built from `merge_pairs` in order (rank = 1..N).
- [ ] Keep determinism:
  - Tie-break on pair frequency by lexicographic order so the same corpus yields the same merges.
  - Avoid unordered Set iteration affecting output.
- [ ] Update/adjust any tests if needed (prefer property-based checks: roundtrip, determinism, save/reload identity; avoid hardcoding exact merge lists).

## Phase 2: Add Byte-level BPE training
Files to add:
- [ ] `src/training/bytebpe_trainer.jl`

Files to modify:
- [ ] `src/training/Training.jl` (include the new trainer file)
- [ ] `src/training/training_types.jl` (add config + artifacts types)
- [ ] `src/training/training_api.jl` (add public API functions)
- [ ] `src/KeemenaSubwords.jl` (export the new training functions)

### New public API
- [ ] `train_bytebpe(corpus; kwargs...)::ByteBPETokenizer`
- [ ] `train_bytebpe_result(corpus; kwargs...)::TrainingResult{ByteBPETokenizer,ByteBPETrainingConfig,ByteBPETrainingArtifacts}`

### ByteBPETrainingConfig fields (proposed)
- vocab_size::Int
- min_frequency::Int
- special_tokens::Dict{Symbol,String}
- end_of_word_marker::String
- pretokenizer::Union{Nothing,Function}
- include_full_byte_alphabet::Bool = true
- model_name::String
- version::VersionNumber

### ByteBPETrainingArtifacts fields (proposed)
- vocab_tokens::Vector{String}
- merge_pairs::Vector{Tuple{String,String}}
- pair_ranks::Dict{Tuple{String,String},Int}
- word_counts::Dict{String,Int} (optional but useful for debugging)

### Trainer algorithm (Byte-level BPE)
- [ ] Reuse the BPE merge algorithm, but the "symbols" for each word are:
  - For each UTF-8 byte in the word: map byte -> unicode char using the same byte_to_unicode table used by ByteBPETokenizer.
  - Each byte becomes a symbol string: `string(mapped_char)`.
  - Append `end_of_word_marker` at the end of the word.
- [ ] If `include_full_byte_alphabet=true`, initialize the base vocabulary alphabet with ALL 256 byte symbols (string forms) regardless of corpus contents.
- [ ] Validate `vocab_size` can fit:
  - special tokens + marker + (256 if full byte alphabet) + at least 0 merges
  - throw a clear error if too small.
- [ ] Construct the output tokenizer:
  - Build base `BPETokenizer(vocab, pair_ranks, unk_token, end_of_word_marker, base_metadata)`
  - Wrap into `ByteBPETokenizer(base, byte_to_unicode, unicode_to_byte, metadata)`

## Phase 3: Tests for Byte-level BPE training (deterministic, no downloads)
Files to add:
- [ ] `test/training/test_bytebpe_training_e2e.jl`
- [ ] `test/training/test_bytebpe_training_offsets.jl`

Files to modify:
- [ ] `test/training/runtests_training.jl` (include the new tests)

### test_bytebpe_training_e2e.jl requirements
- Train on a tiny in-memory corpus (Vector{String}), include:
  - ASCII + punctuation
  - at least one multibyte character (e.g., "caf√©" or an emoji) to ensure byte handling is real
- Assert end-to-end:
  - `decode(tokenizer, encode(tokenizer, text; add_special_tokens=true)) == text` (choose text with single spaces so decode matches)
  - Save with `save_tokenizer(tokenizer, outdir; format=:bpe)` (current export path)
  - Reload with `load_tokenizer(outdir; format=:bytebpe, unk_token=..., end_of_word_marker=...)`
  - Reloaded tokenizer produces identical tokenize/encode/decode results on the same samples.

### test_bytebpe_training_offsets.jl requirements
- Use the same workflow as other offset tests:
  - `tokenization_text = tokenization_view(tokenizer, clean_text)`
  - `res = encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)`
- Validate contract:
  - `assert_offsets_contract(tokenization_text, res.offsets; require_string_boundaries=false)`
  - `offsets_are_nonoverlapping(res.offsets)`
- (Optional) For at least one non-sentinel offset, verify `span_codeunits(tokenization_text, offset)` is non-empty.

## Phase 4: Offsets support for ByteBPETokenizer (only if tests fail)
- [ ] If encode_result offsets error or are unusable for ByteBPETokenizer, add a specialized `_encode_result_offsets(::ByteBPETokenizer, ...)` method:
  - coordinate unit: UTF-8 codeunits
  - 1-based, half-open spans
  - inserted special tokens (bos/eos) use sentinel `(0,0)`
  - end_of_word_marker tokens can use sentinel `(0,0)` (no-span structural tokens)
  - byte tokens map to spans by consuming byte lengths from the corresponding word region in the normalized tokenization_text
- [ ] Ensure the new offsets method passes Phase 3 tests.
