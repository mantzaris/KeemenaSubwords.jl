## KeemenaSubwords tasks: enable robust normalization/offsets contract for KeemenaPreprocessing integration

### Goal
Ensure KeemenaSubwords can participate in a strict integration contract where:
- KeemenaPreprocessing owns pipeline normalization and canonical text orchestration,
- KeemenaSubwords owns tokenizer intrinsic normalization and token offsets,
- all offsets are consistent and computed against a single `tokenization_text` view.

### Requirements (high-level)
1) Provide tokenizer intrinsic normalization as a first-class, public API.
2) Provide an explicit "assume already normalized" switch so tokenization does not double-normalize.
3) Ensure token-level offsets and masks are available for all supported tokenizer families.
4) Document these semantics clearly for downstream consumers (especially KeemenaPreprocessing).
5) Add tests that lock in the contract and prevent regressions.

### Requirements (concrete API)
#### S1) Public tokenizer normalization API
- Add/confirm:
  - `normalize(tokenizer::AbstractSubwordTokenizer, text::AbstractString)::String`
- Behavior:
  - Applies tokenizer intrinsic normalization only (no pipeline cleaning).
  - For tokenizers without intrinsic normalization, returns text unchanged.
- HF tokenizer.json:
  - Must apply the tokenizer.json normalizer pipeline.

#### S2) Tokenization must support bypassing intrinsic normalization
- Add/confirm keyword:
  - `encode_result(tokenizer, text; assume_normalized::Bool=false, return_offsets::Bool=false, return_masks::Bool=false, add_special_tokens::Bool=true, ...)`
- Contract:
  - If `assume_normalized=true`, encode_result MUST NOT run tokenizer intrinsic normalization.
  - Offsets returned MUST be relative to the provided `text` argument (the canonical tokenization_text).

#### S3) TokenizationResult invariants needed for alignment
- Ensure TokenizationResult contains (or can include):
  - `ids::Vector{Int}`
  - `tokens::Vector{String}` (recommended)
  - `offsets::Union{Nothing, Vector{Tuple{Int,Int}}}` (or your span type)
  - `special_tokens_mask::Union{Nothing, Vector{Int}}`
  - `token_type_ids::Union{Nothing, Vector{Int}}` (pair sequences if supported)
- Define and document offset convention:
  - unit: bytes or chars (pick one and be consistent across package)
  - span: half-open [start, end)
  - special token offsets: documented sentinel (eg (0,0) and mask=1)

#### S4) Ensure coverage across tokenizer families
For each family, the normalize/assume_normalized/offsets pipeline must behave correctly:
- tiktoken
- ByteBPE / GPT2-style BPE
- WordPiece
- SentencePiece Unigram and SentencePiece BPE compatibility
- Unigram TSV
- HF tokenizer.json (BPE, WordPiece, Unigram pipelines)

At minimum:
- normalize works (or is identity)
- encode_result(...; assume_normalized=true, return_offsets=true, return_masks=true) returns offsets/masks with correct lengths and invariants

#### S5) Coordination hooks for KeemenaPreprocessing
Provide simple helper(s) that KeemenaPreprocessing can rely on:
- `tokenization_view(tokenizer, clean_text) = normalize(tokenizer, clean_text)`
- Optionally:
  - `requires_tokenizer_normalization(tokenizer)::Bool`
  - `offsets_coordinate_system()::Symbol` (eg :bytes or :chars)
These are not strictly required, but improve clarity and integration robustness.

### Documentation tasks
- Add a dedicated docs section, eg:
  - "Normalization and offsets contract for KeemenaPreprocessing"
- Must include:
  - definitions of pipeline normalization vs tokenizer intrinsic normalization
  - the canonical integration flow
  - meaning of assume_normalized
  - offsets convention and special token behavior
  - example snippet showing:
    - clean_text -> tokenization_text -> encode_result(...)

### Tests (must-have)
Add tests that explicitly enforce the contract:

1) Normalization bypass correctness
- Choose at least one tokenizer with intrinsic normalization (HF tokenizer.json fixture with Lowercase/NFKC).
- Test:
  - `normalized = normalize(tok, input)`
  - `encode_result(tok, normalized; assume_normalized=true)` equals `encode_result(tok, input; assume_normalized=false)` in ids
  - and offsets returned from the assume_normalized path index into `normalized` correctly.

2) Offsets are relative to input text, not some internal transformed text
- For a tokenizer with intrinsic normalization:
  - If `assume_normalized=true`, offsets should refer to the provided normalized text.
  - Verify monotonicity and bounds.

3) Special tokens masking
- For a tokenizer with TemplateProcessing (HF fixture):
  - Verify special_tokens_mask is 1 for inserted tokens
  - Verify offsets for specials follow the documented sentinel pattern

4) Family coverage smoke
- For representative tokenizers of each family:
  - `normalize` executes (or returns identity)
  - `encode_result(...; assume_normalized=true, return_offsets=true, return_masks=true)` returns arrays of consistent lengths and no crashes.

### Non-goals (for now)
- Mapping offsets back to raw-text coordinates across tokenizer normalization.
  - This belongs in KeemenaPreprocessing by composing its raw->clean mapping with the tokenization_text mapping.
