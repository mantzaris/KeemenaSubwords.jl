# KeemenaSubwords.jl high-level implementation plan

Context:
- KeemenaSubwords.jl is intended as a downstream companion to KeemenaPreprocessing.jl, providing subword tokenization methods for the Keemena* ecosystem. 
- KeemenaPreprocessing (https://github.com/mantzaris/KeemenaPreprocessing.jl/) accepts tokenizers either as built-in symbols or as a custom callable with the shape `tokenizer(text::AbstractString) -> Vector{<:AbstractString}`.
- When `tokenizer_name` is a callable, KeemenaPreprocessing’s level key (the `Symbol` under which the resulting level is stored) is inferred from `Symbol(typeof(fn))`. This matters for how users retrieve the subword stream from a `PreprocessBundle`.

---

## 1) Scope: goals and non-goals

### Goals
- Provide a clean, Julia-native, extensible subword tokenization toolkit that covers the major families:
  - BPE (classic + GPT-2 style byte-level BPE)
  - WordPiece (BERT-style)
  - Unigram LM (SentencePiece-style)
  - SentencePiece format compatibility (load and run models; both BPE and Unigram)
- Make it easy to:
  - load and use existing trained tokenizers (from common file formats)
  - ship a small set of built-in “core” models with the package (no manual downloads by users)
  - let users supply their own tokenizer model files via paths
- Provide a stable callable interface so KeemenaPreprocessing can use these tokenizers directly without tight coupling (aligning with KeemenaPreprocessing’s callable tokenizer contract). 

### Non-goals (for first releases)
- Full parity with every feature of Hugging Face `tokenizers` (e.g., all normalizers, pre-tokenizers, special decoding tricks).
- Re-implementing “tiktoken” binary formats or every LLM tokenizer variant immediately.
- Advanced features like dropout/sampling tokenization in the first milestone (but design for later extension).

---

## 2) Algorithms to implement (what KeemenaSubwords will provide)

This list is intentionally “major algorithms + compatibility” rather than “everything”.

### A. BPE family
1) **Classic BPE (Sennrich-style)**
- Tokenization: apply merges to a pre-tokenized word stream (or optionally on raw text with a default pretokenizer).
- Model assets: `merges` + `vocab` (or internal unified format).
- Use cases: general subword segmentation.

2) **Byte-level BPE (GPT-2 style)**
- Tokenization: byte-to-unicode mapping, pretokenization, then BPE merges.
- Model assets: `vocab.json` + `merges.txt` (common GPT-2 style) or equivalent.
- Use cases: robust handling of arbitrary bytes / unknown unicode, common LLM tokenizers.

### B. WordPiece family
3) **WordPiece (BERT-style greedy longest-match)**
- Tokenization: whitespace pretokenize -> per-word greedy longest-match with continuation prefix (e.g., "##").
- Model assets: `vocab.txt` (one token per line) plus special tokens and continuation marker rules.
- Use cases: BERT-style models and many downstream pipelines.

(Training for WordPiece can be later; tokenization + compatibility is the priority.)

### C. Unigram LM family (SentencePiece-style)
4) **Unigram LM tokenizer**
- Tokenization: Viterbi/DP segmentation using log-probabilities for tokens.
- Model assets: token -> score/prob + special tokens; optionally byte fallback.
- Use cases: SentencePiece unigram models (common in many transformer families).

5) **SentencePiece compatibility layer**
- SentencePiece is both:
  - a model format (proto)
  - and a family (BPE or Unigram)
- Plan: implement loaders for SentencePiece `.model` and expose them as either:
  - `SentencePieceBPE` (internally using your BPE engine)
  - `SentencePieceUnigram` (internally using your Unigram engine)
- Minimal normalization hooks:
  - support basic whitespace marker behavior (the “▁” style boundary marker)
  - optionally allow an external normalizer callable (advanced normalization can be incremental)

---

## 3) Built-in models and model storage strategy

Requirement: “store within the package the mapping files so users do not have to fetch/supply them manually; ship only core models; allow custom file paths”.

### Recommended approach: Julia Artifacts + small in-repo defaults
- Add `Artifacts.toml` to KeemenaSubwords (currently not present in repo root listing) to manage model files in a reproducible way.
- Ship:
  - at least 1 small-but-realistic model per algorithm family:
    - `:core_bpe_en` (BPE merges+vocab)
    - `:core_wordpiece_en` (WordPiece vocab)
    - `:core_sentencepiece_unigram_en` (SentencePiece unigram `.model` OR internal unigram format)
  - keep these “core” artifacts modest in size to avoid bloating installs
- Allow user-supplied models:
  - `load_tokenizer("/path/to/model_dir")`
  - `load_tokenizer("/path/to/spm.model")`
  - `load_tokenizer((vocab_path, merges_path))` for BPE-style formats

### Model registry API concept
- A small registry mapping `Symbol => (format, artifact_name or relative path, metadata)`.
- Users can do:
  - `available_models()`
  - `describe_model(:core_bpe_en)`
  - `load_tokenizer(:core_bpe_en)`

---

## 4) API surface overview

You want two “faces”:
1) API that KeemenaPreprocessing uses
2) Direct API that KeemenaSubwords users use

### 4.1 Integration surface for KeemenaPreprocessing

KeemenaPreprocessing expects:
- either a built-in symbol (`:whitespace`, `:unicode`, `:byte`, `:char`)
- or a callable `tokenizer(text::AbstractString) -> Vector{<:AbstractString}`.

So KeemenaSubwords should provide tokenizers that are directly callable.

Key integration decision:
- Implement tokenizer types as `struct MyTokenizer <: Function ... end`
- Provide `(tok::MyTokenizer)(text::AbstractString)::Vector{String}`

This ensures:
- `tok isa Function` (robust for config validation)
- `typeof(tok)` is a stable named type, so KeemenaPreprocessing stores the level key as `Symbol(typeof(tok))`. 

#### Integration helper functions (KeemenaSubwords)
- `keemena_callable(tokenizer)::Function`
  - returns a `Function` that matches KeemenaPreprocessing contract
  - mostly a no-op if tokenizer is already `<: Function`, but helpful for wrapping configured tokenizers or pipelines
- `level_key(tokenizer)::Symbol`
  - returns `Symbol(typeof(tokenizer))` so users can retrieve the correct level from `PreprocessBundle`

Example usage pattern (document this clearly):
```julia
using KeemenaPreprocessing
using KeemenaSubwords

tokenizer = load_tokenizer(:core_bpe_en)
cfg = PreprocessConfiguration(tokenizer_name = tokenizer)

bundle = preprocess_corpus(docs; config = cfg)

lvl = level_key(tokenizer)  # e.g. :BPETokenizer
subword_corpus = get_corpus(bundle, lvl)
```

Note:
- This aligns with KeemenaPreprocessing’s “callables” philosophy. 
- You should explicitly document the level key behavior, since KeemenaPreprocessing infers it for callables. 

Optional future integration (keep out of v0.1 to reduce complexity):
- Provide a tiny helper in KeemenaPreprocessing docs: “Using KeemenaSubwords tokenizers”
- Consider an optional extension module (Julia package extensions) only if you later want `tokenizer_name = :keemena_bpe` style symbols.

### 4.2 Direct user API (KeemenaSubwords)

Design principles:
- Separate “tokenization into pieces” from “encoding into ids”.
- Keep types explicit and easy to dispatch on.
- Keep file format support behind a small I/O layer.

User-facing API proposal:

#### Core types
- `abstract type AbstractSubwordTokenizer <: Function end`
- Concrete tokenizers:
  - `struct BPETokenizer <: AbstractSubwordTokenizer ... end`
  - `struct ByteBPETokenizer <: AbstractSubwordTokenizer ... end`
  - `struct WordPieceTokenizer <: AbstractSubwordTokenizer ... end`
  - `struct UnigramTokenizer <: AbstractSubwordTokenizer ... end`
  - `struct SentencePieceTokenizer <: AbstractSubwordTokenizer ... end` (wrapper around BPE/Unigram or direct parsed model)

#### Construction / loading
- `load_tokenizer(name::Symbol; kwargs...) -> AbstractSubwordTokenizer`
- `load_tokenizer(path::AbstractString; format::Symbol = :auto, kwargs...) -> AbstractSubwordTokenizer`
- `load_tokenizer(spec::NamedTuple; kwargs...) -> AbstractSubwordTokenizer`
  - e.g. `(format=:bpe_gpt2, vocab="...", merges="...")`

#### Tokenization
- `tokenize(tokenizer::AbstractSubwordTokenizer, text::AbstractString)::Vector{String}`
- Call overload: `(tokenizer)(text::AbstractString)::Vector{String}` delegates to `tokenize`

#### Encoding/decoding (IDs)
- `encode(tokenizer::AbstractSubwordTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}`
- `decode(tokenizer::AbstractSubwordTokenizer, token_ids::AbstractVector{Int})::String`
- `token_to_id(tokenizer, token::AbstractString)::Int`
- `id_to_token(tokenizer, id::Int)::String`

#### Vocabulary / metadata
- `vocab_size(tokenizer)::Int`
- `special_tokens(tokenizer)::Dict{Symbol,String}` or `Dict{Symbol,Int}` depending on design
- `unk_id(tokenizer)::Int`
- `model_info(tokenizer)::NamedTuple` (format, version, normalizer info, etc.)

#### Saving/export
- `save_tokenizer(tokenizer, outdir::AbstractString; format::Symbol = :internal)`
- `export_tokenizer(tokenizer, outdir; format=:bpe_gpt2 | :wordpiece_vocab | :sentencepiece_model | ...)`

#### Training (future-facing but plan now)
- `train_bpe(corpus; vocab_size::Int, min_frequency::Int=2, specials=..., ...) -> BPETokenizer`
- `train_unigram(corpus; vocab_size::Int, seed_size::Int, num_iters::Int, ...) -> UnigramTokenizer`
- `train_wordpiece(...)` (optional later)

---

## 5) Changes to current KeemenaSubwords package layout

From the repo root, you already have a standard skeleton:
- `.github/`, `docs/`, `src/`, `test/`, `Project.toml`, etc. 

Recommended additions/changes:
1) Add `Artifacts.toml` at repo root for built-in models.
2) Add `src/models/` (or `src/registry.jl`) to centralize built-in model registry + artifact resolution.
3) Split `src` into multiple included files so each algorithm is isolated but the package remains simple.
4) Keep one public module `KeemenaSubwords` (no need for deep submodules unless you want strict namespaces).
5) Add `test/golden/` or `test/data/` with small fixtures for deterministic tests (tiny vocab/merges files).

Avoid over-engineering:
- Prefer “flat includes” over many nested modules unless you truly need separate namespaces.

---

## 6) Proposed file/module tree + function signature contracts

Below is a pragmatic structure that stays simple but keeps algorithms isolated.

### 6.1 Tree
```
KeemenaSubwords.jl/
  Artifacts.toml                     # built-in models (core)
  src/
    KeemenaSubwords.jl               # main module, exports, includes
    types.jl                         # AbstractSubwordTokenizer + shared structs
    vocab.jl                         # vocab + special tokens utilities
    normalization.jl                 # optional normalizer hooks (minimal)
    models.jl                        # built-in model registry + artifact resolution
    io.jl                            # auto-detect formats + load/save dispatch
    bpe.jl                           # BPE tokenizer + helpers
    bytebpe.jl                       # GPT-2 style byte-level BPE specifics
    wordpiece.jl                     # WordPiece tokenizer
    unigram.jl                       # Unigram tokenizer (DP/Viterbi)
    sentencepiece.jl                 # SentencePiece model loading + wrappers
    training.jl                      # shared training front-ends (calls into *_train.jl)
    bpe_train.jl                     # BPE training
    unigram_train.jl                 # Unigram training
  test/
    runtests.jl
    fixtures/
      bpe/
      wordpiece/
      sentencepiece/
      unigram/
```

### 6.2 `src/KeemenaSubwords.jl`
```julia
module KeemenaSubwords

# include files in dependency order
include("types.jl")
include("vocab.jl")
include("normalization.jl")
include("models.jl")
include("io.jl")
include("bpe.jl")
include("bytebpe.jl")
include("wordpiece.jl")
include("unigram.jl")
include("sentencepiece.jl")
include("training.jl")
include("bpe_train.jl")
include("unigram_train.jl")

# Exports (keep minimal, stable)
export AbstractSubwordTokenizer,
       BPETokenizer, ByteBPETokenizer, WordPieceTokenizer, UnigramTokenizer, SentencePieceTokenizer,
       load_tokenizer, save_tokenizer, export_tokenizer,
       tokenize, encode, decode,
       level_key, available_models, describe_model

end
```

### 6.3 `src/types.jl`
```julia
"""
Abstract parent type for all tokenizers.

Design requirement:
- Must be callable to satisfy KeemenaPreprocessing's tokenizer contract:
  tokenizer(text::AbstractString) -> Vector{String}
"""
abstract type AbstractSubwordTokenizer <: Function end

"""
Shared vocabulary container.

Suggested invariants:
- ids are 1-based (Julia-friendly)
- special tokens are present (at least :unk)
"""
struct SubwordVocabulary
    id_to_token::Vector{String}
    token_to_id::Dict{String,Int}
    special_token_ids::Dict{Symbol,Int}
end

"""
Standard metadata about a tokenizer model.
"""
struct TokenizerMetadata
    format::Symbol              # :bpe, :bytebpe, :wordpiece, :unigram, :sentencepiece
    model_name::String          # human readable
    version::VersionNumber
    normalizer::Symbol          # :none, :nfkc, etc (minimal for now)
end

"""
Return the level key KeemenaPreprocessing will store when using this tokenizer callable.

KeemenaPreprocessing uses Symbol(typeof(fn)) for Function tokenizers. 
"""
level_key(tokenizer::AbstractSubwordTokenizer)::Symbol = Symbol(typeof(tokenizer))
```

### 6.4 `src/vocab.jl`
```julia
# Contracts:
# - Provide fast token_to_id lookups
# - Provide stable handling of unknown tokens

unk_id(vocab::SubwordVocabulary)::Int
pad_id(vocab::SubwordVocabulary)::Union{Int,Nothing}
bos_id(vocab::SubwordVocabulary)::Union{Int,Nothing}
eos_id(vocab::SubwordVocabulary)::Union{Int,Nothing}

token_to_id(vocab::SubwordVocabulary, token::AbstractString)::Int
id_to_token(vocab::SubwordVocabulary, id::Int)::String

vocab_size(vocab::SubwordVocabulary)::Int

"""
Construct vocabulary from a list of tokens.

- tokens[1] is id 1, etc.
- insert specials first if requested (stable ordering).
"""
build_vocab(tokens::Vector{String};
            special_tokens::Dict{Symbol,String} = Dict(:unk => "<UNK>"))::SubwordVocabulary
```

### 6.5 `src/normalization.jl`
```julia
"""
Normalization hook.
Keep minimal at first: allow user to pass a custom callable.

Return:
- normalized text
"""
normalize_text(text::AbstractString; normalizer::Union{Nothing,Function}=nothing)::String
```

### 6.6 `src/models.jl`
```julia
"""
Return list of built-in models shipped via Artifacts/fixtures.
"""
available_models()::Vector{Symbol}

"""
Return metadata about a built-in model.
"""
describe_model(name::Symbol)::NamedTuple

"""
Resolve a built-in model to an on-disk path (artifact path).
"""
model_path(name::Symbol)::String
```

### 6.7 `src/io.jl`
```julia
"""
Load tokenizer from a built-in model name.
"""
load_tokenizer(name::Symbol; kwargs...)::AbstractSubwordTokenizer

"""
Load tokenizer from file path(s).

format = :auto attempts to detect based on extension or directory contents:
- SentencePiece .model
- WordPiece vocab.txt
- GPT2 BPE vocab.json + merges.txt
- internal KeemenaSubwords format (e.g. tokenizer.json)
"""
load_tokenizer(path::AbstractString; format::Symbol = :auto, kwargs...)::AbstractSubwordTokenizer

"""
Save tokenizer to an internal canonical format for round-tripping.
"""
save_tokenizer(tokenizer::AbstractSubwordTokenizer, outdir::AbstractString)::Nothing

"""
Export tokenizer to external formats.
"""
export_tokenizer(tokenizer::AbstractSubwordTokenizer, outdir::AbstractString; format::Symbol)::Nothing
```

### 6.8 `src/bpe.jl`
```julia
struct BPETokenizer <: AbstractSubwordTokenizer
    vocab::SubwordVocabulary
    # pair_ranks maps (a,b) => merge rank (lower is earlier)
    pair_ranks::Dict{Tuple{String,String},Int}
    metadata::TokenizerMetadata
    # configuration knobs
    end_of_word_marker::Union{Nothing,String}   # optional, for classic BPE variants
end

"""
Tokenize a string into subword pieces (strings).

Contract:
- Return Vector{String}
- Deterministic given same tokenizer + input
"""
tokenize(tokenizer::BPETokenizer, text::AbstractString)::Vector{String}

# Make it callable for KeemenaPreprocessing integration.
(tokenizer::BPETokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::BPETokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::BPETokenizer, ids::AbstractVector{Int})::String
```

### 6.9 `src/bytebpe.jl`
```julia
struct ByteBPETokenizer <: AbstractSubwordTokenizer
    base::BPETokenizer
    # byte encoder/decoder tables
    byte_to_unicode::Vector{Char}     # length 256
    unicode_to_byte::Dict{Char,UInt8}
    metadata::TokenizerMetadata
end

tokenize(tokenizer::ByteBPETokenizer, text::AbstractString)::Vector{String}
(tokenizer::ByteBPETokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::ByteBPETokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::ByteBPETokenizer, ids::AbstractVector{Int})::String
```

### 6.10 `src/wordpiece.jl`
```julia
struct WordPieceTokenizer <: AbstractSubwordTokenizer
    vocab::SubwordVocabulary
    continuation_prefix::String   # usually "##"
    metadata::TokenizerMetadata
end

"""
Greedy longest-match WordPiece tokenization.

Contract:
- If a word cannot be segmented, emit unk token
- Keep continuation_prefix rules consistent
"""
tokenize(tokenizer::WordPieceTokenizer, text::AbstractString)::Vector{String}
(tokenizer::WordPieceTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::WordPieceTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::WordPieceTokenizer, ids::AbstractVector{Int})::String
```

### 6.11 `src/unigram.jl`
```julia
struct UnigramTokenizer <: AbstractSubwordTokenizer
    vocab::SubwordVocabulary
    # token log-probabilities (aligned with vocab ids)
    logprobs::Vector{Float64}
    metadata::TokenizerMetadata
end

"""
Viterbi segmentation using log-probs.

Contract:
- Deterministic best-path segmentation
- If no path exists, fall back to <UNK> (or optional byte fallback later)
"""
tokenize(tokenizer::UnigramTokenizer, text::AbstractString)::Vector{String}
(tokenizer::UnigramTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::UnigramTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::UnigramTokenizer, ids::AbstractVector{Int})::String
```

### 6.12 `src/sentencepiece.jl`
```julia
"""
SentencePiece wrapper.

Design:
- parse SentencePiece .model (proto) into either:
  - UnigramTokenizer
  - BPETokenizer
and provide SentencePiece-specific whitespace marker behavior as needed.
"""
struct SentencePieceTokenizer <: AbstractSubwordTokenizer
    inner::AbstractSubwordTokenizer   # BPETokenizer or UnigramTokenizer
    whitespace_marker::String         # typically "▁"
    metadata::TokenizerMetadata
end

tokenize(tokenizer::SentencePieceTokenizer, text::AbstractString)::Vector{String}
(tokenizer::SentencePieceTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)

encode(tokenizer::SentencePieceTokenizer, text::AbstractString; add_special_tokens::Bool=false)::Vector{Int}
decode(tokenizer::SentencePieceTokenizer, ids::AbstractVector{Int})::String
```

### 6.13 `src/training.jl`
```julia
"""
High-level training entry points.

These should accept either:
- Vector{String} documents
- iterator of strings
- (future) file paths

Keep memory predictable where possible.
"""

train_bpe(corpus;
          vocab_size::Int,
          min_frequency::Int = 2,
          special_tokens::Dict{Symbol,String} = Dict(:unk=>"<UNK>", :pad=>"<PAD>"),
          pretokenizer::Union{Nothing,Function} = nothing)::BPETokenizer

train_unigram(corpus;
              vocab_size::Int,
              seed_size::Int = 200_000,
              num_iters::Int = 5,
              special_tokens::Dict{Symbol,String} = Dict(:unk=>"<UNK>", :pad=>"<PAD>"),
              pretokenizer::Union{Nothing,Function} = nothing)::UnigramTokenizer
```

### 6.14 `src/bpe_train.jl`
```julia
"""
Implement BPE training.

Contract:
- Build initial symbol vocabulary (characters or byte symbols)
- Iteratively merge most frequent pairs until vocab_size reached
- Output merges + vocab

Implementation note:
- Use a priority queue for pair counts OR a batched recomputation strategy.
- Keep it correct first; optimize later.
"""
```

### 6.15 `src/unigram_train.jl`
```julia
"""
Implement Unigram LM training (SentencePiece-style).

Contract:
- Build seed vocab (frequent substrings)
- Run EM iterations to estimate token probabilities
- Prune to target vocab_size
- Output vocab + logprobs
"""
```

---

## 7) Implementation order (recommended)

This is the shortest path to a usable package with good ergonomics.

### Phase 0: groundwork (small, fast)
1) `types.jl` + `vocab.jl`
- Define `AbstractSubwordTokenizer <: Function`
- Define `SubwordVocabulary`
- Define `level_key(tokenizer)` helper

2) `models.jl` + `Artifacts.toml`
- Add a minimal registry and stub built-in model hooks (even if models come later)

3) `io.jl` (skeleton)
- Define `load_tokenizer(::Symbol)` and `load_tokenizer(::String)` API
- Implement format dispatch stubs with informative errors

### Phase 1: tokenization (deliver value early)
4) WordPiece tokenization + loader (`wordpiece.jl`)
- Load `vocab.txt`
- Implement greedy longest-match tokenization
- Tests with tiny vocab fixture

5) BPE tokenization + loader (`bpe.jl`)
- Support a simple merges+vocab format first (internal)
- Add GPT-2 style loader later if desired

6) Byte-level BPE (`bytebpe.jl`)
- Implement byte<->unicode mapping
- Connect to BPE engine
- Tests with tiny fixture

7) Unigram tokenization (`unigram.jl`)
- Implement DP/Viterbi using token logprobs
- Tests with tiny fixture

8) SentencePiece loading wrapper (`sentencepiece.jl`)
- Parse `.model` (ProtoBuf) OR initially support exporting/importing a simplified JSON representation
- Wrap into `SentencePieceTokenizer(inner=...)`

### Phase 2: built-in core models (user experience)
9) Add 2-3 built-in “core” models via artifacts
- Ensure `load_tokenizer(:core_...)` works out of the box

10) Add documentation pages:
- “Using KeemenaSubwords with KeemenaPreprocessing”
- “Loading tokenizers from files”
- “Built-in models”

### Phase 3: training (bigger effort, but now on stable foundation)
11) BPE training (`bpe_train.jl`)
- Start with a straightforward implementation
- Add streaming-friendly variants later

12) Unigram training (`unigram_train.jl`)
- Implement seed vocab generation + EM + pruning

13) Save/export formats
- Ensure `save_tokenizer` and `export_tokenizer` produce files users can load later

### Phase 4: polish and extensibility
14) Performance passes (without changing API)
- caching, faster pair-rank lookup, tries for WordPiece, optimized DP for Unigram

15) Optional advanced features
- dropout / sampling
- byte fallback for unigram
- additional format adapters

---

## 8) Minimal “contract” summary for extensibility

If you want to add a new tokenizer later, the only hard contract should be:

- Define `struct NewTokenizer <: AbstractSubwordTokenizer ... end`
- Implement:
  - `tokenize(tokenizer::NewTokenizer, text::AbstractString)::Vector{String}`
  - `encode(tokenizer::NewTokenizer, text::AbstractString; add_special_tokens=false)::Vector{Int}` (optional early)
  - `decode(tokenizer::NewTokenizer, ids)::String` (optional early)
- Make it callable:
  - `(tokenizer::NewTokenizer)(text::AbstractString)::Vector{String} = tokenize(tokenizer, text)`
- Provide I/O:
  - `load_tokenizer(path; format=:new_format)` and/or registry entries
- Provide `level_key(tokenizer)` (already generic) so KeemenaPreprocessing users can retrieve the right level.

This keeps the package simple while remaining extensible.

---

## 9) Add downloadable pretrained tokenizer assets (tiktoken + public baselines) and register them as built-in models

### 9.1 Goal
Add a small set of well-known pretrained tokenizer files (from reputable upstream sources) so users can do:

- `load_tokenizer(:tiktoken_o200k_base)` / `load_tokenizer(:tiktoken_cl100k_base)`
- `load_tokenizer(:openai_gpt2_bpe)`
- `load_tokenizer(:bert_base_uncased_wordpiece)`
- `load_tokenizer(:t5_small_sentencepiece_unigram)`

without the user manually downloading vocab/merges/model files.

This should integrate cleanly with the existing model registry UX:
- `available_models()` lists them
- `describe_model(:key)` explains them
- `load_tokenizer(:key)` resolves files and constructs the correct tokenizer instance

### 9.2 Where to place the files (layout decision)
Do NOT commit large tokenizer assets directly into `models/` in the git repo.

Instead:
- Keep only metadata and registry code in the repo.
- Store external pretrained files as Julia Artifacts (lazy downloads) or in a user cache directory outside the package source tree.

Rationale:
- Keeps the package lightweight
- Avoids modifying the installed package directory
- Allows optional prefetch for offline environments

### 9.3 Directory structure for built-in assets (inside the artifact or cache root)
Organize by parser/format first, then by model key:

- tiktoken/
  - o200k_base/
    - o200k_base.tiktoken
  - cl100k_base/
    - cl100k_base.tiktoken
  - (optional) r50k_base/
    - r50k_base.tiktoken
  - (optional) p50k_base/
    - p50k_base.tiktoken

- bpe/
  - openai_gpt2/
    - encoder.json
    - vocab.bpe

- wordpiece/
  - bert_base_uncased/
    - vocab.txt

- sentencepiece/
  - t5_small/
    - spiece.model

This mirrors your parser types and makes it easy to add more models later without changing code structure.

### 9.4 Built-in model keys and descriptions (Symbols as stable user-facing IDs)
Add these additional built-in keys:

Tiktoken (OpenAI hosted encodings):
- `:tiktoken_o200k_base`  - OpenAI tiktoken encoding file (o200k_base.tiktoken)
- `:tiktoken_cl100k_base` - OpenAI tiktoken encoding file (cl100k_base.tiktoken)
- Optional legacy encodings:
  - `:tiktoken_r50k_base`
  - `:tiktoken_p50k_base`

Classic / GPT-2 style byte-level BPE:
- `:openai_gpt2_bpe` - GPT-2 style BPE (encoder.json + vocab.bpe)

WordPiece:
- `:bert_base_uncased_wordpiece` - BERT base uncased WordPiece vocab.txt

SentencePiece (Unigram LM):
- `:t5_small_sentencepiece_unigram` - T5-small SentencePiece model (spiece.model)

Each key should have:
- a one-line human description
- a format tag (`:tiktoken`, `:bpe_gpt2`, `:wordpiece_vocab`, `:sentencepiece_model`)
- resolved absolute paths to the required files
- (optional but recommended) provenance info: upstream URL(s), license string, version/date

### 9.5 Reputable upstream download sources (authoritative URLs)
OpenAI public tiktoken encodings:
- https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken
- https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken
- (optional) https://openaipublic.blob.core.windows.net/encodings/r50k_base.tiktoken
- (optional) https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken

OpenAI hosted GPT-2 BPE assets:
- https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json
- https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe

Hugging Face hosted WordPiece vocab:
- https://huggingface.co/bert-base-uncased/raw/main/vocab.txt

Hugging Face hosted T5 SentencePiece model:
- https://huggingface.co/google-t5/t5-small/resolve/main/spiece.model

### 9.6 Registry constants and public API behavior
Implementation intent (keep it simple, match existing style):

- Extend the existing model registry so `available_models()` includes the new keys.
- Extend `describe_model(key)` so it prints:
  - format/type
  - file list (relative + resolved absolute)
  - a short description + provenance URL(s)

- Extend `load_tokenizer(key::Symbol)` so it:
  1) recognizes these new keys
  2) ensures files exist (artifact resolve / cached download)
  3) chooses the correct parser based on `format`
  4) returns a tokenizer instance that matches your existing `tokenize(...)` API

Add a convenience function:
- `prefetch_models(keys=available_models(); force=false)` so users can download all built-ins once (useful for offline environments later).

### 9.7 Minimal acceptance checks
- For each new built-in key, add a small smoke test:
  - load it
  - run `tokenize` on a simple string
  - ensure result is non-empty and stable
  - if decode exists, check roundtrip on a safe sample
Gate any network downloads behind an env var if needed, but prefer artifacts so CI can be deterministic.


## 10) LLM-oriented built-in model coverage review and next recommendations

### What is already good (keep it)
- The public API surface is already aligned with what users expect:
  - `load_tokenizer(::Symbol)` for built-ins
  - `load_tokenizer(path_or_spec; format=...)` for user-supplied assets
  - `available_models()`, `describe_model(key)`, `model_path(key)`
  - `prefetch_models(keys)` so users can fetch/cache what they need up front
- The built-in selection already covers the major file families used in practice:
  - tiktoken encodings (OpenAI-style)
  - GPT-2 / RoBERTa style BPE (vocab.json + merges.txt, sometimes encoder.json + vocab.bpe)
  - WordPiece (vocab.txt)
  - SentencePiece Unigram (spiece.model)
  - SentencePiece BPE (sentencepiece.bpe.model, tokenizer.model(.v3), etc)
- The KeemenaPreprocessing integration story is already clean: "tokenizer is callable", and the corpus is stored under a level key.

### What is incomplete or risky for this stage
- README vs docs drift:
  - The GitHub README still reads like the older scope (core toy models and "training not implemented"),
    while the docs show a broader set of built-ins and prefetching.
  - Action: update README to match the docs and the true built-in set.
- Built-in model positioning:
  - Users will judge "coverage" mostly by whether the package includes tokenizers for the LLM families they actually use.
  - Right now, the biggest missing family perception-wise is Llama and Qwen.
- License/gating pitfalls:
  - Some popular tokenizers (notably some Llama assets) may be gated or have redistribution limits.
  - Action: draw a strict line between:
    - "shipped built-ins" (redistribution-safe)
    - "supported but not shipped" (user must provide files)

### Recommended additions to improve "one stop shop" perception (without becoming exhaustive)

#### A) Add as shipped built-ins (high value, common in LLM work)
1) Qwen2.5 tokenizer (BPE)
   - Files: vocab.json + merges.txt (+ tokenizer.json if you want fast loading / exact parity)
   - Add one representative key (example):
     - `:qwen2_5_bpe` or `:qwen2_5_7b_instruct_bpe`
   - This single addition covers a large share of modern open LLM usage patterns.

2) Mistral Tekken tokenizer (tiktoken-based), if redistributable
   - Add one key (example):
     - `:mistral_tekken_tiktoken`
   - This matters because newer Mistral tokenization can be tiktoken-based (separate from SentencePiece).

#### B) Support explicitly, but do not ship assets (to avoid license/gating trouble)
3) Llama 2 style SentencePiece (tokenizer.model)
   - Add a documented "supported external spec" path:
     - `load_tokenizer("/path/to/tokenizer.model")` or `load_tokenizer("/path/to/llama2_dir")`
   - Provide a helper doc page: "How to point KeemenaSubwords at Llama tokenizers".

4) Llama 3 style tiktoken-model file
   - Add a documented external spec:
     - `load_tokenizer("/path/to/llama3_*.tiktoken"; format=:tiktoken)`
   - Do NOT ship by default unless you have a redistribution-safe source.

### Make the built-in registry feel professional (small changes, big UX win)
- Ensure every built-in has metadata fields exposed via `describe_model`:
  - `key::Symbol`
  - `family` (openai, mistral, qwen, llama, bert, t5, roberta, etc)
  - `format` (tiktoken, bpe_gpt2, bytebpe, wordpiece, sentencepiece_unigram, sentencepiece_bpe)
  - `files` (expected filenames)
  - `upstream_source` (URL string)
  - `license` (short string)
  - `description` (1-2 lines)
- Add simple filters:
  - `available_models(; format=nothing, family=nothing)`
- Add a "recommended_defaults_for_llms()" convenience list:
  - returns a short list of keys a user can prefetch for typical LLM usage.

### Directory layout recommendation (keep it consistent and predictable)
- Prefer:
  - `models/tiktoken/<model_key>/...`
  - `models/bpe/<model_key>/...`
  - `models/wordpiece/<model_key>/...`
  - `models/sentencepiece/<model_key>/...`
- Keep "in-repo fallback" files minimal (only tiny demo/core models).
- Use Artifacts for real model assets to avoid bloating the git repo.
- Document the mapping from model key -> artifact name -> files.

### Test expectations (minimal but confidence-building)
- Add one test per built-in model key:
  - load succeeds
  - encode/decode round-trip for a small string does not throw
  - (optional) token ids match a small golden vector for 1-2 flagship models

### Implementation order (suggested)
1) Update README to reflect current API + built-ins (fast win).
2) Add Qwen2.5 built-in (BPE) + tests + docs.
3) Add Mistral Tekken built-in (tiktoken) if redistributable + tests + docs.
4) Add explicit "supported external specs" docs for Llama 2 and Llama 3 (no shipping).
5) Add `available_models` filters + "recommended_defaults_for_llms()".


## 11) Close the remaining "no Python needed" gaps for LLM tokenization

### 11.1 Objective
Make KeemenaSubwords feel like a one-stop Julia solution for most LLM tokenization work by addressing the last big gaps:
- Support Hugging Face `tokenizer.json` (json-only tokenizers) in pure Julia so users do not need Python.
- Provide a convenient path for LLaMA-family tokenizers (without bundling or redistributing gated assets).
- Add one more flagship BPE LLM tokenizer as a built-in to make the coverage feel real.
- Optionally add 1 more WordPiece tokenizer for multilingual coverage (small cost, improves perception).

### 11.2 Deliverables
- A new loader: `load_tokenizer(path; format=:hf_tokenizer_json)` that loads Hugging Face `tokenizer.json` directly.
- Directory autodetection: `load_tokenizer(dir)` prefers `tokenizer.json` when present (unless the user forces another format).
- A "Hugging Face hub download" utility (opt-in) to fetch tokenizer assets into the user cache using public URLs and optionally an auth token.
- New built-in registry keys:
  - `:qwen2_5_bpe` (flagship BPE LLM)
  - `:bert_base_multilingual_cased_wordpiece` (extra WordPiece)
- Documentation pages explaining:
  - what is shipped vs artifact-downloaded vs user-supplied / gated
  - how to use tokenizer.json and HF downloads
  - how to use LLaMA tokenizers via user-supplied paths or optional download
- Tests covering:
  - tokenizer.json loader
  - the new built-in keys
  - the HF download helper (network-gated or mocked)

### 11.3 Work plan

#### 11.3.1 Add Hugging Face tokenizer.json support (no Python)
Goal: Parse and execute the common Hugging Face tokenization pipeline stored in `tokenizer.json`.

Steps:
1) Add a new internal module, example:
   - `src/huggingface_json/`
     - `hf_json_types.jl` (structs for parsed JSON)
     - `hf_json_parse.jl` (JSON3 parsing and validation)
     - `hf_json_pipeline.jl` (normalizer, pretokenizer, postprocessor, decoder)
     - `hf_json_loader.jl` (public load function + integration with `load_tokenizer`)
2) Implement a minimal but high-value supported matrix first, with clear errors for unsupported parts.

Minimum supported "model" types:
- BPE (most common for LLMs in tokenizer.json)
- WordPiece (for BERT ecosystem)
- Unigram (less common in tokenizer.json, but support if feasible)

Minimum supported pipeline components:
- Normalizers:
  - Lowercase
  - NFKC (or a general unicode normalization wrapper)
  - Sequence (compose multiple)
- Pre-tokenizers:
  - ByteLevel
  - Whitespace / WhitespaceSplit
  - Metaspace
  - Sequence (compose multiple)
  - Split with regex (common in many LLM tokenizer.json files)
- Post-processors:
  - TemplateProcessing (special tokens insertion)
  - Sequence (compose multiple)
- Decoders:
  - ByteLevel
  - WordPiece
  - Metaspace
  - Sequence (compose multiple)

3) Make unsupported JSON components fail fast with a good message:
- "Unsupported pre_tokenizer type: X"
- "Unsupported post_processor type: X"
- Include the json path to the failing block and suggest a workaround:
  - "Use vocab.json + merges.txt if present"
  - "Export a simplified tokenizer.json"

4) Ensure the resulting tokenizer integrates with the existing KeemenaSubwords interface:
- returns `AbstractSubwordTokenizer <: Function`
- supports `tokenize`, `encode`, and `decode`
- preserves special tokens and added tokens from tokenizer.json

5) Add `export_tokenizer(tokenizer; format=:hf_tokenizer_json)` as a later optional step.
For stage 11, prioritize loading (inbound compatibility) over exporting.

Acceptance tests:
- Load tokenizer.json from a known repo fixture and run:
  - tokenize("Hello world")
  - encode/decode round-trip smoke tests
  - verify special tokens insertion works for at least one TemplateProcessing example

#### 11.3.2 Add flagship BPE LLM built-in: Qwen2.5
Goal: Provide a modern, popular BPE LLM tokenizer out of the box.

Steps:
1) Choose one Qwen2.5 model repo that is permissively licensed and stable.
Recommended: Qwen2.5-1.5B (Apache-2.0) since the tokenizer files are small and widely representative.
2) Artifact contents for `:qwen2_5_bpe` should include:
- `vocab.json`
- `merges.txt`
- `tokenizer.json`
- `tokenizer_config.json`
- `special_tokens_map.json` (if present and useful)
3) Add registry entry:
- key: `:qwen2_5_bpe`
- format: `:hf_tokenizer_json` (preferred), with fallback `:bpe_gpt2` using merges/vocab
- description: "Qwen2.5 BPE tokenizer (Hugging Face)."
- include upstream repo id and pinned revision in metadata
4) Add tests:
- ensure `load_tokenizer(:qwen2_5_bpe)` works after `prefetch_models([:qwen2_5_bpe])`
- ensure stable tokenization for 2-3 short strings (golden outputs)

#### 11.3.3 LLaMA family convenience without redistribution
Goal: Make LLaMA tokenizers easy to use, but do not ship or redistribute gated assets.

Key idea:
- Provide "supported external specs" and an opt-in download helper for users who already have access and accept the license.

Steps:
1) Document supported file types:
- LLaMA 2 style: SentencePiece BPE `.model` file (user supplies path)
- LLaMA 3 style: tiktoken model file (user supplies path)
2) Add an opt-in HF hub download helper (not a built-in artifact):
- `download_hf_files(repo_id, filenames; revision, outdir, token=nothing)`
- If `token` is provided, pass it as Authorization header to support gated repos
- Store into KeemenaSubwords cache path (not into the package source tree)
3) Add `register_local_model!(key::Symbol, path_or_dir; format, description)`:
- persists a small local registry file in the user cache (not in git)
- lets users do:
  - register_local_model!(:llama3, "/path/to/tokenizer.model"; format=:tiktoken)
  - load_tokenizer(:llama3)
4) Add docs:
- "We do not distribute LLaMA tokenizer files; you must have access yourself."
- show a simple workflow:
  - user downloads from their authorized source
  - user registers local model
  - user loads it by Symbol key

Acceptance tests:
- Unit test for `register_local_model!` and `load_tokenizer(:custom_key)` using a small local fixture model (not LLaMA).

#### 11.3.4 Add one extra WordPiece built-in (optional but improves completeness)
Goal: Provide a second WordPiece option that is genuinely useful.

Recommendation:
- `:bert_base_multilingual_cased_wordpiece`
- Artifact includes only `vocab.txt`

Notes:
- WordPiece is less common for LLMs, but it is still important for classic NLP and multilingual pipelines.
- Adding one multilingual WordPiece model improves coverage perception at low cost.

#### 11.3.5 UX and documentation polish for "one stop shop"
- Add `available_models(; format=nothing, family=nothing, shipped=nothing)` filters.
- Add `recommended_defaults_for_llms()` returning a short list:
  - OpenAI tiktoken encodings
  - Mistral sentencepiece
  - Qwen2.5 BPE
  - Phi-2 BPE
  - RoBERTa BPE
- Ensure `describe_model(key)` prints:
  - format
  - whether it is artifact-backed or repo-shipped or user-registered
  - expected filenames
  - short description

### 11.4 Suggested implementation order
1) Hugging Face tokenizer.json loader (core minimal matrix)
2) Add `:qwen2_5_bpe` built-in artifact + tests
3) Add local model registry + HF download helper (opt-in)
4) Add multilingual WordPiece built-in
5) Docs + UX filters + recommended defaults


## 12) Eliminate README/docs drift, finish WordPiece artifact coverage, and add LLaMA "one command install" convenience (without redistributing gated files)

### 12.1 Objective
Make the package feel finished and trustworthy by:
- Ensuring README and Documenter docs are generated from the same source of truth (no drift).
- Ensuring WordPiece coverage is fully real (artifact is actually downloadable and prefetchable), not just present in docs.
- Providing LLaMA-family convenience that feels like built-ins (automatic download + key-based loading) while respecting gated licensing and not redistributing files.

### 12.2 What "done" looks like
- The README built-in key list exactly matches `available_models()` output and includes WordPiece multilingual key.
- Docs examples use the correct function name(s) for local registry (`register_local_model!` if that is the canonical API).
- WordPiece built-ins:
  - `:bert_base_uncased_wordpiece`
  - `:bert_base_multilingual_cased_wordpiece`
  are both artifact-backed and `prefetch_models([...])` succeeds.
- LLaMA convenience:
  - Users can run one command like `install_model!(:llama3_8b_instruct; token=ENV["HF_TOKEN"])`
  - After install: `load_tokenizer(:llama3_8b_instruct)` works without specifying file paths.
  - The package does not ship LLaMA files in Artifacts.toml and does not redistribute them.

### 12.3 Tasks

#### A) Single source of truth for model inventory (stop drift)
1) Create one canonical registry table in code (if not already):
- Keep all model keys and metadata in one place (example file: `src/model_registry.jl`).
- Include fields used everywhere:
  - `key`, `format`, `family`, `license`, `description`
  - `expected_files`
  - `distribution` (one of: `:shipped`, `:artifact_public`, `:installable_gated`, `:user_local`)
  - `upstream_repo`, `upstream_ref`, `upstream_files`

2) Auto-generate README models section
- Add markers to README.md:
  - `<!-- KEEMENA_MODELS_START -->`
  - `<!-- KEEMENA_MODELS_END -->`
- Add `tools/sync_readme_models.jl` that:
  - loads KeemenaSubwords
  - queries registry and renders a markdown table grouped by format/family
  - replaces the marked section
- Add a CI check (or pre-commit script) that fails if the README is out of date.

3) Auto-generate docs "Built-In Models" content
- Either:
  - render the same table in Documenter by calling a function that returns markdown, or
  - include a generated markdown file produced by a doc build step.
Goal: docs and README are always consistent.

#### B) Fix naming drift in external/local registry API
1) Decide canonical API name:
- If `register_local_model!` is the intended public API, keep it and:
  - add `register_external_model!` as a deprecated alias (with a deprecation warning)
  - update docs examples to use `register_local_model!`
- If `register_external_model!` is the intended name, then:
  - update notes/docs to stop mentioning `register_local_model!`
  - ensure it persists to cache if that is implemented

2) Update docs and README examples to use the canonical name.

#### C) Ensure WordPiece artifact coverage is actually complete
1) Make `:bert_base_multilingual_cased_wordpiece` a first-class built-in everywhere:
- Add it to the README built-in list (via the auto-generation above).
- Ensure `describe_model(:bert_base_multilingual_cased_wordpiece)` works and prints correct provenance.

2) Ensure artifact binding exists and is stable
- Confirm `Artifacts.toml` includes the multilingual vocab artifact with:
  - pinned upstream revision (avoid floating "main" if possible)
  - sha256 verification
- Ensure `prefetch_models([:bert_base_multilingual_cased_wordpiece])` succeeds.

3) Optional: add one more WordPiece for completeness (only if you want)
- `:bert_base_cased_wordpiece` (English cased) as a small, cheap addition.
- Do not add more WordPiece beyond this; it becomes redundant noise.

4) Tests
- Add a test that prefetches multilingual WordPiece when network is allowed (ENV gated),
  and always tests that the registry entry exists and expected_files are correct.

#### D) LLaMA convenience without redistributing gated assets
Important constraint:
- Do not include Meta LLaMA tokenizer files as public package artifacts, and do not host them yourself.
- Provide an install workflow that uses the user's credentials or signed URL, stores into cache, and registers locally.

1) Add "installable gated models" registry entries
- Add entries with `distribution=:installable_gated`, for example:
  - `:llama2_tokenizer` (SentencePiece)
  - `:llama3_8b_tokenizer` (prefer tokenizer.json if available, otherwise documented alternatives)
- These are not "shipped" and not in Artifacts.toml, but they appear in:
  - `available_models(distribution=:installable_gated)` or similar.

2) Implement installer API
- `install_model!(key::Symbol; token=nothing, revision="main", force=false)`
  - Looks up registry entry
  - Calls `download_hf_files(...)` (already exists) for the listed filenames
  - Stores them under `KEEMENA_SUBWORDS_CACHE_DIR/install/<key>/...`
  - Calls `register_local_model!(key, installed_dir; format=..., family=:llama, description=...)`
- Optional: add `install_llama2_tokenizer!` and `install_llama3_tokenizer!` wrappers for discoverability.

3) Loading behavior after install
- `load_tokenizer(:llama3_8b_tokenizer)` should:
  - first check user local registry key (installed)
  - then fall back to regular built-ins
  - never silently attempt gated downloads without explicit `install_model!`

4) Docs
- Add a clear docs section:
  - "LLaMA tokenizers are gated; you must accept Meta license and have access"
  - "Use install_model! with HF token or use manual path loading"

5) Tests
- Add a unit test for install flow without network:
  - when token is missing, ensure a clear error message that explains how to proceed
- Add optional network test (ENV gated):
  - attempts to download only tokenizer files for a gated model when token is provided

### 12.4 Suggested implementation order
1) Auto-generate README and docs models tables from registry (kills drift fast).
2) Fix registry helper naming drift (register_local_model! vs register_external_model!).
3) Make multilingual WordPiece artifact fully solid + tests.
4) Add LLaMA install_model! workflow + docs.
5) Optional: add :bert_base_cased_wordpiece.


## 13) Documentation consolidation and path-based loader contracts for all tokenizer families

### 13.1 Objective
Make KeemenaSubwords feel "complete and professional" by:
1) Moving detailed inventories and usage guidance out of README into docs pages (README becomes a clean quickstart).
2) Making "bring your own tokenizer files" a first-class, clearly documented workflow across all families:
   - BPE (GPT-2/RoBERTa style)
   - Byte-level BPE
   - WordPiece
   - SentencePiece (Unigram and BPE models)
   - tiktoken (including LLaMA-3 style files that may be named .model)
   - Hugging Face tokenizer.json
3) Ensuring every loader has explicit, predictable function signatures and strong validation:
   - correct file expectations
   - clear errors when files are missing or the wrong format
   - robust auto-detection that can be overridden

### 13.2 Deliverables
- README reduced to: install, quickstart, 4-6 "featured" built-ins, links to docs.
- New or expanded docs pages:
  - "Tokenizer formats and required files" (single reference page)
  - "Loading tokenizers from local paths" (examples for each family)
  - "LLM cookbook" (OpenAI, Mistral, Qwen, LLaMA workflows)
  - "Installable gated models" (how install_model! works and why)
  - "Troubleshooting" (most common file/layout mistakes)
- Explicit loader contracts with convenience constructors for each format.
- Strong format detection and content sniffing for ambiguous cases (notably .model being SentencePiece binary vs tiktoken text).
- Tests that enforce the contracts and catch regressions in detection and error messages.

### 13.3 Tasks

#### A) Move depth from README to docs (reduce drift risk and improve clarity)
1) README.md should contain only:
   - 1 paragraph overview
   - install instructions
   - minimal quickstart examples:
     - load built-in and tokenize
     - prefetch_models
     - load from local path (one example)
   - a short "Featured models" list (not the full table)
   - links to docs pages: Models, Loading, Formats, LLaMA install, Troubleshooting

2) Docs pages to add or expand:
   - docs/src/formats.md
     - a table: format -> accepted inputs -> required files -> typical sources -> recommended loader call
   - docs/src/loading_local.md
     - "Bring your own files" recipes by family (BPE, WordPiece, SentencePiece, tiktoken, tokenizer.json)
   - docs/src/llm_cookbook.md
     - OpenAI tiktoken, Mistral SentencePiece, Qwen tokenizer.json, LLaMA install + local usage
   - docs/src/gated_models.md
     - explain install_model! and what it does/does not do (no redistribution)
   - docs/src/troubleshooting.md
     - "why did auto-detect pick the wrong format?"
     - "missing merges.txt"
     - "tokenizer.model is not SentencePiece, it is tiktoken text"

3) Keep the full model inventory table in docs only.
   - README should link to it.
   - Keep the auto-generation pipeline so tables remain consistent with registry.

#### B) Standardize and document path-based loader contracts (per family)
Add explicit public convenience constructors (in addition to load_tokenizer auto-detect):

BPE (GPT-2/RoBERTa style):
- load_bpe_gpt2(vocab_json::AbstractString, merges_txt::AbstractString; byte_level::Bool=true, kwargs...) -> AbstractSubwordTokenizer

BPE (encoder.json + vocab.bpe variant):
- load_bpe_encoder(encoder_json::AbstractString, vocab_bpe::AbstractString; kwargs...) -> AbstractSubwordTokenizer

WordPiece:
- load_wordpiece(vocab_txt::AbstractString; continuation_prefix::String="##", kwargs...) -> AbstractSubwordTokenizer

SentencePiece:
- load_sentencepiece(model_file::AbstractString; kind::Symbol=:auto, kwargs...) -> AbstractSubwordTokenizer
  - kind=:auto chooses unigram vs bpe based on model metadata

Unigram (non-SentencePiece internal, if supported):
- load_unigram(vocab_file::AbstractString, scores_file::AbstractString; kwargs...) -> AbstractSubwordTokenizer
  - if not supported, do not add this signature; instead document that Unigram is loaded via SentencePiece .model or tokenizer.json

tiktoken:
- load_tiktoken(encoding_file::AbstractString; kwargs...) -> AbstractSubwordTokenizer

Hugging Face tokenizer.json:
- load_hf_tokenizer_json(tokenizer_json::AbstractString; kwargs...) -> HuggingFaceJSONTokenizer

For each function:
- Validate file existence
- Validate expected extension OR content signature (see section C)
- Produce errors that tell the user exactly which files are missing and show an example call

#### C) Improve auto-detection for ambiguous local files (especially LLaMA)
1) Implement a single internal detector:
- detect_tokenizer_format(path::AbstractString)::Symbol
- detect_tokenizer_files(dir::AbstractString)::NamedTuple or FilesSpec

2) Detection rules (in order):
- If directory contains tokenizer.json: choose :hf_tokenizer_json
- If directory contains vocab.json + merges.txt: choose :bpe_gpt2 (byte-level default)
- If directory contains encoder.json + vocab.bpe: choose :bpe_encoder
- If directory contains any of:
  - tokenizer.model
  - spiece.model
  - sentencepiece.bpe.model
  choose :sentencepiece_model
- If file extension is .tiktoken: choose :tiktoken
- If file extension is .model:
  - sniff first bytes:
    - if it looks like SentencePiece protobuf (binary): choose :sentencepiece_model
    - else if it looks like tiktoken text (lines like "<base64> <int>"): choose :tiktoken
  - document this explicitly (LLaMA 3 commonly ships a tiktoken text file named tokenizer.model)

3) Always allow user override:
- load_tokenizer(path; format=:tiktoken) should bypass auto-detect.

#### D) Make "user supplied local models" ergonomic and format-safe
1) Ensure register_local_model! accepts:
- register_local_model!(key::Symbol, path::AbstractString; format=:auto, description="", family=nothing)
- register_local_model!(key::Symbol, spec; description="", family=nothing)
  where spec is a FilesSpec or NamedTuple for explicit file sets.

2) Stored metadata should include:
- format
- expected_files
- resolved paths
- optional notes

3) load_tokenizer(:key) should:
- check local registry first (installed or user-registered)
- then check built-in registry (artifact/shipped)

#### E) LLaMA convenience: keep it opt-in, but make it feel built-in
1) Keep LLaMA out of Artifacts.toml if access is gated.
2) Treat LLaMA as distribution=:installable_gated:
- appears in docs under "Installable gated"
- install_model!(key; token=...) downloads into cache and then registers locally
3) Ensure docs give 2 workflows:
- install_model! (best UX if user has HF token)
- manual local path (if user already downloaded files elsewhere)

#### F) Tests that enforce contracts and prevent regressions
Add tests for:
- each explicit loader function:
  - missing file error messages mention expected files and show example usage
- detect_tokenizer_format and detect_tokenizer_files:
  - directory with tokenizer.json prefers hf_tokenizer_json
  - .model binary chooses sentencepiece
  - .model text chooses tiktoken
- register_local_model! works with:
  - directory path
  - explicit file tuple/spec
- load_tokenizer(path) with format overrides behaves as expected

### 13.4 Suggested implementation order
1) Docs restructure: create formats.md, loading_local.md, troubleshooting.md; slim README.
2) Add explicit loader functions and validation helpers (keep old paths working).
3) Implement robust .model sniffing and detection tests.
4) Expand register_local_model! to accept explicit specs and improve metadata.
5) Final docs polish with concrete examples for each family.


## 14) README slimming + documentation polish + API discoverability (final "user trust" pass)

### 14.1 Objective
Make KeemenaSubwords feel finished and easy to adopt by:
- Keeping README short (quickstart + links), pushing detail into Documenter docs.
- Eliminating remaining docs/README drift and example inconsistencies.
- Making all path-based loaders and file contracts obvious and discoverable in both docs narrative pages and the API reference.

### 14.2 What "done" looks like
- README is a quickstart only:
  - install
  - 2 to 3 minimal examples
  - a short "Featured models" list (not the full inventory)
  - links to docs pages for Models, Formats, Local Loading, LLM Cookbook, Gated Models, Troubleshooting, API
- Docs are the authoritative reference:
  - consistent example calls and filenames everywhere
  - consistent naming for local registry function (no deprecated name in primary examples)
- API reference page lists the explicit loader functions clearly (and they are exported).
- A CI check prevents drift:
  - generated README "Featured models" stays in sync with registry
  - docs models table stays in sync with registry
  - doctests or example checks catch wrong file names (vocab.txt vs vocab.json, etc.)

### 14.3 Tasks

#### A) Reduce README and make docs the default
1) Rewrite README to include only:
- Install instructions
- Quickstart examples:
  - built-in load + tokenize
  - local path load + format override example
  - gated install example (install_model! + load_tokenizer)
- Small "Featured models" bullet list (8 to 12 keys)
- Link list to docs:
  - Models
  - Formats and required files
  - Loading local tokenizers
  - LLM cookbook
  - Gated models (install_model!)
  - Troubleshooting
  - API reference

2) Remove any manual full key inventory from README (no large tables).

#### B) Keep inventory generated, but split output: README summary vs docs full table
1) Update the generator tool so it can render:
- A short featured list for README (grouped by format)
- A full table for docs models page
2) Add marker blocks:
- README: <!-- KEEMENA_FEATURED_MODELS_START --> ... <!-- KEEMENA_FEATURED_MODELS_END -->
- docs/src/models.md: keep full table markers
3) Add CI check that fails if generated blocks are stale.

#### C) Documentation consistency fixes (examples and naming)
1) Canonicalize named-spec fields and filenames across all docs:
- GPT2/RoBERTa BPE must use:
  - vocab.json + merges.txt
  - named spec keys: vocab_json, merges_txt
- encoder.json + vocab.bpe variant must use:
  - named spec keys: encoder_json, vocab_bpe
- WordPiece must use:
  - vocab.txt
  - named spec key: vocab_txt
- SentencePiece must use:
  - *.model / *.model.v3 / sentencepiece.bpe.model
  - named spec key: model_file (or model_path), but pick one and use it everywhere
- tiktoken must use:
  - *.tiktoken
  - also document the LLaMA3 case where the tiktoken text file may be named tokenizer.model and requires format=:tiktoken
- HF tokenizer.json must use:
  - tokenizer.json
  - named spec key: tokenizer_json

2) Fix any incorrect examples (especially any mention of vocab.txt for GPT2 BPE).

3) Ensure all docs pages use the canonical public API names:
- Prefer register_local_model! everywhere
- If register_external_model! exists, mention it only in one short deprecation note (not in examples)

#### D) API reference discoverability (explicit loaders)
1) Ensure the explicit loader functions are:
- exported (or clearly documented if intentionally not exported)
- documented with docstrings that include required files and example calls
2) Update docs API page to explicitly list:
- load_bpe_gpt2
- load_bpe_encoder
- load_wordpiece
- load_sentencepiece
- load_tiktoken
- load_hf_tokenizer_json
- load_tokenizer
- detect_tokenizer_format / detect_tokenizer_files (if public)
- register_local_model! / install_model! / prefetch_models / available_models / describe_model

#### E) Add doc correctness guardrails
1) Add doctests (or doc snippet tests) for the examples in:
- Formats page
- Loading Local page
- Gated Models page
2) Add a lightweight CI check that scans docs markdown for common wrong patterns, for example:
- "format=:bpe_gpt2" together with "vocab.txt" should fail
(This is optional but extremely effective at preventing regressions.)

### 14.4 Suggested implementation order
1) README rewrite + links to docs
2) Update generator to produce README featured list + docs full table
3) Fix docs examples and canonicalize naming
4) Ensure API reference lists the explicit loaders
5) Add doc guardrails in CI


## 15) Final docs polish + API consistency + Hugging Face tokenizer.json compliance expansion

### 15.1 Objective
Finish the "user trust" and "no Python needed" story by:
- Removing remaining documentation rough edges (marker leakage, inconsistent examples, unclear SentencePiece claims).
- Making the explicit loader APIs discoverable and consistent (docs and API reference agree).
- Expanding Hugging Face `tokenizer.json` support so most modern HF tokenizers (BPE, WordPiece, Unigram, SentencePiece-style pipelines expressed in JSON) work reliably in pure Julia.

### 15.2 Short term tasks (polish and consistency)

#### A) Fix docs marker leakage (Built-In Models page)
Problem: model table sync markers are rendering as visible text in docs.
Tasks:
- Replace HTML comment markers with safe plain-text sentinel markers that will not be "smart-dash" converted by markdown tooling.
  Example sentinel strings:
  - `@@KEEMENA_MODELS_START@@`
  - `@@KEEMENA_MODELS_END@@`
- Update `tools/sync_readme_models.jl` (and any docs sync script) to locate and replace between these sentinels.
- Ensure the sentinels never appear in the rendered docs output (they can remain in source as plain text, but should be placed inside a fenced code comment block or removed after generation).

Acceptance:
- The docs Models page renders cleanly with no visible marker artifacts.

#### B) Make named-spec examples canonical everywhere (docs + API reference + docstrings)
Problem: inconsistent example keys and wrong filenames (example: vocab.txt used for GPT2 BPE).
Tasks:
- Define one canonical field schema per format and use it everywhere:
  - `:bpe_gpt2` expects `vocab.json + merges.txt`
    - named spec keys: `vocab_json`, `merges_txt`
  - `:bpe_encoder` expects `encoder.json + vocab.bpe`
    - named spec keys: `encoder_json`, `vocab_bpe`
  - `:wordpiece` expects `vocab.txt`
    - named spec key: `vocab_txt`
  - `:sentencepiece_model` expects `*.model` / `*.model.v3` / `sentencepiece.bpe.model`
    - named spec key: `model_file` (pick exactly one name and standardize)
  - `:tiktoken` expects `*.tiktoken` OR a tiktoken-text file that might be named `tokenizer.model`
    - named spec key: `encoding_file`
  - `:hf_tokenizer_json` expects `tokenizer.json`
    - named spec key: `tokenizer_json`
- Update:
  - docstrings in `load_tokenizer`, `load_*` functions
  - docs pages: Formats, Loading, Loading Local, LLM Cookbook, API
- Add a small docs lint script (or CI grep) to detect known wrong pairings:
  - if a docs page contains `format=:bpe_gpt2` then it must not mention `vocab.txt`.

Acceptance:
- No docs page or docstring mentions GPT2 BPE with vocab.txt.

#### C) Decide and document the public explicit loaders
Problem: docs reference loaders that may not be visible in API reference.
Tasks:
- Choose a single public set of explicit loaders (recommended):
  - `load_bpe_gpt2(vocab_json, merges_txt; kwargs...)`
  - `load_bpe_encoder(encoder_json, vocab_bpe; kwargs...)`
  - `load_wordpiece(vocab_txt; kwargs...)`
  - `load_sentencepiece(model_file; kwargs...)`
  - `load_tiktoken(encoding_file; kwargs...)`
  - `load_hf_tokenizer_json(tokenizer_json; kwargs...)`
- Export them and add docstrings that explicitly list required files.
- Add an "Explicit loader APIs" section to the API docs page so these appear prominently (not only via autodocs).

Acceptance:
- Users can find explicit loader functions from the API page without searching source code.

#### D) Clarify SentencePiece support (binary vs extracted text)
Problem: docs can read as contradictory if one page says "binary .model" and another implies a text form.
Tasks:
- Decide the truth and document it consistently:
  - If you support standard SentencePiece binary `.model` directly, say so everywhere.
  - If you support an extracted text representation, give it a distinct format name (example: `:sentencepiece_text`) and document how to obtain it.
- Ensure `detect_tokenizer_format` aligns with the documented behavior.

Acceptance:
- There is exactly one clear story for SentencePiece, and the loaders match it.

#### E) README stays short and stable
Tasks:
- Keep README as:
  - install
  - quickstart examples (built-in, local path, gated install_model!)
  - a short "Featured models" list
  - links to docs pages
- Do not keep full inventories in README (or if you keep them, collapse using `<details>` so it does not dominate).

Acceptance:
- README is under control and points to docs as the source of truth.

### 15.3 Hugging Face tokenizer.json compliance expansion (key for "no Python needed")

The goal is not to replicate 100 percent of Rust Tokenizers immediately, but to reliably cover common tokenizer.json pipelines used by LLMs.

#### A) Parse and validate the tokenizer.json schema robustly
Tasks:
- Ensure the loader recognizes and uses these top-level fields when present:
  - `model`
  - `normalizer`
  - `pre_tokenizer`
  - `post_processor`
  - `decoder`
  - `added_tokens`
  - optional: `truncation`, `padding` (parse, but apply only when user requests)
- Treat unknown fields as ignored (not an error), but unknown component types as a clear error.

Acceptance:
- tokenizer.json with extra HF metadata loads fine; unknown component types give a precise error.

#### B) Model implementations (algorithms) must match HF expectations
Implement or harden the three major models as represented in tokenizer.json:

1) BPE model (most LLM tokenizers.json)
Required fields to support:
- vocab map token -> id
- merges list (pair merges)
Common flags to support (even if some are no-ops at first, they must be parsed):
- `unk_token`
- `continuing_subword_prefix` (if present)
- `end_of_word_suffix` (if present)
- `fuse_unk` (optional)
- `byte_fallback` (important for robustness)
- `dropout` (optional, can be unsupported with clear message)

Algorithm guidance:
- Build pair ranks from merges once at load time.
- Apply merges to each pre-tokenized piece deterministically.
- If `byte_fallback=true`, define a fallback strategy when no merge path exists:
  - split to bytes and map bytes to tokens if the vocab contains them (common patterns), otherwise use unk token.

2) WordPiece model (BERT family)
Required fields:
- vocab map token -> id
- `unk_token`
- `continuing_subword_prefix`
- `max_input_chars_per_word`

Algorithm guidance:
- Greedy longest-match per word.
- If segmentation fails, emit `unk_token`.
- Respect `max_input_chars_per_word`.

3) Unigram model (SentencePiece-style expressed in JSON)
Required fields:
- vocab list of (token, score) or equivalent representation
- `unk_id` and/or unk token
- optional `byte_fallback`

Algorithm guidance:
- DP/Viterbi segmentation using token scores (logprobs).
- If no path exists, use unk (or byte fallback if enabled).

Acceptance:
- Each model produces stable tokenization for fixture JSON tokenizers.

#### C) Pipeline compliance: normalizer, pre_tokenizer, post_processor, decoder
This is the "HF fast tokenizers" core.

1) Added tokens handling (very important)
Tasks:
- Implement a pre-pass that detects and preserves `added_tokens` (including special tokens).
- Respect added token attributes commonly present in tokenizer.json:
  - `content`
  - `special`
  - `single_word`
  - `lstrip` / `rstrip`
  - `normalized`
- Rule of thumb:
  - Special tokens must not be normalized and should be matched verbatim.
  - For non-special added tokens, match according to normalized flag.

Implementation guidance:
- Build an efficient matcher at load time:
  - Aho-Corasick trie (preferred) or a compiled regex union.
- Split input into segments:
  - segments that are added tokens bypass normalizer and model, go straight to ids
  - other segments go through normalizer -> pre_tokenizer -> model

2) Normalizers (expand beyond the current minimum)
Short-term additions that matter:
- StripAccents
- Replace (pattern -> replacement)
- Prepend (prefix)
- NFC/NFD (optional)
- Sequence (composition)

Implementation guidance:
- Represent normalizer pipeline as a vector of callable structs, run sequentially.

3) Pre-tokenizers (expand to cover common LLM JSON tokenizers)
Short-term additions that matter:
- ByteLevel (must match GPT-2 byte mapping behavior)
- Metaspace (SentencePiece-like whitespace marker)
- Split with regex
- Punctuation / Digits (optional but common)
- Sequence (composition)

Implementation guidance:
- For ByteLevel, reuse your existing byte mapping utilities so outputs match your ByteBPE and tiktoken behavior as closely as possible.

4) Post-processors
Support at minimum:
- TemplateProcessing (already important)
Add next:
- BertProcessing / RobertaProcessing equivalents if you see them in the wild

Implementation guidance:
- Implement placeholder substitution for single and pair sequences.
- If you do not want to expose token_type_ids publicly yet:
  - compute them internally for correctness
  - expose via an optional return struct later

5) Decoders
Support:
- Sequence
- ByteLevel
- Metaspace
- WordPiece
- BPE decoder (if necessary for some JSON tokenizers)

Acceptance:
- encode/decode roundtrip is stable for representative JSON fixtures.

#### D) Hooking HF JSON loader into the package cleanly
Tasks:
- `load_tokenizer(path; format=:hf_tokenizer_json)` remains the single entry point.
- Directory autodetection chooses tokenizer.json first when present.
- Built-in HF models:
  - prefer tokenizer.json
  - fallback to merges/vocab when tokenizer.json missing (already implemented)
- Ensure `describe_model` states:
  - format=:hf_tokenizer_json
  - expected_files include tokenizer.json and fallback files

#### E) Tests for compliance
Tasks:
- Add offline fixtures representing real patterns:
  - a ByteLevel BPE tokenizer.json (RoBERTa-like)
  - a Metaspace Unigram tokenizer.json (SentencePiece-like)
  - a WordPiece tokenizer.json
  - at least one fixture with TemplateProcessing and special tokens
  - at least one fixture with added_tokens and lstrip/rstrip rules
- Add tests:
  - tokenization golden tokens/ids for a few strings
  - decode(encode(text)) smoke tests
  - robust error messages for unsupported components include JSON path and suggested workaround

### 15.4 Longer term tasks (no training)
- Expand HF tokenizer.json component coverage progressively:
  - more normalizers, more pre-tokenizers, more post-processors, more decoders
- Improve outputs beyond ids/tokens:
  - optional offsets, attention_mask, token_type_ids (return a struct)
- Add 1 to 3 additional flagship model tokenizers as artifacts where license allows:
  - keep it curated, not exhaustive
- Performance hardening:
  - caching (BPE merges per token)
  - trie-based WordPiece
  - optimized Unigram DP
  - faster added_tokens matching (Aho-Corasick)



## 16) Solidify the current state (docs contracts, CI stability, and public API coherence)

### Objectives
- Make CI green on the supported Julia versions (or formally raise the minimum Julia version and align CI).
- Eliminate docs drift: the “Formats” and “Loading Local” pages must match the real exported API and the named-spec contracts.
- Make byte-level behavior explicit in docs so LLM users understand what they are getting.

### Tasks

#### A) Fix CI failures (highest priority)
1. Reproduce CI locally on the CI matrix versions:
   - Julia 1.10 (first) then Julia 1.6 (only if you still intend to support it).
2. Decide and document minimum supported Julia version:
   - If you require modern Julia (recommended for tokenization performance and JSON stacks), set Project.toml compat accordingly and remove 1.6 from CI.
   - If you keep 1.6, patch code/tests for 1.6 compatibility and add version-gated tests where needed.
3. Remove hidden/bidi Unicode characters from `.github/workflows/CI.yml`:
   - Rewrite the file in plain ASCII, reindent, and re-commit to remove “hidden Unicode” warnings.

#### B) Resolve API/doc drift for local loading contracts
4. Unify the named-spec “canonical keys” for single-file formats:
   - Choose one canonical pattern, recommended:
     - `path` for single-file formats (wordpiece vocab, unigram.tsv, tokenizer.json, tokenizer.model)
     - format-specific keys for multi-file formats (vocab_json + merges_txt, encoder_json + vocab_bpe)
5. Implement backward-compatible aliases (so old docs or user code still works):
   - For WordPiece:
     - accept both `path` and `vocab_txt`
   - For HF tokenizer.json:
     - accept both `path` and `tokenizer_json`
   - For Unigram:
     - accept both `path` and `unigram_tsv` (optional alias, but helpful)
6. Update docs so every page uses the same canonical keys:
   - Formats page: update the “Canonical Named Spec Keys” column.
   - Loading Local page: update examples to match canonical keys.
   - API reference examples: align them too.

#### C) Make classic BPE / ByteBPE / Unigram loader API consistent
7. Decide whether `load_bpe`, `load_bytebpe`, `load_unigram` are public:
   - If public:
     - export them, add docstrings, and include them in the API reference page.
     - add at least one unit test per loader.
   - If not public:
     - remove them from docs and replace with:
       - `load_tokenizer(path; format=:bpe)`
       - `load_tokenizer(path; format=:bytebpe)`
       - `load_tokenizer(path; format=:unigram)`

#### D) Byte-level documentation tightening (small but high UX value)
8. Add a short “Byte-level behavior” subsection to docs:
   - Which formats are byte-level (GPT2/RoBERTa style) vs not (WordPiece, SentencePiece).
   - What decode/encode guarantees exist per tokenizer type.
   - How byte-level interacts with HF tokenizer.json pipelines.

#### E) Test coverage improvements to prevent regressions
9. Add tests that enforce doc-contract stability:
   - Named-spec alias tests: `path` and the format-specific key must both work where intended.
   - Format detection precedence tests (directory with both tokenizer.json and vocab/merges should pick tokenizer.json unless overridden).
10. Add a small “docs-contract check” script or test that fails if:
   - docs Formats table lists keys not supported by `load_tokenizer` named-spec parsing
   - docs reference non-exported functions (unless explicitly module-qualified)

### Implementation order
1) CI green + workflow unicode cleanup
2) Named-spec canonical keys + alias support
3) Docs updates for formats/loading-local/api examples
4) Loader API decision for load_bpe/load_bytebpe/load_unigram and tests
5) Byte-level doc subsection + tests for detection and aliases



## 17) Conformance, HF tokenizer.json parity, structured outputs, and robust local loading

### 17.1 Objective
Move from "works and has good coverage" to "users trust it like a Python tokenizer stack" by focusing on:
1) Conformance testing against reference implementations (highest leverage).
2) Expanding Hugging Face tokenizer.json compliance for the common real-world pipelines.
3) Providing a structured encode output that downstream users can rely on.
4) Making "bring your own files" rock-solid for all tokenizer families (BPE, WordPiece, SentencePiece, Unigram, tiktoken, HF JSON).

This section intentionally avoids training. The goal is correctness, usability, and parity.

### 17.2 Deliverables
- A conformance test suite driven by golden vectors for flagship tokenizers:
  - token ids (primary)
  - optional token strings and decode checks where meaningful
- A "golden generator" tool (optional dependency) that can regenerate vectors from reference libs (Python) without making Python a runtime requirement.
- Expanded HF tokenizer.json support for the components that matter most:
  - added_tokens behavior
  - ByteLevel parity
  - byte_fallback
  - more normalizers and pre_tokenizers
  - common post-processors and decoders
- A stable `TokenizationResult` struct and `encode_result(...)` API (non-breaking).
- Strong path-based file contracts and validation across all families, with helpful error messages and deterministic auto-detection.

### 17.3 Tasks

#### A) Conformance testing against reference implementations (golden vectors)
Goal: If a user points KeemenaSubwords at the same tokenizer files as Python/Rust, token ids should match for the supported feature set.

A1) Define a golden vector file format
- Add `test/golden/` with one file per tokenizer family or per model, using JSON or TOML.
- Each golden case should include:
  - `name` (human readable)
  - `format` (e.g., bpe_gpt2, sentencepiece_model, hf_tokenizer_json, tiktoken)
  - `source` (built-in key or local fixture path relative to test/)
  - `settings` (add_special_tokens on/off, etc.)
  - `cases`: list of `{ text, expected_ids }`
  - optional: `{ expected_tokens }` when stable and meaningful
  - optional: `{ expected_decoded }` when the reference decode is stable

A2) Choose a curated set of public, non-gated reference tokenizers
Use models that are redistdurableributable and already in your built-ins/artifacts:
- tiktoken: cl100k_base (and optionally o200k_base)
- Byte-level BPE: GPT-2 (encoder.json + vocab.bpe) and RoBERTa (vocab.json + merges.txt)
- WordPiece: bert-base-uncased and bert-base-multilingual-cased
- SentencePiece: t5-small (unigram), mistral v1/v3 (if available publicly), xlm-roberta-base sentencepiece.bpe.model
- HF tokenizer.json: qwen2.5 tokenizer.json (plus one ByteLevel BPE json if available)

Avoid gated models (LLaMA) in conformance goldens.

A3) Build a representative test string corpus (shared across models)
Create `test/golden/strings.txt` (or embed in generator) with 50 to 200 strings including:
- ASCII and punctuation
- multiple spaces and tabs
- newlines
- emojis and non-Latin scripts
- combining marks (unicode normalization sensitivity)
- strings containing special tokens like "<s>", "</s>", "[CLS]" depending on model
- tricky ByteLevel inputs (leading space behavior, repeated whitespace)

A4) Add a golden generator tool (optional, not required to run tests)
Add `tools/generate_goldens/`:
- `requirements.txt` (tiktoken, tokenizers, transformers, sentencepiece)
- `generate_goldens.py` that:
  - downloads tokenizer assets (for public models only)
  - runs reference encoding
  - writes golden JSON/TOML into `test/golden/`
Design rules:
- Goldens are committed into git.
- Tests do not require Python, only the committed goldens.
- The generator is for maintainers only.

A5) Julia test runner that validates goldens
Implement `test/test_goldens.jl` that:
- loads each golden spec
- constructs the tokenizer via the same public API users use:
  - built-in key
  - load_tokenizer(local fixture dir/file)
- runs `encode(...)` and compares ids exactly
- provides clear diff output on failure (first mismatch index, nearby tokens)

Acceptance:
- Golden tests pass on the supported Julia versions and platforms.

#### B) Expand HF tokenizer.json compliance (focus on what breaks users most)
Goal: Make most modern HF tokenizers usable directly from tokenizer.json without requiring Python.

B1) Added tokens correctness (highest impact)
Implement full added token handling with fields commonly seen in tokenizer.json:
- `content`
- `special`
- `single_word`
- `lstrip`
- `rstrip`
- `normalized`

Behavior guidance:
- Special tokens must bypass normalization and pretokenization.
- Added tokens must be matched before the normal pipeline runs.
- `lstrip`/`rstrip` control whitespace consumption around matches.
- `single_word` must enforce word-boundary matching.

Implementation guidance:
- Build an efficient matcher at load time:
  - Aho-Corasick or trie-based longest match over all added tokens
- Split input into segments:
  - added-token segments -> emit their ids directly
  - other segments -> normalizer -> pre_tokenizer -> model -> post_processor -> decoder

Add tests:
- added token match with and without surrounding whitespace
- overlapping tokens (longest match wins)
- single_word boundary cases
- special token bypass of normalization

B2) ByteLevel pretokenizer parity
Ensure ByteLevel behavior matches HF expectations:
- whitespace mapping and leading-space handling
- byte to unicode mapping consistency
- stable round-trip for arbitrary bytes where supported

Add tests:
- known ByteLevel behavior strings (leading space, repeated space, unusual bytes)
- compare to goldens from reference tokenizers

B3) byte_fallback support
Many tokenizers rely on byte fallback rather than emitting unk.
- For BPE:
  - if no merge path yields a vocab token, fall back to byte tokens if present
- For Unigram:
  - if DP fails, fall back to bytes or unk depending on configuration

Add tests:
- inputs containing rare/unseen unicode characters
- confirm fallback behavior matches reference goldens

B4) Normalizers expansion (next most common)
Add support (parse + execute) for:
- StripAccents
- Replace (pattern, replacement)
- Prepend (prefix)
- NFC / NFD (optional but common)
- Sequence composition

B5) Pre-tokenizers expansion (common in real tokenizer.json)
Add support for:
- Punctuation (common)
- Digits (common)durable
- UnicodeScripts or Split-on-script boundaries (optional, but appears in some)
- Ensure Sequence composition works for all of the above

B6) Post-processors and decoders (target the common ones)
Post-processors:
- TemplateProcessing (ensure correctness for single and pair sequences)
- BertProcessing and RobertaProcessing equivalents if encountered

Decoders:
- Sequence
- ByteLevel
- Metaspace
- WordPiece
- (optional) BPE decoder if needed for some JSON tokenizers

Acceptance:
- A significantly broader set of real tokenizer.json files can be loaded and produce correct ids (validated by goldens).

#### C) Provide structured encode output (non-breaking)
Goal: Make downstream pipelines easier by returning more than just ids when requested.

C1) Define a stable result struct
Add a public struct:
- `TokenizationResult`
  - `ids::Vector{Int}`
  - `tokens::Vector{String}` (optional or always computed)
  - `offsets::Union{Nothing, Vector{Tuple{Int,Int}}}` (char offsets)durable
  - `attention_mask::Union{Nothing, Vector{Int}}`
  - `token_type_ids::Union{Nothing, Vector{Int}}`
  - `special_tokens_mask::Union{Nothing, Vector{Int}}`
  - `metadata::NamedTuple` (format, added_special_tokens, etc.)

C2) Add a new API that does not break existing callers
Keep:
- `encode(tokenizer, text; kwargs...) -> Vector{Int}`

Add:
- `encode_result(tokenizer, text; add_special_tokens=true, return_offsets=false, return_masks=false, ...) -> TokenizationResult`

Optionally add:
- `encode_batch_result(tokenizer, texts; ...)`

C3) Offset tracking strategy (be explicit)
Offsets are hard when normalization changes text.
Start with a pragmatic definition:
- Offsets are in the normalized text coordinate system, unless `return_offsets=:original` is explicitly supported later.
Document this clearly.

Implement offsets for:
- Whitespace and regex splits
- ByteLevel segments durable(best-effort)
- WordPiece and BPE subpieces within a pretokenized segment

If a pipeline makes offsets ambiguous, return `nothing` and document why.

Add tests:
- offsets sanity on a few simple inputs for each major pipeline

Acceptance:
- Users can get ids only (fast path) or request a rich result struct when needed.

#### D) Robust "bring your own files" experience for all families
Goal: Make local-path loading predictable and hard to misuse.

D1) Unify file-spec handling internally
Add/standardize a `FilesSpec` (or NamedTuple convention) that can represent:
- single-file formats:
  - hf tokenizer.json
  - wordpiece vocab.txt
  - sentencepiece .model
  - tiktoken encoding file
- multi-file formats:
  - bpe_gpt2 vocab.json + merges.txt
  - encoder.json + vocab.bpe
  - any internal unigram representation (if supported)

D2) Validate inputs with strong error messages
For every explicit loader and for `load_tokenizer` named-spec:
- check file existence
- check expected extensions OR content sniffing where necessary
- if missing, throw an error that includes:
  - which files are required
  - what was provided
  - an example call
  - a hint about common directory layouts

D3) Directory autodetection hardening
Improve `detect_tokenizer_files(dir)`:
- prefer tokenizer.json when present
- then bpe_gpt2 (vocab.json + merges.txt)
- then bpe_encoder (encoder.json + vocab.bpe)
- then sentencepiece model filenames
- then tiktoken filenames

Add tests that cover directories containing multiple candidate file sets.

D4) Local model registry UX
Ensure `register_local_model!` accepts:
- a directory path (auto-detect + store resolved spec)
- an explicit `FilesSpec` / NamedTuple spec for multi-file layouts

Ensure `load_tokenizer(:key)` checks:
- local registry first
- then built-ins
and the error message explains how to install or register if not found.

Acceptance:
- Users can point to a HF folder, a single tokenizer file, or explicit file pairs, and it "just works" with clear errors when it cannot.

### 17.4 Suggested implementation order
1) Golden format and Julia test runner (start failing usefully).
2) Golden generator tool (optional) and initial goldens for 4 to 6 flagship tokenizers.
3) HF JSON compliance: added_tokens, ByteLevel parity, byte_fallback (in that order).
4) Structured output: TokenizationResult + encode_result + minimal offsets.
5) Bring-your-own-files hardening: FilesSpec + validation + detection tests.
6) Expand HF JSON: more normalizers, pretokenizers, post-processors, decoders as encountered.


## 18) Fix CI failures by stabilizing artifacts and making model availability tests robust

### 18.1 Objective
Get CI green by fixing the real root cause:
- Artifact-backed tokenizer assets are not downloading/being installed in CI, causing built-in key availability tests to fail.
Also fix the small registry metadata mismatch for GPT-2.

This section is about distribution and test robustness, not algorithm changes.

### 18.2 Tasks

#### A) Fix GPT-2 registry metadata vs test expectation (small, immediate)
1) Decide what `gpt2_info.files` is supposed to mean:
   - Option 1 (recommended): `files` lists required file names (or required file roles) for the model format.
   - Option 2: `files` is a single FilesSpec entry (like a directory spec).

2) Update either:
   - the GPT-2 model registry entry to list both required files (encoder.json + vocab.bpe), OR
   - the test to check the correct invariant (for example `expected_files` contains both names) rather than `length(files) == 2`.

Acceptance:
- The "Model registry" testset passes without relying on artifacts.

#### B) Make artifact hosting actually valid (the real CI breaker)
3) Audit Artifacts.toml bindings for:
   - `keemena_public_tokenizer_assets_v1`
   - `mistral_v1_sentencepiece`, `mistral_v3_sentencepiece`, `phi2_bpe`, `qwen2_5_bpe`, `roberta_base_bpe`, `xlm_roberta_base_sentencepiece_bpe`
   - `tiktoken_*` assets (especially o200k_base)

For each artifact, confirm:
- the download URL exists and returns a tarball (not an HTML page)
- sha256 matches the tarball
- git-tree-sha1 matches the extracted tree

4) Establish a stable artifact hosting strategy
Pick one and commit to it:

Strategy 1 (recommended): GitHub Releases for this repo (or a dedicated assets repo)
- Create a release (tagged) and upload each artifact tarball with stable filenames.
- Update Artifacts.toml to point to those release asset URLs.
- This makes CI deterministic.

Strategy 2: "Installable public" instead of artifacts
- If you do not want to maintain tarballs/releases yet, reclassify these public models as installable downloads (like gated models but without auth).
- Use your `download_hf_files(...)` pipeline to fetch plain files into cache.
- Update `prefetch_models` to use this mechanism for those keys.
- This avoids tarballs entirely, but is less "Julia artifact-native".

Acceptance:
- `prefetch_models` succeeds in CI for the baseline and curated keys without manual setup.

#### C) Ensure model_path/load_tokenizer do not rely on package-local placeholder directories
5) Remove or reduce reliance on `models/_artifacts_only/...` as a required on-disk location.
Instead:
- Use the actual artifact install directory from Pkg artifacts as the source of truth.
- If you want stable "relative paths", store them as relative paths inside the artifact tree.

Acceptance:
- `model_path(:some_key)` resolves to an installed artifact path (or a cache path for installable models),
  not to a package directory placeholder that is not populated by default.

#### D) Make the artifact-dependent tests deterministic and non-spammy
6) Stop repeated download attempts in tests
- Ensure tests prefetch once per artifact set, not once per key.
- Cache the result and reuse it.

7) Make failures report the real underlying cause
- When a prefetch fails, print/log:
  - the artifact name
  - the URL that was attempted
  - the exception message
- Consider setting `ENV["JULIA_DEBUG"]="Pkg.Artifacts"` during CI tests or when failures occur.

Acceptance:
- If artifacts fail again, CI output tells you exactly why (404, checksum mismatch, TLS, etc.)

#### E) Adjust availability tests to match "distribution mode"
8) Split "registry correctness" from "download integration"
- Always run:
  - registry metadata shape tests (expected_files, format, family, distribution)
- Only run download integration tests when:
  - `ENV["KEEMENA_TEST_DOWNLOADS"] == "1"`
  - OR on main branch CI after artifact hosting is confirmed stable

9) Ensure the baseline/curated tests do not throw "exception outside of @test"
- Wrap load checks in `@test` blocks and report meaningful failures.

Acceptance:
- Unit tests are stable offline; integration download tests are stable when enabled.

### 18.3 Suggested implementation order
1) Fix GPT-2 metadata/test mismatch (fast).
2) Implement artifact hosting strategy (release tarballs or installable-public fallback).
3) Refactor model_path to resolve actual artifact/cache locations (not package placeholders).
4) Improve test structure: one prefetch, better diagnostics, optional gating.


## 19) Asset download UX hardening, SentencePiece fallback fix, and CI download verification

### 19.1 Objective
Close the remaining "real world usability" gaps around built-in model assets by:
1) Removing scary warnings when artifact install fails but the model is successfully obtained via the fallback downloader.
2) Fixing the SentencePiece fallback hole (example: T5 uses spiece.model and must be discoverable/usable when downloaded into cache).
3) Making CI exercise real downloads so a green pipeline implies that users can actually fetch and use the built-in public models.

### 19.2 Tasks

#### A) Make artifact install failures non-scary when fallback succeeds
Problem:
- Users may see warnings like "Failure artifact: ..." even though fallback download succeeds and the model works.
- This looks like the package is broken, even when it is not.

Tasks:
1) Change prefetch logging behavior
- In `prefetch_models` (or the internal asset resolver):
  - Attempt artifact install first (if configured).
  - If artifact install fails, do NOT emit a warning immediately.
  - Try the fallback file-by-file download into cache.
  - Only warn (or throw) if both:
    - artifact install failed, AND
    - fallback download failed or did not produce the expected files.

2) Provide a verbose/debug mode for maintainers
- Add an environment variable, for example:
  - `KEEMENA_SUBWORDS_ASSET_DEBUG=1`
- When enabled, log:
  - artifact install exception
  - the attempted artifact URLs (if available)
  - fallback URLs and per-file results
- When disabled, keep output quiet and user-friendly.

3) Make prefetch_models return richer status
- Currently it likely returns `Dict{Symbol,Bool}` or similar.
- Extend (without breaking callers) by adding a new function:
  - `prefetch_models_status(keys; ...) -> Dict{Symbol,NamedTuple}`
  where each value includes:
  - `available::Bool`
  - `method::Symbol` (:artifact, :fallback_download, :already_present, :failed)
  - `path::Union{Nothing,String}`
  - `error::Union{Nothing,String}`
- Keep existing `prefetch_models` returning `Bool` map for simplicity (it can call the richer version internally).

Acceptance:
- Default user experience: no artifact failure warnings if the model ends up usable.
- Maintainers can still see artifact errors via debug mode.

#### B) Fix SentencePiece fallback cache discovery (spiece.model hole)
Problem:
- Some SentencePiece models (T5) use `spiece.model`.
- If your directory detection only searches for a limited set of filenames, a successfully downloaded model may still be "invisible" to the loader.

Tasks:
1) Extend candidate filename list for SentencePiece discovery
- Add at least:
  - `spiece.model`
  - (optional) `tokenizer.model` and `tokenizer.model.v3` already exist
- Consider a safe fallback rule:
  - If the directory has exactly one `*.model` file and no tokenizer.json, treat it as SentencePiece.

2) Ensure model_path / describe_model uses the same detection logic
- Avoid situations where:
  - `prefetch_models` downloads the right files, but
  - `model_path` checks a different filename set and reports missing.

3) Add a regression test for this exact case
- Create a temp directory simulating a cached public model:
  - place a small SentencePiece fixture file named `spiece.model`
- Verify:
  - `detect_tokenizer_files(dir)` recognizes it
  - `load_tokenizer(dir)` loads it (or at least chooses format=:sentencepiece_model)
  - `load_tokenizer(:t5_small_sentencepiece_unigram)` succeeds in a local-cache simulation

Acceptance:
- Public SentencePiece models that download as spiece.model are actually usable via built-in keys.

#### C) CI should verify real downloads and real loading
Goal:
- A green CI should mean users can actually fetch and use the built-in public models.

Design constraints:
- Network tests can be flaky if they hit third-party hosts.
- Make them reliable by:
  - using retry logic,
  - caching,
  - and keeping the download set small and representative.

Tasks:
1) Add a dedicated CI job "download-smoke"
- Run tests with:
  - `KEEMENA_TEST_DOWNLOADS=1`
- Use caching:
  - cache `~/.julia/artifacts`, `~/.julia/registries`, and your package cache dir if applicable
- This job should be required for main branch merges (or at least run on push to main and nightly).

2) Keep the download smoke list minimal but meaningful
Pick 1 model per major format:
- tiktoken: `:tiktoken_o200k_base` (or cl100k_base)
- GPT2 byte-level BPE: `:openai_gpt2_bpe`
- WordPiece: `:bert_base_uncased_wordpiece`
- SentencePiece Unigram: `:t5_small_sentencepiece_unigram` (this catches the spiece.model hole)
- HF tokenizer.json: `:qwen2_5_bpe` (ensures tokenizer.json path works with real files)

3) Make download tests assert real usability
For each key:
- `prefetch_models([key])` returns available=true
- `load_tokenizer(key)` returns a tokenizer
- `encode(tokenizer, "Hello world")` returns non-empty ids
- optional: `decode(encode(...))` smoke test where meaningful

4) Add retries and timeouts to downloads (to reduce CI flakiness)
- Wrap Downloads.download in a helper:
  - retries (3 to 5)
  - exponential backoff (0.5s, 1s, 2s, 4s)
  - clear error message including URL
- Use the helper in both fallback downloads and any "installable public" path.

5) Optional: add a scheduled workflow
- A nightly workflow that runs download-smoke against upstream URLs.
- This catches upstream changes early without making every PR brittle.

Acceptance:
- CI green (including download-smoke) implies models can be fetched and loaded on a clean machine.

### 19.3 Suggested implementation order
1) Fix SentencePiece discovery (spiece.model) + regression test.
2) Quiet artifact warnings when fallback succeeds + add debug mode.
3) Add download-smoke CI job and small download testset.
4) Add retry helper and wire it into fallback downloads.
