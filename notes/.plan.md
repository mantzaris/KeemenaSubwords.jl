## 25) Minor offsets polish: fill remaining family gaps + add optional strict validators

### 25.1 Context
KeemenaSubwords now exposes a stable offsets contract and downstream-safe span utilities:
- offsets are 1-based UTF-8 codeunit spans, half-open [start, stop)
- sentinel (0,0) indicates "no source-text span"
- `has_nonempty_span` is the recommended participation predicate for alignment
- `try_span_substring` is safe and returns `nothing` when boundaries are not sliceable
- boundary validity expectations are documented by tokenizer family (string-level vs byte-level)

This is already strong enough for KeemenaPreprocessing integration, but we can make it feel more complete and easier to debug.

### 25.2 Goals
1) Ensure the "boundary-valid by family" test coverage includes a couple of missing-but-expected families.
2) Provide an optional strict validation path for debugging and future tokenizer additions, without changing the existing contract or behavior.

### 25.3 Tasks

#### A) Expand Section 24 boundary-valid test coverage to cover 2 more representative tokenizers
1) Add SentencePiece BPE compatibility to the "string-level tokenizers" boundary-valid testset:
- tokenizer: `load_sentencepiece(fixture("sentencepiece", "toy_bpe.model"))`
- texts: reuse the existing multibyte inputs (cafe + combining, Ã©, ðŸ™‚, aðŸ™‚b)
- assertions: for each non-empty span, boundaries are valid and `try_span_substring(...) isa String`

2) Add tiktoken fixture to the "byte-level tokenizers on ASCII" boundary-valid testset:
- tokenizer: `load_tiktoken(fixture("tiktoken_model", "tokenizer.model"))`
- texts: reuse the existing ASCII whitespace set
- assertions: for each non-empty span, `try_span_substring(...) isa String` and matches `span_codeunits(...)`

Acceptance:
- Section 24 tests cover representative SentencePiece BPE and tiktoken behavior consistent with the contract.

#### B) Add strict offset validation helpers (optional but useful)
Problem:
- Current helpers are designed to be safe and non-throwing. That is correct for downstream usage.
- However, maintainers benefit from a strict validator to catch regressions early with actionable errors.

Add (public or internal, but recommended public):
1) `validate_offsets_contract(text::AbstractString, offsets::Vector{Tuple{Int,Int}}; require_string_boundaries::Bool=false)::Bool`
- returns true/false without throwing
- checks:
  - bounds consistent with the contract
  - sentinel correctness
  - if `require_string_boundaries=true`, checks `is_valid_string_boundary` for start/stop of non-empty spans

2) `assert_offsets_contract(text, offsets; require_string_boundaries=false)::Nothing`
- throws `ArgumentError` with a clear message on first failure

Add tests:
- small unit tests for both functions using:
  - sentinel offsets
  - empty spans
  - a known-good offsets vector from a string-level tokenizer
  - an intentionally invalid offset

Acceptance:
- Maintainers have a strict path that can be used in future tokenizer additions and debugging.
- No behavior changes to encode_result or existing helpers.

#### C) Documentation touch-up (small)
- Add a short paragraph to the offsets contract doc:
  - mention the existence of strict validators and when to use them (debugging, new tokenizer development)
- Keep the contract source-of-truth sync workflow unchanged.

### 25.4 Suggested implementation order
1) Expand Section 24 tests (SentencePiece BPE + tiktoken ASCII).
2) Add strict validation helpers + unit tests.
3) Update contract docs and sync.

### 25.5 Non-goals
- No change to offset convention, sentinel, or existing helper semantics.
- No refactors outside offsets utilities + tests + minimal docs.
