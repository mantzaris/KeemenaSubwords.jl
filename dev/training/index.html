<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training ¬∑ KeemenaSubwords.jl</title><meta name="title" content="Training ¬∑ KeemenaSubwords.jl"/><meta property="og:title" content="Training ¬∑ KeemenaSubwords.jl"/><meta property="twitter:title" content="Training ¬∑ KeemenaSubwords.jl"/><meta name="description" content="Documentation for KeemenaSubwords.jl."/><meta property="og:description" content="Documentation for KeemenaSubwords.jl."/><meta property="twitter:description" content="Documentation for KeemenaSubwords.jl."/><meta property="og:url" content="https://mantzaris.github.io/KeemenaSubwords.jl/training/"/><meta property="twitter:url" content="https://mantzaris.github.io/KeemenaSubwords.jl/training/"/><link rel="canonical" href="https://mantzaris.github.io/KeemenaSubwords.jl/training/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">KeemenaSubwords.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../integration/">Integration</a></li><li><a class="tocitem" href="../normalization_offsets_contract/">Normalization &amp; Offsets</a></li><li><a class="tocitem" href="../loading/">Loading</a></li><li class="is-active"><a class="tocitem" href>Training</a><ul class="internal"><li><a class="tocitem" href="#Training-API"><span>Training API</span></a></li><li><a class="tocitem" href="#HF-BERT-WordPiece-Preset"><span>HF BERT WordPiece Preset</span></a></li><li><a class="tocitem" href="#HF-RoBERTa-ByteBPE-Preset"><span>HF RoBERTa ByteBPE Preset</span></a></li><li><a class="tocitem" href="#HF-GPT-2-ByteBPE-Preset"><span>HF GPT-2 ByteBPE Preset</span></a></li><li><a class="tocitem" href="#Note-on-pretokenizer"><span>Note on pretokenizer</span></a></li><li><a class="tocitem" href="#Training-Bundles"><span>Training Bundles</span></a></li></ul></li><li><a class="tocitem" href="../formats/">Formats</a></li><li><a class="tocitem" href="../loading_local/">Loading Local</a></li><li><a class="tocitem" href="../llm_cookbook/">LLM Cookbook</a></li><li><a class="tocitem" href="../gated_models/">Gated Models</a></li><li><a class="tocitem" href="../troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../models/">Built-In Models</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Training</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/mantzaris/KeemenaSubwords.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/main/docs/src/training.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-(Experimental)"><a class="docs-heading-anchor" href="#Training-(Experimental)">Training (Experimental)</a><a id="Training-(Experimental)-1"></a><a class="docs-heading-anchor-permalink" href="#Training-(Experimental)" title="Permalink"></a></h1><p>Training support is currently experimental and intentionally separated from the pretrained tokenizer loading/encoding workflows.</p><p>Available now:</p><ul><li><code>train_bpe(...)</code></li><li><code>train_bytebpe(...)</code></li><li><code>train_unigram(...)</code></li><li><code>train_wordpiece(...)</code></li><li><code>train_wordpiece_result(...)</code></li><li><code>train_sentencepiece(...)</code></li><li><code>train_sentencepiece_result(...)</code></li><li><code>train_hf_bert_wordpiece(...)</code></li><li><code>train_hf_bert_wordpiece_result(...)</code></li><li><code>train_hf_roberta_bytebpe(...)</code></li><li><code>train_hf_roberta_bytebpe_result(...)</code></li><li><code>train_hf_gpt2_bytebpe(...)</code></li><li><code>train_hf_gpt2_bytebpe_result(...)</code></li><li><code>save_training_bundle(result, out_dir; ...)</code></li><li><code>load_training_bundle(out_dir)</code></li></ul><h2 id="Training-API"><a class="docs-heading-anchor" href="#Training-API">Training API</a><a id="Training-API-1"></a><a class="docs-heading-anchor-permalink" href="#Training-API" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_bpe"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_bpe"><code>KeemenaSubwords.Training.train_bpe</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a character-level BPE tokenizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L1-L3">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_bpe_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_bpe_result"><code>KeemenaSubwords.Training.train_bpe_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a character-level BPE tokenizer and return model artifacts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L26-L28">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_bytebpe"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_bytebpe"><code>KeemenaSubwords.Training.train_bytebpe</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a byte-level BPE tokenizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L51-L53">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_bytebpe_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_bytebpe_result"><code>KeemenaSubwords.Training.train_bytebpe_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a byte-level BPE tokenizer and return model artifacts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L78-L80">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_unigram"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_unigram"><code>KeemenaSubwords.Training.train_unigram</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>High-level Unigram training entry point.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L105-L107">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_unigram_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_unigram_result"><code>KeemenaSubwords.Training.train_unigram_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a Unigram tokenizer and return model artifacts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L136-L138">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_wordpiece"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_wordpiece"><code>KeemenaSubwords.Training.train_wordpiece</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a WordPiece tokenizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L167-L169">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_wordpiece_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_wordpiece_result"><code>KeemenaSubwords.Training.train_wordpiece_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a WordPiece tokenizer and return model artifacts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L200-L202">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_sentencepiece"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_sentencepiece"><code>KeemenaSubwords.Training.train_sentencepiece</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a SentencePiece tokenizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L233-L235">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_sentencepiece_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_sentencepiece_result"><code>KeemenaSubwords.Training.train_sentencepiece_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><p>Train a SentencePiece tokenizer and return model artifacts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_api.jl#L268-L270">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_hf_bert_wordpiece"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_hf_bert_wordpiece"><code>KeemenaSubwords.Training.train_hf_bert_wordpiece</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">train_hf_bert_wordpiece(corpus; kwargs...) -&gt; HuggingFaceJSONTokenizer</code></pre><p>Train a BERT-style WordPiece tokenizer and return a <code>HuggingFaceJSONTokenizer</code> pipeline composed of:</p><ul><li><code>BertNormalizer</code></li><li><code>BertPreTokenizer</code></li><li><code>BertProcessing</code> (CLS/SEP insertion)</li><li><code>WordPiece</code> decoder</li></ul><p>Special token behavior:</p><ul><li><code>add_special_tokens=true</code> inserts <code>[CLS]</code> and <code>[SEP]</code> via post-processing.</li><li>Special tokens present verbatim in input text can also be matched via HF <code>added_tokens</code> patterns.</li></ul><p>KeemenaPreprocessing integration:</p><ul><li><code>tokenization_text = tokenization_view(tokenizer, clean_text)</code></li><li><code>encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)</code></li></ul><p>Export/reload flow:</p><ul><li><code>export_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)</code></li><li><code>load_hf_tokenizer_json(joinpath(out_dir, &quot;tokenizer.json&quot;))</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/presets/bert_wordpiece_hf.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_hf_bert_wordpiece_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_hf_bert_wordpiece_result"><code>KeemenaSubwords.Training.train_hf_bert_wordpiece_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">train_hf_bert_wordpiece_result(corpus; kwargs...) -&gt;
    TrainingResult{HuggingFaceJSONTokenizer,BertWordPieceTrainingConfig,BertWordPieceTrainingArtifacts}</code></pre><p>Train a BERT-style WordPiece tokenizer and return:</p><ul><li><code>tokenizer::HuggingFaceJSONTokenizer</code></li><li><code>config::BertWordPieceTrainingConfig</code></li><li><code>artifacts::BertWordPieceTrainingArtifacts</code></li></ul><p>The returned tokenizer includes <code>BertNormalizer</code>, <code>BertPreTokenizer</code>, <code>BertProcessing</code>, and <code>WordPiece</code> decoding, with special tokens exported as HF <code>added_tokens</code> for deterministic save/reload parity.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/presets/bert_wordpiece_hf.jl#L61-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_hf_roberta_bytebpe"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_hf_roberta_bytebpe"><code>KeemenaSubwords.Training.train_hf_roberta_bytebpe</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">train_hf_roberta_bytebpe(corpus; kwargs...) -&gt; HuggingFaceJSONTokenizer</code></pre><p>Train a RoBERTa-style ByteLevel BPE tokenizer and return a <code>HuggingFaceJSONTokenizer</code> pipeline composed of:</p><ul><li>ByteLevel pre-tokenization</li><li>RobertaProcessing (<code>&lt;s&gt; ... &lt;/s&gt;</code> insertion)</li><li>ByteLevel decoding</li></ul><p>Special token behavior:</p><ul><li><code>add_special_tokens=true</code> inserts BOS/EOS via RobertaProcessing.</li><li>Special tokens present verbatim in input text can be matched via HF <code>added_tokens</code> patterns.</li><li>By default the preset enables HF-style ByteLevel settings: <code>use_regex=true</code>, <code>add_prefix_space=true</code>, and <code>trim_offsets=true</code>.</li></ul><p>KeemenaPreprocessing integration:</p><ul><li><code>tokenization_text = tokenization_view(tokenizer, clean_text)</code></li><li><code>encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)</code></li></ul><p>Export/reload flow:</p><ul><li><code>export_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)</code></li><li><code>load_hf_tokenizer_json(joinpath(out_dir, &quot;tokenizer.json&quot;))</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/presets/roberta_bytebpe_hf.jl#L1-L25">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_hf_roberta_bytebpe_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_hf_roberta_bytebpe_result"><code>KeemenaSubwords.Training.train_hf_roberta_bytebpe_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">train_hf_roberta_bytebpe_result(corpus; kwargs...) -&gt;
    TrainingResult{HuggingFaceJSONTokenizer,RobertaByteBPETrainingConfig,RobertaByteBPETrainingArtifacts}</code></pre><p>Train a RoBERTa-style ByteLevel BPE tokenizer and return:</p><ul><li><code>tokenizer::HuggingFaceJSONTokenizer</code></li><li><code>config::RobertaByteBPETrainingConfig</code></li><li><code>artifacts::RobertaByteBPETrainingArtifacts</code></li></ul><p>The returned tokenizer wraps an inner trained <code>ByteBPETokenizer</code> and preserves a HF-native pipeline (<code>ByteLevel</code> pre-tokenizer/decoder + <code>RobertaProcessing</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/presets/roberta_bytebpe_hf.jl#L62-L73">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_hf_gpt2_bytebpe"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_hf_gpt2_bytebpe"><code>KeemenaSubwords.Training.train_hf_gpt2_bytebpe</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">train_hf_gpt2_bytebpe(corpus; kwargs...) -&gt; HuggingFaceJSONTokenizer</code></pre><p>Train a GPT-2 style ByteLevel BPE tokenizer and return a <code>HuggingFaceJSONTokenizer</code> pipeline composed of:</p><ul><li>No-op normalizer</li><li>ByteLevel pre-tokenization</li><li>ByteLevel post-processing (no BOS/EOS insertion)</li><li>ByteLevel decoding</li></ul><p>Special token behavior:</p><ul><li>By default, this preset uses a single special token: <code>special_tokens=Dict(:unk =&gt; &quot;&lt;|endoftext|&gt;&quot;)</code>.</li><li><code>add_special_tokens=true</code> does not change ids by default because GPT-2 style ByteLevel pipelines do not auto-insert BOS/EOS.</li><li>Special tokens present verbatim in input text can still be matched through HF <code>added_tokens</code> patterns.</li></ul><p>KeemenaPreprocessing integration:</p><ul><li><code>tokenization_text = tokenization_view(tokenizer, clean_text)</code></li><li><code>encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)</code></li></ul><p>Export/reload flow:</p><ul><li><code>export_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)</code></li><li><code>load_hf_tokenizer_json(joinpath(out_dir, &quot;tokenizer.json&quot;))</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/presets/gpt2_bytebpe_hf.jl#L1-L27">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.train_hf_gpt2_bytebpe_result"><a class="docstring-binding" href="#KeemenaSubwords.Training.train_hf_gpt2_bytebpe_result"><code>KeemenaSubwords.Training.train_hf_gpt2_bytebpe_result</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">train_hf_gpt2_bytebpe_result(corpus; kwargs...) -&gt;
    TrainingResult{HuggingFaceJSONTokenizer,GPT2ByteBPETrainingConfig,GPT2ByteBPETrainingArtifacts}</code></pre><p>Train a GPT-2 style ByteLevel BPE tokenizer and return:</p><ul><li><code>tokenizer::HuggingFaceJSONTokenizer</code></li><li><code>config::GPT2ByteBPETrainingConfig</code></li><li><code>artifacts::GPT2ByteBPETrainingArtifacts</code></li></ul><p>The returned tokenizer wraps an inner trained <code>ByteBPETokenizer</code> and preserves a HF-native ByteLevel pipeline. By default, exported HF JSON uses <code>model.unk_token = null</code> for GPT-2 compatibility while the internal Julia base tokenizer still uses a concrete unknown token string.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/presets/gpt2_bytebpe_hf.jl#L58-L71">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.write_training_manifest"><a class="docstring-binding" href="#KeemenaSubwords.Training.write_training_manifest"><code>KeemenaSubwords.Training.write_training_manifest</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">write_training_manifest(outdir, manifest)</code></pre><p>Write a <code>TrainingManifestV1</code> to <code>outdir/keemena_training_manifest.json</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_manifest.jl#L18-L22">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.read_training_manifest"><a class="docstring-binding" href="#KeemenaSubwords.Training.read_training_manifest"><code>KeemenaSubwords.Training.read_training_manifest</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">read_training_manifest(outdir) -&gt; TrainingManifestV1</code></pre><p>Read <code>outdir/keemena_training_manifest.json</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_manifest.jl#L40-L44">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.save_training_bundle"><a class="docstring-binding" href="#KeemenaSubwords.Training.save_training_bundle"><code>KeemenaSubwords.Training.save_training_bundle</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">save_training_bundle(result, outdir; export_format=:auto, overwrite=false)</code></pre><p>Export a trained tokenizer result and write a deterministic v1 manifest into <code>outdir</code> so the bundle can be reloaded later with <code>load_training_bundle</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_manifest.jl#L122-L127">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KeemenaSubwords.Training.load_training_bundle"><a class="docstring-binding" href="#KeemenaSubwords.Training.load_training_bundle"><code>KeemenaSubwords.Training.load_training_bundle</code></a> ‚Äî <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">load_training_bundle(outdir) -&gt; AbstractSubwordTokenizer</code></pre><p>Load a tokenizer bundle previously written by <code>save_training_bundle</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/75d1546662937dfed35b829acbcf593dacde3528/src/training/training_manifest.jl#L154-L158">source</a></section></details></article><h2 id="HF-BERT-WordPiece-Preset"><a class="docs-heading-anchor" href="#HF-BERT-WordPiece-Preset">HF BERT WordPiece Preset</a><a id="HF-BERT-WordPiece-Preset-1"></a><a class="docs-heading-anchor-permalink" href="#HF-BERT-WordPiece-Preset" title="Permalink"></a></h2><pre><code class="language-julia hljs">using KeemenaSubwords

corpus = [
    &quot;Hello, world!&quot;,
    &quot;Caf√© na√Øve fa√ßade&quot;,
    &quot;‰Ω†Â•Ω ‰∏ñÁïå&quot;,
]

tok = train_hf_bert_wordpiece(
    corpus;
    vocab_size=128,
    min_frequency=1,
    lowercase=true,
    strip_accents=nothing,
    handle_chinese_chars=true,
    clean_text=true,
)

export_tokenizer(tok, &quot;out_hf_bert&quot;; format=:hf_tokenizer_json)
reloaded = load_hf_tokenizer_json(&quot;out_hf_bert/tokenizer.json&quot;)</code></pre><h2 id="HF-RoBERTa-ByteBPE-Preset"><a class="docs-heading-anchor" href="#HF-RoBERTa-ByteBPE-Preset">HF RoBERTa ByteBPE Preset</a><a id="HF-RoBERTa-ByteBPE-Preset-1"></a><a class="docs-heading-anchor-permalink" href="#HF-RoBERTa-ByteBPE-Preset" title="Permalink"></a></h2><pre><code class="language-julia hljs">using KeemenaSubwords

corpus = [
    &quot;hello world&quot;,
    &quot;hello, world!&quot;,
    &quot;caf√© costs 5 euros&quot;,
]

tok = train_hf_roberta_bytebpe(
    corpus;
    vocab_size=384,
    min_frequency=1,
)

export_tokenizer(tok, &quot;out_hf_roberta&quot;; format=:hf_tokenizer_json)
reloaded = load_hf_tokenizer_json(&quot;out_hf_roberta/tokenizer.json&quot;)</code></pre><p>RoBERTa preset defaults are chosen for HF-style ByteLevel behavior:</p><ul><li><code>use_regex=true</code> applies GPT-2 ByteLevel regex splitting.</li><li><code>add_prefix_space=true</code> matches RoBERTa-style leading-space handling.</li><li><code>trim_offsets=true</code> trims span edges for whitespace while preserving the offsets contract: non-span specials use sentinel <code>(0,0)</code>, while trimmed real tokens may become empty but remain in-bounds spans like <code>(k,k)</code> (never sentinel).</li></ul><h2 id="HF-GPT-2-ByteBPE-Preset"><a class="docs-heading-anchor" href="#HF-GPT-2-ByteBPE-Preset">HF GPT-2 ByteBPE Preset</a><a id="HF-GPT-2-ByteBPE-Preset-1"></a><a class="docs-heading-anchor-permalink" href="#HF-GPT-2-ByteBPE-Preset" title="Permalink"></a></h2><pre><code class="language-julia hljs">using KeemenaSubwords

corpus = [
    &quot;Hello my friend, how is your day going?&quot;,
    &quot;caf√© üôÇ&quot;,
]

tok = train_hf_gpt2_bytebpe(
    corpus;
    vocab_size=384,
    min_frequency=1,
)

export_tokenizer(tok, &quot;out_hf_gpt2&quot;; format=:hf_tokenizer_json)
reloaded = load_hf_tokenizer_json(&quot;out_hf_gpt2/tokenizer.json&quot;)</code></pre><h2 id="Note-on-pretokenizer"><a class="docs-heading-anchor" href="#Note-on-pretokenizer">Note on pretokenizer</a><a id="Note-on-pretokenizer-1"></a><a class="docs-heading-anchor-permalink" href="#Note-on-pretokenizer" title="Permalink"></a></h2><ul><li><code>pretokenizer</code> is used only during training to split input text into units for frequency counts.</li><li>Trained tokenizers do not persist or apply the training <code>pretokenizer</code> at runtime.</li><li>For consistent behavior, apply equivalent preprocessing upstream (for example via KeemenaPreprocessing) before calling <code>encode</code>/<code>encode_result</code>.</li><li>ByteBPE exports as <code>vocab.txt + merges.txt</code>; when reloading exported files, use <code>format=:bytebpe</code> if format auto-detection is ambiguous.</li></ul><h2 id="Training-Bundles"><a class="docs-heading-anchor" href="#Training-Bundles">Training Bundles</a><a id="Training-Bundles-1"></a><a class="docs-heading-anchor-permalink" href="#Training-Bundles" title="Permalink"></a></h2><pre><code class="language-julia hljs">using KeemenaSubwords

corpus = [&quot;hello world&quot;, &quot;caf√© costs 5&quot;]
result = train_wordpiece_result(corpus; vocab_size=96, min_frequency=1)

save_training_bundle(result, &quot;out_bundle&quot;)
reloaded = load_training_bundle(&quot;out_bundle&quot;)

encode(reloaded, &quot;hello caf√©&quot;; add_special_tokens=false)</code></pre><p><code>save_training_bundle</code> writes exported tokenizer files plus <code>keemena_training_manifest.json</code>, so reload does not require remembering loader kwargs. Offsets behavior remains unchanged and compatible with <code>tokenization_view(...)</code> + <code>encode_result(...; assume_normalized=true)</code>.</p><p>Current behavior:</p><ul><li>SentencePiece training supports both <code>model_type=:unigram</code> and <code>model_type=:bpe</code>.</li><li>Unigram training defaults to SentencePiece-style <code>whitespace_marker=&quot;‚ñÅ&quot;</code> so multi-word text can round-trip through <code>decode(encode(...))</code>.</li><li>If <code>whitespace_marker=&quot;&quot;</code>, runtime Unigram tokenization is still word-split, so decoding may collapse spaces in multi-word text (for example <code>&quot;hello world&quot;</code> -&gt; <code>&quot;helloworld&quot;</code>).</li></ul><p>The pretrained-tokenizer APIs (<code>load_tokenizer</code>, <code>tokenize</code>, <code>encode</code>, <code>encode_result</code>, <code>decode</code>) remain stable and independent from training codepaths.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../loading/">¬´ Loading</a><a class="docs-footer-nextpage" href="../formats/">Formats ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 16 February 2026 05:36">Monday 16 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
