var documenterSearchIndex = {"docs":
[{"location":"normalization_offsets_contract/#Normalization-and-Offsets-Contract","page":"Normalization & Offsets","title":"Normalization and Offsets Contract","text":"This document is the canonical contract for normalization and offsets behavior in KeemenaSubwords when integrated with KeemenaPreprocessing.","category":"section"},{"location":"normalization_offsets_contract/#Normalization-Ownership","page":"Normalization & Offsets","title":"Normalization Ownership","text":"Pipeline normalization (KeemenaPreprocessing): produces clean_text.\nTokenizer intrinsic normalization (KeemenaSubwords): produces tokenization_text.\n\nUse:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\n\nThen call:\n\nresult = encode_result(\n    tokenizer,\n    tokenization_text;\n    assume_normalized=true,\n    return_offsets=true,\n    return_masks=true,\n    add_special_tokens=true,\n)\n\nWhen assume_normalized=true, KeemenaSubwords must not re-run tokenizer intrinsic normalization.","category":"section"},{"location":"normalization_offsets_contract/#Offset-Convention","page":"Normalization & Offsets","title":"Offset Convention","text":"coordinate unit: UTF-8 codeunits\nindex base: 1-based\nspan style: half-open [start, stop)\nvalid bounds for spanful offsets:\n1 <= start <= stop <= ncodeunits(text) + 1\n\nProgrammatic helpers:\n\noffsets_coordinate_system() == :utf8_codeunits\noffsets_index_base() == 1\noffsets_span_style() == :half_open\noffsets_sentinel() == (0, 0)\nhas_span(offset) is true iff offset != (0, 0)\nhas_nonempty_span(offset) is true iff the offset is spanful and stop > start\nspan_ncodeunits(offset) returns span length in codeunits (0 for sentinel/empty)","category":"section"},{"location":"normalization_offsets_contract/#Sentinel-and-Special-Tokens","page":"Normalization & Offsets","title":"Sentinel and Special Tokens","text":"Sentinel:\n\n(0, 0) means \"no source-text span\".\nIn a 1-based scheme, (0, 0) is out-of-range and unambiguous.\n\nSpecial token semantics:\n\nInserted special tokens (TemplateProcessing/post-processor inserted):\nspecial_tokens_mask[i] == 1\noffsets[i] == (0, 0)\nSpecial tokens matched from user text as added tokens:\nspecial_tokens_mask[i] == 1\noffsets[i] is a real span into the input text (offsets[i] != (0, 0))\n\nImportant:\n\nspecial_tokens_mask marks special-token identity.\nSpan participation is determined by offsets/sentinel, not by mask alone.","category":"section"},{"location":"normalization_offsets_contract/#Alignment-Rule-for-KeemenaPreprocessing","page":"Normalization & Offsets","title":"Alignment Rule for KeemenaPreprocessing","text":"Canonical coordinate system is tokenization_text.\n\nKeemenaPreprocessing should:\n\nProduce clean_text via pipeline normalization.\nProduce tokenization_text = tokenization_view(tokenizer, clean_text).\nCall encode_result(tokenizer, tokenization_text; assume_normalized=true, ...).\nCompute both word offsets and subword offsets on tokenization_text.\nIgnore sentinel offsets (0, 0) during span alignment.\nDo not drop all mask==1 tokens blindly; present-in-text special tokens may have real spans.\n\nRecommended span participation policy:\n\nParticipate in span alignment iff has_nonempty_span(offset).","category":"section"},{"location":"normalization_offsets_contract/#Downstream-Safe-Span-Inspection","page":"Normalization & Offsets","title":"Downstream-Safe Span Inspection","text":"Offsets are codeunit spans. Do not assume they are always valid Julia string slicing boundaries, especially for byte-level tokenizers on multibyte text.\n\nUse these helpers for robust downstream handling:\n\nspan_codeunits(text, offset):\nreturns UInt8[] for sentinel/empty spans,\nreturns the exact byte slice for non-empty spans.\ntry_span_substring(text, offset):\nreturns \"\" for sentinel/empty spans,\nreturns String only when both boundaries are valid Julia string boundaries,\nreturns nothing otherwise.\nis_valid_string_boundary(text, idx) can be used to inspect boundary validity.\noffsets_are_nonoverlapping(offsets; ignore_sentinel=true, ignore_empty=true) validates a downstream non-overlap invariant.","category":"section"},{"location":"normalization_offsets_contract/#Boundary-Validity-Expectations-By-Tokenizer-Family","page":"Normalization & Offsets","title":"Boundary Validity Expectations By Tokenizer Family","text":"Non-byte-level tokenizers (for example WordPiece, SentencePiece, Unigram TSV, and non-byte-level HF tokenizer.json pipelines) are expected to produce spanful offsets that land on valid Julia string boundaries.\nByte-level tokenizers (for example ByteBPE and HF ByteLevel pretokenizers) may produce non-boundary offsets on multibyte Unicode inputs.\n\nDownstream interpretation:\n\nWhen try_span_substring(text, offset) returns nothing for a byte-level multibyte case, treat this as expected \"unsafe to slice\" behavior.\nUse span_codeunits(text, offset) for byte-accurate span inspection regardless of string-boundary validity.","category":"section"},{"location":"normalization_offsets_contract/#Strict-Validator-Helpers-(Maintainer-Debugging)","page":"Normalization & Offsets","title":"Strict Validator Helpers (Maintainer Debugging)","text":"For tokenizer development and regression debugging, KeemenaSubwords also exposes strict contract validators:\n\nvalidate_offsets_contract(text, offsets; require_string_boundaries=false): returns Bool without throwing.\nassert_offsets_contract(text, offsets; require_string_boundaries=false): throws ArgumentError on first violation with a targeted message.\n\nUse require_string_boundaries=true when validating string-level tokenizers where spanful offsets are expected to land on valid Julia string boundaries.","category":"section"},{"location":"models/#Built-In-Models","page":"Built-In Models","title":"Built-In Models","text":"using KeemenaSubwords\n\navailable_models()\navailable_models(format=:tiktoken)\navailable_models(format=:bpe_gpt2)\navailable_models(format=:hf_tokenizer_json)\navailable_models(family=:qwen)\navailable_models(family=:mistral)\navailable_models(distribution=:artifact_public)\navailable_models(distribution=:installable_gated)\navailable_models(shipped=true)\n\ndescribe_model(:core_bpe_en)\ndescribe_model(:core_wordpiece_en)\ndescribe_model(:core_sentencepiece_unigram_en)\ndescribe_model(:tiktoken_o200k_base)\ndescribe_model(:openai_gpt2_bpe)\ndescribe_model(:bert_base_uncased_wordpiece)\ndescribe_model(:bert_base_multilingual_cased_wordpiece)\ndescribe_model(:t5_small_sentencepiece_unigram)\ndescribe_model(:mistral_v1_sentencepiece)\ndescribe_model(:mistral_v3_sentencepiece)\ndescribe_model(:phi2_bpe)\ndescribe_model(:qwen2_5_bpe)\ndescribe_model(:roberta_base_bpe)\ndescribe_model(:xlm_roberta_base_sentencepiece_bpe)\ndescribe_model(:llama3_8b_tokenizer)\nrecommended_defaults_for_llms()\n\nmodel_path(:core_bpe_en)\n\nThe table below is generated from the same registry used by available_models() and describe_model(...).\n\nGenerated from the registry by `tools/syncreadmemodels.jl(excluding:userlocal` entries)._","category":"section"},{"location":"models/#bpe-/-core","page":"Built-In Models","title":"bpe / core","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:core_bpe_en shipped MIT in-repo/core in-repo:core vocab.txt, merges.txt Tiny built-in English classic BPE model (vocab.txt + merges.txt).","category":"section"},{"location":"models/#bpe_gpt2-/-openai","page":"Built-In Models","title":"bpe_gpt2 / openai","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:openai_gpt2_bpe artifact_public MIT openaipublic/gpt-2 openaipublic:gpt-2/encodings/main vocab.json + merges.txt, encoder.json + vocab.bpe OpenAI GPT-2 byte-level BPE assets (encoder.json + vocab.bpe).","category":"section"},{"location":"models/#bpe_gpt2-/-phi","page":"Built-In Models","title":"bpe_gpt2 / phi","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:phi2_bpe artifact_public MIT microsoft/phi-2 huggingface:microsoft/phi-2@810d367871c1d460086d9f82db8696f2e0a0fcd0 vocab.json + merges.txt, encoder.json + vocab.bpe Microsoft Phi-2 GPT2-style tokenizer files (vocab.json + merges.txt).","category":"section"},{"location":"models/#bpe_gpt2-/-roberta","page":"Built-In Models","title":"bpe_gpt2 / roberta","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:roberta_base_bpe artifact_public MIT FacebookAI/roberta-base huggingface:FacebookAI/roberta-base@e2da8e2f811d1448a5b465c236feacd80ffbac7b vocab.json + merges.txt, encoder.json + vocab.bpe RoBERTa-base byte-level BPE tokenizer files (vocab.json + merges.txt).","category":"section"},{"location":"models/#hf_tokenizer_json-/-llama","page":"Built-In Models","title":"hf_tokenizer_json / llama","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:llama3_8b_tokenizer installable_gated Llama-3.1-Community-License meta-llama/Meta-Llama-3-8B-Instruct huggingface:meta-llama/Meta-Llama-3-8B-Instruct@main tokenizer.json (preferred), vocab.json + merges.txt (fallback) Meta Llama 3 8B tokenizer (gated; install with install_model!).","category":"section"},{"location":"models/#hf_tokenizer_json-/-qwen","page":"Built-In Models","title":"hf_tokenizer_json / qwen","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:qwen2_5_bpe artifact_public Apache-2.0 Qwen/Qwen2.5-7B huggingface:Qwen/Qwen2.5-7B@d149729398750b98c0af14eb82c78cfe92750796 tokenizer.json (preferred), vocab.json + merges.txt (fallback) Qwen2.5 BPE tokenizer assets (tokenizer.json with vocab/merges fallback).","category":"section"},{"location":"models/#sentencepiece_model-/-core","page":"Built-In Models","title":"sentencepiece_model / core","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:core_sentencepiece_unigram_en shipped MIT in-repo/core in-repo:core spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Tiny built-in SentencePiece Unigram model (.model).","category":"section"},{"location":"models/#sentencepiece_model-/-llama","page":"Built-In Models","title":"sentencepiece_model / llama","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:llama2_tokenizer installable_gated Llama-2-Community-License meta-llama/Llama-2-7b-hf huggingface:meta-llama/Llama-2-7b-hf@main spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Meta Llama 2 tokenizer (gated; install with install_model!).","category":"section"},{"location":"models/#sentencepiece_model-/-mistral","page":"Built-In Models","title":"sentencepiece_model / mistral","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:mistral_v1_sentencepiece artifact_public Apache-2.0 mistralai/Mixtral-8x7B-Instruct-v0.1 huggingface:mistralai/Mixtral-8x7B-Instruct-v0.1@eba92302a2861cdc0098cc54bc9f17cb2c47eb61 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Mistral/Mixtral tokenizer.model SentencePiece model.\n:mistral_v3_sentencepiece artifact_public Apache-2.0 mistralai/Mistral-7B-Instruct-v0.3 huggingface:mistralai/Mistral-7B-Instruct-v0.3@c170c708c41dac9275d15a8fff4eca08d52bab71 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Mistral-7B-Instruct-v0.3 tokenizer.model.v3 SentencePiece model.","category":"section"},{"location":"models/#sentencepiece_model-/-t5","page":"Built-In Models","title":"sentencepiece_model / t5","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:t5_small_sentencepiece_unigram artifact_public Apache-2.0 google-t5/t5-small huggingface:google-t5/t5-small@df1b051c49625cf57a3d0d8d3863ed4d13564fe4 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Hugging Face google-t5/t5-small SentencePiece model (Unigram).","category":"section"},{"location":"models/#sentencepiece_model-/-xlm_roberta","page":"Built-In Models","title":"sentencepiece_model / xlm_roberta","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:xlm_roberta_base_sentencepiece_bpe artifact_public MIT FacebookAI/xlm-roberta-base huggingface:FacebookAI/xlm-roberta-base@e73636d4f797dec63c3081bb6ed5c7b0bb3f2089 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model XLM-RoBERTa-base sentencepiece.bpe.model file.","category":"section"},{"location":"models/#tiktoken-/-openai","page":"Built-In Models","title":"tiktoken / openai","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:tiktoken_cl100k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/cl100k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken cl100k_base encoding.\n:tiktoken_o200k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/o200k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken o200k_base encoding.\n:tiktoken_p50k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/p50k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken p50k_base encoding.\n:tiktoken_r50k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/r50k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken r50k_base encoding.","category":"section"},{"location":"models/#wordpiece_vocab-/-bert","page":"Built-In Models","title":"wordpiece_vocab / bert","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:bert_base_multilingual_cased_wordpiece artifact_public Apache-2.0 google-bert/bert-base-multilingual-cased huggingface:google-bert/bert-base-multilingual-cased@3f076fdb1ab68d5b2880cb87a0886f315b8146f8 vocab.txt Hugging Face bert-base-multilingual-cased WordPiece vocabulary.\n:bert_base_uncased_wordpiece artifact_public Apache-2.0 bert-base-uncased huggingface:bert-base-uncased@86b5e0934494bd15c9632b12f734a8a67f723594 vocab.txt Hugging Face bert-base-uncased WordPiece vocabulary.","category":"section"},{"location":"models/#wordpiece_vocab-/-core","page":"Built-In Models","title":"wordpiece_vocab / core","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:core_wordpiece_en shipped MIT in-repo/core in-repo:core vocab.txt Tiny built-in English WordPiece model.\n\ndescribe_model(key) includes provenance metadata such as license, family, distribution, upstream_repo, upstream_ref, and upstream_files.\n\nBuilt-ins resolve from artifact paths when present, with in-repo fallback model files only for tiny :core_* assets.\n\nprefetch_models(recommended_defaults_for_llms())","category":"section"},{"location":"training/#Training-(Experimental)","page":"Training","title":"Training (Experimental)","text":"Training support is currently experimental and intentionally separated from the pretrained tokenizer loading/encoding workflows.\n\nAvailable now:\n\ntrain_bpe(...)\ntrain_bytebpe(...)\ntrain_unigram(...)\ntrain_wordpiece(...)\ntrain_wordpiece_result(...)\ntrain_sentencepiece(...)\ntrain_sentencepiece_result(...)\ntrain_hf_bert_wordpiece(...)\ntrain_hf_bert_wordpiece_result(...)","category":"section"},{"location":"training/#Training-API","page":"Training","title":"Training API","text":"","category":"section"},{"location":"training/#HF-BERT-WordPiece-Preset","page":"Training","title":"HF BERT WordPiece Preset","text":"using KeemenaSubwords\n\ncorpus = [\n    \"Hello, world!\",\n    \"Café naïve façade\",\n    \"你好 世界\",\n]\n\ntok = train_hf_bert_wordpiece(\n    corpus;\n    vocab_size=128,\n    min_frequency=1,\n    lowercase=true,\n    strip_accents=nothing,\n    handle_chinese_chars=true,\n    clean_text=true,\n)\n\nexport_tokenizer(tok, \"out_hf_bert\"; format=:hf_tokenizer_json)\nreloaded = load_hf_tokenizer_json(\"out_hf_bert/tokenizer.json\")","category":"section"},{"location":"training/#Note-on-pretokenizer","page":"Training","title":"Note on pretokenizer","text":"pretokenizer is used only during training to split input text into units for frequency counts.\nTrained tokenizers do not persist or apply the training pretokenizer at runtime.\nFor consistent behavior, apply equivalent preprocessing upstream (for example via KeemenaPreprocessing) before calling encode/encode_result.\nByteBPE exports as vocab.txt + merges.txt; when reloading exported files, use format=:bytebpe if format auto-detection is ambiguous.\n\nCurrent behavior:\n\nSentencePiece training supports both model_type=:unigram and model_type=:bpe.\nUnigram training defaults to SentencePiece-style whitespace_marker=\"▁\" so multi-word text can round-trip through decode(encode(...)).\nIf whitespace_marker=\"\", runtime Unigram tokenization is still word-split, so decoding may collapse spaces in multi-word text (for example \"hello world\" -> \"helloworld\").\n\nThe pretrained-tokenizer APIs (load_tokenizer, tokenize, encode, encode_result, decode) remain stable and independent from training codepaths.","category":"section"},{"location":"training/#KeemenaSubwords.Training.train_bpe","page":"Training","title":"KeemenaSubwords.Training.train_bpe","text":"Train a character-level BPE tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_bpe_result","page":"Training","title":"KeemenaSubwords.Training.train_bpe_result","text":"Train a character-level BPE tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_bytebpe","page":"Training","title":"KeemenaSubwords.Training.train_bytebpe","text":"Train a byte-level BPE tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_bytebpe_result","page":"Training","title":"KeemenaSubwords.Training.train_bytebpe_result","text":"Train a byte-level BPE tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_unigram","page":"Training","title":"KeemenaSubwords.Training.train_unigram","text":"High-level Unigram training entry point.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_unigram_result","page":"Training","title":"KeemenaSubwords.Training.train_unigram_result","text":"Train a Unigram tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_wordpiece","page":"Training","title":"KeemenaSubwords.Training.train_wordpiece","text":"Train a WordPiece tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_wordpiece_result","page":"Training","title":"KeemenaSubwords.Training.train_wordpiece_result","text":"Train a WordPiece tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_sentencepiece","page":"Training","title":"KeemenaSubwords.Training.train_sentencepiece","text":"Train a SentencePiece tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_sentencepiece_result","page":"Training","title":"KeemenaSubwords.Training.train_sentencepiece_result","text":"Train a SentencePiece tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_bert_wordpiece","page":"Training","title":"KeemenaSubwords.Training.train_hf_bert_wordpiece","text":"train_hf_bert_wordpiece(corpus; kwargs...) -> HuggingFaceJSONTokenizer\n\nTrain a BERT-style WordPiece tokenizer and return a HuggingFaceJSONTokenizer pipeline composed of:\n\nBertNormalizer\nBertPreTokenizer\nBertProcessing (CLS/SEP insertion)\nWordPiece decoder\n\nSpecial token behavior:\n\nadd_special_tokens=true inserts [CLS] and [SEP] via post-processing.\nSpecial tokens present verbatim in input text can also be matched via HF added_tokens patterns.\n\nKeemenaPreprocessing integration:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\nencode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)\n\nExport/reload flow:\n\nexport_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)\nload_hf_tokenizer_json(joinpath(out_dir, \"tokenizer.json\"))\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_bert_wordpiece_result","page":"Training","title":"KeemenaSubwords.Training.train_hf_bert_wordpiece_result","text":"train_hf_bert_wordpiece_result(corpus; kwargs...) ->\n    TrainingResult{HuggingFaceJSONTokenizer,BertWordPieceTrainingConfig,BertWordPieceTrainingArtifacts}\n\nTrain a BERT-style WordPiece tokenizer and return:\n\ntokenizer::HuggingFaceJSONTokenizer\nconfig::BertWordPieceTrainingConfig\nartifacts::BertWordPieceTrainingArtifacts\n\nThe returned tokenizer includes BertNormalizer, BertPreTokenizer, BertProcessing, and WordPiece decoding, with special tokens exported as HF added_tokens for deterministic save/reload parity.\n\n\n\n\n\n","category":"function"},{"location":"loading_local/#Loading-Tokenizers-From-Local-Paths","page":"Loading Local","title":"Loading Tokenizers From Local Paths","text":"Use explicit loader functions when you know the file contract. Use load_tokenizer(path; format=:auto) only when auto-detection is preferred.\n\nNamed-spec convention:\n\nuse path as the canonical key for single-file formats,\nkeep format-specific pair keys for multi-file formats (vocab_json + merges_txt, encoder_json + vocab_bpe).\nbackward-compatible aliases (vocab_txt, model_file, encoding_file, tokenizer_json) are still accepted.","category":"section"},{"location":"loading_local/#1)-GPT-2-/-RoBERTa-style-BPE-(vocab.json-merges.txt)","page":"Loading Local","title":"1) GPT-2 / RoBERTa style BPE (vocab.json + merges.txt)","text":"tok = load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n\n# equivalent named spec\nload_tokenizer((format=:bpe_gpt2, vocab_json=\"/path/to/vocab.json\", merges_txt=\"/path/to/merges.txt\"))","category":"section"},{"location":"loading_local/#2)-OpenAI-encoder-variant-(encoder.json-vocab.bpe)","page":"Loading Local","title":"2) OpenAI encoder variant (encoder.json + vocab.bpe)","text":"tok = load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n\n# equivalent named spec\nload_tokenizer((format=:bpe_encoder, encoder_json=\"/path/to/encoder.json\", vocab_bpe=\"/path/to/vocab.bpe\"))","category":"section"},{"location":"loading_local/#3)-Classic-BPE-/-Byte-level-BPE-(vocab.txt-merges.txt)","page":"Loading Local","title":"3) Classic BPE / Byte-level BPE (vocab.txt + merges.txt)","text":"classic = load_bpe(\"/path/to/model_dir\")\nbyte_level = load_bytebpe(\"/path/to/model_dir\")","category":"section"},{"location":"loading_local/#4)-WordPiece-(vocab.txt)","page":"Loading Local","title":"4) WordPiece (vocab.txt)","text":"wp = load_wordpiece(\"/path/to/vocab.txt\"; continuation_prefix=\"##\")\n\n# register via canonical key\nregister_local_model!(\n    :my_wordpiece,\n    (format=:wordpiece_vocab, path=\"/path/to/vocab.txt\");\n    description=\"local WordPiece\",\n)","category":"section"},{"location":"loading_local/#5)-SentencePiece-(.model,-.model.v3,-sentencepiece.bpe.model)","page":"Loading Local","title":"5) SentencePiece (.model, .model.v3, sentencepiece.bpe.model)","text":"load_sentencepiece accepts either:\n\nstandard SentencePiece binary model files,\nor Keemena text-exported SentencePiece files (same filename patterns).\n\nsp_auto = load_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nsp_uni = load_sentencepiece(\"/path/to/spm.model\"; kind=:unigram)\nsp_bpe = load_sentencepiece(\"/path/to/tokenizer.model.v3\"; kind=:bpe)\n\nregister_local_model!(:my_sp, (format=:sentencepiece_model, path=\"/path/to/tokenizer.model\"))","category":"section"},{"location":"loading_local/#6)-tiktoken-(*.tiktoken-or-text-tokenizer.model)","page":"Loading Local","title":"6) tiktoken (*.tiktoken or text tokenizer.model)","text":"tt = load_tiktoken(\"/path/to/o200k_base.tiktoken\")\nllama3_style = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\n\nregister_local_model!(:my_tiktoken, (format=:tiktoken, path=\"/path/to/tokenizer.model\"))","category":"section"},{"location":"loading_local/#7)-Hugging-Face-tokenizer.json","page":"Loading Local","title":"7) Hugging Face tokenizer.json","text":"hf = load_hf_tokenizer_json(\"/path/to/tokenizer.json\")\n\nregister_local_model!(:my_hf, (format=:hf_tokenizer_json, path=\"/path/to/tokenizer.json\"))","category":"section"},{"location":"loading_local/#8)-Generic-auto-detect-override","page":"Loading Local","title":"8) Generic auto-detect + override","text":"auto_tok = load_tokenizer(\"/path/to/model_dir\")\nforced = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"loading_local/#9)-Explicit-FilesSpec-objects","page":"Loading Local","title":"9) Explicit FilesSpec objects","text":"spec = FilesSpec(\n    format=:bpe_gpt2,\n    vocab_json=\"/path/to/vocab.json\",\n    merges_txt=\"/path/to/merges.txt\",\n)\ntok = load_tokenizer(spec)\nregister_local_model!(:my_bpe, spec; description=\"explicit file spec\")","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Explicit-Loader-APIs","page":"API Reference","title":"Explicit Loader APIs","text":"Structured encoding and file-spec APIs are also part of the public surface: TokenizationResult, FilesSpec, normalize, tokenization_view, requires_tokenizer_normalization, offsets_coordinate_system, offsets_index_base, offsets_span_style, offsets_sentinel, has_span, has_nonempty_span, span_ncodeunits, span_codeunits, is_valid_string_boundary, try_span_substring, offsets_are_nonoverlapping, validate_offsets_contract, assert_offsets_contract, encode_result, encode_batch_result.","category":"section"},{"location":"api/#Registry-and-Installation-APIs","page":"API Reference","title":"Registry and Installation APIs","text":"register_external_model! remains available as a deprecated compatibility alias; prefer register_local_model! in new code.","category":"section"},{"location":"api/#Full-Exported-API","page":"API Reference","title":"Full Exported API","text":"","category":"section"},{"location":"api/#KeemenaSubwords.load_bpe","page":"API Reference","title":"KeemenaSubwords.load_bpe","text":"Load a BPE tokenizer from either a directory (vocab.txt + merges.txt) or a vocab file path.\n\n\n\n\n\nLoad a BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_bytebpe","page":"API Reference","title":"KeemenaSubwords.load_bytebpe","text":"Load a byte-level BPE tokenizer from a directory (vocab.txt + merges.txt) or vocab path.\n\n\n\n\n\nLoad a byte-level BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_bpe_gpt2","page":"API Reference","title":"KeemenaSubwords.load_bpe_gpt2","text":"Load GPT-2 / RoBERTa style BPE from vocab.json + merges.txt.\n\nExample: load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_bpe_encoder","page":"API Reference","title":"KeemenaSubwords.load_bpe_encoder","text":"Load GPT-2 encoder variant from encoder.json + vocab.bpe.\n\nExample: load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_unigram","page":"API Reference","title":"KeemenaSubwords.load_unigram","text":"Load a Unigram tokenizer from unigram.tsv (file or directory).\n\nExpected format (tab-separated): token<TAB>score[<TAB>special_symbol]\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_wordpiece","page":"API Reference","title":"KeemenaSubwords.load_wordpiece","text":"Load a WordPiece tokenizer from a vocab file path or a directory containing vocab.txt.\n\nExamples:\n\nload_wordpiece(\"/path/to/vocab.txt\")\nload_wordpiece(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_sentencepiece","page":"API Reference","title":"KeemenaSubwords.load_sentencepiece","text":"Load a SentencePiece .model file.\n\nSupported inputs:\n\nstandard SentencePiece binary protobuf .model/.model.v3 payloads\nKeemena text-exported model files:\nkey/value lines (type=unigram|bpe, whitespace_marker=▁, unk_token=<unk>)\npiece rows: piece<TAB>token<TAB>score[<TAB>special_symbol]\nbpe merge rows (for type=bpe): merge<TAB>left<TAB>right\n\nExamples:\n\nload_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nload_sentencepiece(\"/path/to/tokenizer.model.v3\"; kind=:bpe)\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_tiktoken","page":"API Reference","title":"KeemenaSubwords.load_tiktoken","text":"Load a tiktoken encoding file (*.tiktoken).\n\nThe expected format is line-based: <base64_token_bytes><space><rank> where ranks are non-negative integers.\n\nExamples:\n\nload_tiktoken(\"/path/to/o200k_base.tiktoken\")\nload_tiktoken(\"/path/to/tokenizer.model\") (when file contains tiktoken text lines)\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_hf_tokenizer_json","page":"API Reference","title":"KeemenaSubwords.load_hf_tokenizer_json","text":"Load a Hugging Face tokenizer.json tokenizer in pure Julia.\n\nExpected files:\n\ntokenizer.json directly, or\na directory containing tokenizer.json.\n\nExamples:\n\nload_hf_tokenizer_json(\"/path/to/tokenizer.json\")\nload_hf_tokenizer_json(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_tokenizer","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer by built-in model name.\n\n\n\n\n\nLoad tokenizer from file system path.\n\nCommon format contracts:\n\n:hf_tokenizer_json -> tokenizer.json\n:bpe_gpt2 -> vocab.json + merges.txt\n:bpe_encoder -> encoder.json + vocab.bpe\n:wordpiece / :wordpiece_vocab -> vocab.txt\n:sentencepiece_model -> *.model / *.model.v3 / sentencepiece.bpe.model\n:tiktoken -> *.tiktoken or tiktoken-text tokenizer.model\n\nExamples:\n\nload_tokenizer(\"/path/to/model_dir\")\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.json\"; format=:hf_tokenizer_json)\n\n\n\n\n\nLoad tokenizer from explicit (vocab_path, merges_path) tuple.\n\nThis tuple form is for classic BPE/byte-level BPE (vocab.txt + merges.txt) or explicit JSON-pair loaders (vocab.json + merges.txt, encoder.json + vocab.bpe) when accompanied by format.\n\n\n\n\n\nLoad tokenizer from a named specification.\n\nExamples:\n\n(format=:wordpiece, path=\"/.../vocab.txt\")\n(format=:hf_tokenizer_json, path=\"/.../tokenizer.json\")\n(format=:unigram, path=\"/.../unigram.tsv\")\n(format=:bpe_gpt2, vocab_json=\"/.../vocab.json\", merges_txt=\"/.../merges.txt\")\n(format=:bpe_encoder, encoder_json=\"/.../encoder.json\", vocab_bpe=\"/.../vocab.bpe\")\n(format=:wordpiece, vocab_txt=\"/.../vocab.txt\") (alias)\n(format=:sentencepiece_model, model_file=\"/.../tokenizer.model\") (alias)\n(format=:tiktoken, encoding_file=\"/.../o200k_base.tiktoken\") (alias)\n(format=:hf_tokenizer_json, tokenizer_json=\"/.../tokenizer.json\") (alias)\n(format=:unigram, unigram_tsv=\"/.../unigram.tsv\") (alias)\n\n\n\n\n\nLoad tokenizer from a FilesSpec.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.detect_tokenizer_format","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_format","text":"Detect tokenizer format from a local file or directory.\n\nReturns one of symbols such as :hf_tokenizer_json, :bpe_gpt2, :bpe_encoder, :sentencepiece_model, :tiktoken, :wordpiece, :bpe, or :unigram.\n\nExamples:\n\ndetect_tokenizer_format(\"/path/to/model_dir\")\ndetect_tokenizer_format(\"/path/to/tokenizer.model\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.detect_tokenizer_files","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_files","text":"Inspect a tokenizer directory and return detected candidate files.\n\nExample: detect_tokenizer_files(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.available_models","page":"API Reference","title":"KeemenaSubwords.available_models","text":"List available built-in model names.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.describe_model","page":"API Reference","title":"KeemenaSubwords.describe_model","text":"Describe a built-in model.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.model_path","page":"API Reference","title":"KeemenaSubwords.model_path","text":"Resolve built-in model name to on-disk path.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.prefetch_models","page":"API Reference","title":"KeemenaSubwords.prefetch_models","text":"Ensure artifact-backed built-in models are present on disk.\n\nReturns a dictionary of key => is_available.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.register_local_model!","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register a local tokenizer path under a symbolic key and persist it in the cache registry.\n\n\n\n\n\nRegister local model files by explicit specification.\n\n\n\n\n\nRegister local model files from a FilesSpec.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.install_model!","page":"API Reference","title":"KeemenaSubwords.install_model!","text":"Install an installable-gated tokenizer into the user cache and register it by key.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.install_llama2_tokenizer!","page":"API Reference","title":"KeemenaSubwords.install_llama2_tokenizer!","text":"Install the gated LLaMA 2 tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama2_tokenizer; ...).\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.install_llama3_8b_tokenizer!","page":"API Reference","title":"KeemenaSubwords.install_llama3_8b_tokenizer!","text":"Install the gated LLaMA 3 8B tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama3_8b_tokenizer; ...).\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.download_hf_files","page":"API Reference","title":"KeemenaSubwords.download_hf_files","text":"Download selected files from a Hugging Face repository revision into cache.\n\nThis helper is opt-in and useful for user-managed / gated tokenizers.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.recommended_defaults_for_llms","page":"API Reference","title":"KeemenaSubwords.recommended_defaults_for_llms","text":"Recommended built-in keys for LLM-oriented default prefetching.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.AbstractSubwordTokenizer","page":"API Reference","title":"KeemenaSubwords.AbstractSubwordTokenizer","text":"Abstract parent type for all subword tokenizers.\n\nTokenizers are callable and support: tokenizer(text::AbstractString) -> Vector{String}.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.FilesSpec","page":"API Reference","title":"KeemenaSubwords.FilesSpec","text":"Structured file specification for local tokenizer loading/registration.\n\nUse path for single-file formats and explicit pairs for multi-file formats.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.SubwordVocabulary","page":"API Reference","title":"KeemenaSubwords.SubwordVocabulary","text":"Vocabulary container with forward/reverse lookup and special token IDs.\n\nIDs are 1-based.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.TokenizationResult","page":"API Reference","title":"KeemenaSubwords.TokenizationResult","text":"Structured tokenization output for downstream pipelines.\n\nOffset contract:\n\ncoordinate unit: UTF-8 codeunits.\nindex base: 1.\nspan style: half-open [start, stop).\nvalid bounds for spanful tokens: 1 <= start <= stop <= ncodeunits(text) + 1.\nsentinel for tokens without source-text spans: (0, 0).\ninserted post-processor specials use sentinel offsets.\npresent-in-text special added tokens keep real spans, and may still have special_tokens_mask[i] == 1.\nspecial_tokens_mask marks special-token identity; offsets determine span participation.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.TokenizerMetadata","page":"API Reference","title":"KeemenaSubwords.TokenizerMetadata","text":"Common metadata for tokenizer instances.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.assert_offsets_contract-Tuple{AbstractString, Vector{Tuple{Int64, Int64}}}","page":"API Reference","title":"KeemenaSubwords.assert_offsets_contract","text":"Assert offsets satisfy the package offset contract.\n\nThrows ArgumentError on first contract violation. With require_string_boundaries=true, non-empty spans must start/end on valid Julia string boundaries.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.asset_status-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.asset_status","text":"Return prefetch status for a single model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.available_models-Tuple{}","page":"API Reference","title":"KeemenaSubwords.available_models","text":"List available built-in model names.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"Return beginning-of-sequence token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.cached_tokenizers-Tuple{}","page":"API Reference","title":"KeemenaSubwords.cached_tokenizers","text":"List cache keys for in-session cached tokenizers.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.clear_tokenizer_cache!-Tuple{}","page":"API Reference","title":"KeemenaSubwords.clear_tokenizer_cache!","text":"Clear the in-session tokenizer cache used by one-call convenience APIs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{AbstractString, AbstractVector{<:Integer}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"One-call decode by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{AbstractSubwordTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode token IDs into text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{BPETokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode token IDs to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{ByteBPETokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode byte-level BPE IDs back to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{SentencePieceTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode SentencePiece IDs back to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{Symbol, AbstractVector{<:Integer}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"One-call decode by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{TiktokenTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode tiktoken rank IDs to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{UnigramTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode unigram token IDs back to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{WordPieceTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode WordPiece token IDs back into text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.describe_model-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.describe_model","text":"Describe a built-in model.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.detect_tokenizer_files-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_files","text":"Inspect a tokenizer directory and return detected candidate files.\n\nExample: detect_tokenizer_files(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.detect_tokenizer_format-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_format","text":"Detect tokenizer format from a local file or directory.\n\nReturns one of symbols such as :hf_tokenizer_json, :bpe_gpt2, :bpe_encoder, :sentencepiece_model, :tiktoken, :wordpiece, :bpe, or :unigram.\n\nExamples:\n\ndetect_tokenizer_format(\"/path/to/model_dir\")\ndetect_tokenizer_format(\"/path/to/tokenizer.model\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.download_hf_files-Tuple{AbstractString, AbstractVector{<:AbstractString}}","page":"API Reference","title":"KeemenaSubwords.download_hf_files","text":"Download selected files from a Hugging Face repository revision into cache.\n\nThis helper is opt-in and useful for user-managed / gated tokenizers.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"One-call encode by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text into token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{BPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{ByteBPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to byte-level BPE IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{SentencePieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to SentencePiece IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"One-call encode by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{TiktokenTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text into tiktoken rank IDs (1-based in this package).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{UnigramTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to unigram token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{WordPieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to WordPiece token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode_batch_result-Tuple{AbstractSubwordTokenizer, AbstractVector{<:AbstractString}}","page":"API Reference","title":"KeemenaSubwords.encode_batch_result","text":"Batch variant of encode_result.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode_result","page":"API Reference","title":"KeemenaSubwords.encode_result","text":"Encode text and return a structured TokenizationResult.\n\nKey keyword arguments:\n\nassume_normalized::Bool=false: when true, tokenizer intrinsic normalization is skipped and offsets are computed against the exact provided text.\nreturn_offsets::Bool=false: include token-level offsets when available.\nreturn_masks::Bool=false: include attention/token-type/special-token masks.\n\nOffset note:\n\nOffsets use the package-wide 1-based UTF-8 codeunit half-open convention.\nassume_normalized changes whether intrinsic normalization runs; it does not change the offset coordinate system.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.encode_result-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode_result","text":"One-call structured encode by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode_result-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode_result","text":"One-call structured encode by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"Return end-of-sequence token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.export_tokenizer-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.export_tokenizer","text":"Export tokenizer to external formats.\n\nSupported format values:\n\n:internal\n:bpe / :bpe_gpt2\n:wordpiece_vocab\n:unigram_tsv\n:sentencepiece_model\n:hf_tokenizer_json\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.get_tokenizer_cached-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.get_tokenizer_cached","text":"Return a cached tokenizer for a model key or path, loading and caching on first use.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.has_nonempty_span-Tuple{Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.has_nonempty_span","text":"Return true when an offset carries a non-empty source-text span.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.has_span-Tuple{Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.has_span","text":"Return true when an offset carries a real source-text span.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{AbstractSubwordTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{BPETokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{ByteBPETokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{SentencePieceTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{SubwordVocabulary, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Map ID to token string.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{TiktokenTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{UnigramTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{WordPieceTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.install_llama2_tokenizer!-Tuple{}","page":"API Reference","title":"KeemenaSubwords.install_llama2_tokenizer!","text":"Install the gated LLaMA 2 tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama2_tokenizer; ...).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.install_llama3_8b_tokenizer!-Tuple{}","page":"API Reference","title":"KeemenaSubwords.install_llama3_8b_tokenizer!","text":"Install the gated LLaMA 3 8B tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama3_8b_tokenizer; ...).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.install_model!-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.install_model!","text":"Install an installable-gated tokenizer into the user cache and register it by key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.is_valid_string_boundary-Tuple{AbstractString, Int64}","page":"API Reference","title":"KeemenaSubwords.is_valid_string_boundary","text":"Return whether idx is a valid Julia string boundary for text.\n\nThis includes the exclusive end boundary ncodeunits(text) + 1.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.keemena_callable-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.keemena_callable","text":"Return a function compatible with KeemenaPreprocessing's callable tokenizer contract.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.level_key-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.level_key","text":"Level key used by KeemenaPreprocessing for callable tokenizers.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe","text":"Load a BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe","text":"Load a BPE tokenizer from either a directory (vocab.txt + merges.txt) or a vocab file path.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe_encoder-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe_encoder","text":"Load GPT-2 encoder variant from encoder.json + vocab.bpe.\n\nExample: load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe_gpt2-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe_gpt2","text":"Load GPT-2 / RoBERTa style BPE from vocab.json + merges.txt.\n\nExample: load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bytebpe-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bytebpe","text":"Load a byte-level BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bytebpe-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bytebpe","text":"Load a byte-level BPE tokenizer from a directory (vocab.txt + merges.txt) or vocab path.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_hf_tokenizer_json-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_hf_tokenizer_json","text":"Load a Hugging Face tokenizer.json tokenizer in pure Julia.\n\nExpected files:\n\ntokenizer.json directly, or\na directory containing tokenizer.json.\n\nExamples:\n\nload_hf_tokenizer_json(\"/path/to/tokenizer.json\")\nload_hf_tokenizer_json(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_sentencepiece-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_sentencepiece","text":"Load a SentencePiece .model file.\n\nSupported inputs:\n\nstandard SentencePiece binary protobuf .model/.model.v3 payloads\nKeemena text-exported model files:\nkey/value lines (type=unigram|bpe, whitespace_marker=▁, unk_token=<unk>)\npiece rows: piece<TAB>token<TAB>score[<TAB>special_symbol]\nbpe merge rows (for type=bpe): merge<TAB>left<TAB>right\n\nExamples:\n\nload_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nload_sentencepiece(\"/path/to/tokenizer.model.v3\"; kind=:bpe)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tiktoken-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_tiktoken","text":"Load a tiktoken encoding file (*.tiktoken).\n\nThe expected format is line-based: <base64_token_bytes><space><rank> where ranks are non-negative integers.\n\nExamples:\n\nload_tiktoken(\"/path/to/o200k_base.tiktoken\")\nload_tiktoken(\"/path/to/tokenizer.model\") (when file contains tiktoken text lines)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from file system path.\n\nCommon format contracts:\n\n:hf_tokenizer_json -> tokenizer.json\n:bpe_gpt2 -> vocab.json + merges.txt\n:bpe_encoder -> encoder.json + vocab.bpe\n:wordpiece / :wordpiece_vocab -> vocab.txt\n:sentencepiece_model -> *.model / *.model.v3 / sentencepiece.bpe.model\n:tiktoken -> *.tiktoken or tiktoken-text tokenizer.model\n\nExamples:\n\nload_tokenizer(\"/path/to/model_dir\")\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.json\"; format=:hf_tokenizer_json)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{FilesSpec}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from a FilesSpec.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{NamedTuple}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from a named specification.\n\nExamples:\n\n(format=:wordpiece, path=\"/.../vocab.txt\")\n(format=:hf_tokenizer_json, path=\"/.../tokenizer.json\")\n(format=:unigram, path=\"/.../unigram.tsv\")\n(format=:bpe_gpt2, vocab_json=\"/.../vocab.json\", merges_txt=\"/.../merges.txt\")\n(format=:bpe_encoder, encoder_json=\"/.../encoder.json\", vocab_bpe=\"/.../vocab.bpe\")\n(format=:wordpiece, vocab_txt=\"/.../vocab.txt\") (alias)\n(format=:sentencepiece_model, model_file=\"/.../tokenizer.model\") (alias)\n(format=:tiktoken, encoding_file=\"/.../o200k_base.tiktoken\") (alias)\n(format=:hf_tokenizer_json, tokenizer_json=\"/.../tokenizer.json\") (alias)\n(format=:unigram, unigram_tsv=\"/.../unigram.tsv\") (alias)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer by built-in model name.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{Tuple{AbstractString, AbstractString}}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from explicit (vocab_path, merges_path) tuple.\n\nThis tuple form is for classic BPE/byte-level BPE (vocab.txt + merges.txt) or explicit JSON-pair loaders (vocab.json + merges.txt, encoder.json + vocab.bpe) when accompanied by format.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_unigram-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_unigram","text":"Load a Unigram tokenizer from unigram.tsv (file or directory).\n\nExpected format (tab-separated): token<TAB>score[<TAB>special_symbol]\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_wordpiece-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_wordpiece","text":"Load a WordPiece tokenizer from a vocab file path or a directory containing vocab.txt.\n\nExamples:\n\nload_wordpiece(\"/path/to/vocab.txt\")\nload_wordpiece(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Return model metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_path-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.model_path","text":"Resolve built-in model name to on-disk path.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.normalize-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.normalize","text":"Return tokenizer intrinsic normalization output.\n\nThis does not perform pipeline-level preprocessing. Tokenizers without intrinsic normalization return text unchanged.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.normalize_text-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.normalize_text","text":"Normalize text using an optional user-provided callable.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_are_nonoverlapping-Tuple{Vector{Tuple{Int64, Int64}}}","page":"API Reference","title":"KeemenaSubwords.offsets_are_nonoverlapping","text":"Return whether participating offsets are non-overlapping in sequence order.\n\nParticipating offsets satisfy:\n\nnot sentinel when ignore_sentinel=true\nnot empty when ignore_empty=true\n\nFor participating offsets, this enforces next.start >= prev.stop.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_coordinate_system-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_coordinate_system","text":"Offset coordinate system for TokenizationResult.offsets.\n\nOffsets are UTF-8 codeunit indices with half-open span convention [start, stop).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_index_base-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_index_base","text":"Offset index base for TokenizationResult.offsets.\n\nOffsets are 1-based codeunit indices.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_sentinel-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_sentinel","text":"Sentinel used for tokens without a source-text span.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_span_style-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_span_style","text":"Offset span style.\n\nTokenizationResult.offsets use half-open spans [start, stop).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Return padding-token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.prefetch_models-2","page":"API Reference","title":"KeemenaSubwords.prefetch_models","text":"Ensure artifact-backed built-in models are present on disk.\n\nReturns a dictionary of key => is_available.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.prefetch_models_status","page":"API Reference","title":"KeemenaSubwords.prefetch_models_status","text":"Return detailed prefetch status for built-in model keys.\n\nEach value includes:\n\navailable::Bool\nmethod::Symbol (:artifact, :fallback_download, :already_present, or :failed)\npath::Union{Nothing,String}\nerror::Union{Nothing,String}\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.print_asset_status-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.print_asset_status","text":"Print a compact prefetch status line for one model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.recommended_defaults_for_llms-Tuple{}","page":"API Reference","title":"KeemenaSubwords.recommended_defaults_for_llms","text":"Recommended built-in keys for LLM-oriented default prefetching.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_external_model!-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.register_external_model!","text":"Deprecated alias kept for compatibility. Use register_local_model! instead.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_local_model!-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register a local tokenizer path under a symbolic key and persist it in the cache registry.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_local_model!-Tuple{Symbol, FilesSpec}","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register local model files from a FilesSpec.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_local_model!-Tuple{Symbol, NamedTuple}","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register local model files by explicit specification.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.requires_tokenizer_normalization-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.requires_tokenizer_normalization","text":"Whether this tokenizer defines intrinsic normalization that can change text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.save_tokenizer-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.save_tokenizer","text":"Save tokenizer to a canonical on-disk format.\n\nformat=:internal chooses a tokenizer-family specific default:\n\nWordPieceTokenizer -> vocab.txt\nBPETokenizer / ByteBPETokenizer -> vocab.txt + merges.txt\nUnigramTokenizer -> unigram.tsv\nSentencePieceTokenizer -> spm.model\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.span_codeunits-Tuple{AbstractString, Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.span_codeunits","text":"Return the offset span as UTF-8 codeunits.\n\nSentinel and empty spans return UInt8[]. Invalid or out-of-bounds spans also return UInt8[] to keep this helper non-throwing for downstream inspection.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.span_ncodeunits-Tuple{Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.span_ncodeunits","text":"Return span length measured in UTF-8 codeunits.\n\nSentinel and empty spans return 0.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Return special token IDs keyed by symbol.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{BPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{ByteBPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{SentencePieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{SubwordVocabulary, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Map token string to ID, falling back to :unk.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{TiktokenTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{UnigramTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{WordPieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenization_view-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenization_view","text":"Canonical tokenizer text view used for subword offsets/alignment.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"One-call tokenize by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text into subword pieces.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{BPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize with classic BPE merges.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{ByteBPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text by first mapping bytes to unicode symbols, then applying BPE merges.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{SentencePieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text with SentencePiece wrapper behavior.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"One-call tokenize by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{TiktokenTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text into b64:<...> token pieces.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{UnigramTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text using deterministic Viterbi segmentation.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{WordPieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Greedy longest-match WordPiece tokenization.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.try_span_substring-Tuple{AbstractString, Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.try_span_substring","text":"Attempt to return a substring for a half-open codeunit span [start, stop).\n\nSentinel and empty spans return \"\". If span boundaries are not valid Julia string boundaries, this returns nothing. This helper never throws.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Return unknown-token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.validate_offsets_contract-Tuple{AbstractString, Vector{Tuple{Int64, Int64}}}","page":"API Reference","title":"KeemenaSubwords.validate_offsets_contract","text":"Validate offsets against the package offset contract.\n\nReturns true when all offsets satisfy bounds/sentinel invariants. With require_string_boundaries=true, non-empty spans must also start/end on valid Julia string boundaries.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"loading/#Loading-Tokenizers","page":"Loading","title":"Loading Tokenizers","text":"Use load_tokenizer for key-based and auto-detected loading, and use explicit constructors when you want strict file contracts.","category":"section"},{"location":"loading/#Built-in-keys","page":"Loading","title":"Built-in keys","text":"using KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\nqwen = load_tokenizer(:qwen2_5_bpe)","category":"section"},{"location":"loading/#Auto-detected-local-paths","page":"Loading","title":"Auto-detected local paths","text":"load_tokenizer(\"/path/to/model_dir\")\nload_tokenizer(\"/path/to/tokenizer.model\")\nload_tokenizer(\"/path/to/tokenizer.json\")","category":"section"},{"location":"loading/#Force-format-override","page":"Loading","title":"Force format override","text":"load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)","category":"section"},{"location":"loading/#Explicit-constructors","page":"Loading","title":"Explicit constructors","text":"load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\nload_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\nload_wordpiece(\"/path/to/vocab.txt\")\nload_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nload_tiktoken(\"/path/to/o200k_base.tiktoken\")\nload_hf_tokenizer_json(\"/path/to/tokenizer.json\")","category":"section"},{"location":"loading/#Structured-encode-outputs","page":"Loading","title":"Structured encode outputs","text":"tok = load_tokenizer(:core_wordpiece_en)\ntokenization_text = normalize(tok, \"hello world\")\nresult = encode_result(\n    tok,\n    tokenization_text;\n    assume_normalized=true,\n    add_special_tokens=false,\n    return_offsets=true,\n    return_masks=true,\n)\n\nresult.ids\nresult.tokens\nresult.offsets  # UTF-8 codeunit spans on tokenization_text\n\nFor complete local path recipes, see Loading Tokenizers From Local Paths. For explicit file contracts and named-spec keys, see Tokenizer Formats and Required Files.","category":"section"},{"location":"troubleshooting/#Troubleshooting","page":"Troubleshooting","title":"Troubleshooting","text":"","category":"section"},{"location":"troubleshooting/#Auto-detect-picked-the-wrong-format","page":"Troubleshooting","title":"Auto-detect picked the wrong format","text":"Force the format explicitly:\n\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)\n\nUse detection helpers to inspect first:\n\ndetect_tokenizer_format(\"/path/to/model_dir\")\ndetect_tokenizer_files(\"/path/to/model_dir\")","category":"section"},{"location":"troubleshooting/#Missing-merges.txt","page":"Troubleshooting","title":"Missing merges.txt","text":"For GPT-2 style BPE you need both files:\n\nvocab.json + merges.txt\nor encoder.json + vocab.bpe\n\nUse explicit loaders for clearer errors:\n\nload_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\nload_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")","category":"section"},{"location":"troubleshooting/#tokenizer.model-is-not-SentencePiece","page":"Troubleshooting","title":"tokenizer.model is not SentencePiece","text":"Some models (notably LLaMA3-style releases) provide tiktoken text in a file named tokenizer.model.\n\nKeemenaSubwords sniffs .model files:\n\ntiktoken-like text lines => :tiktoken\nbinary / SentencePiece-like payload => :sentencepiece_model\n\nIf needed, override manually:\n\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"troubleshooting/#Gated-model-key-fails-to-load","page":"Troubleshooting","title":"Gated model key fails to load","text":"If load_tokenizer(:llama3_8b_tokenizer) says not installed, install first:\n\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])\n\nYou must have accepted upstream license terms and have valid access.","category":"section"},{"location":"integration/#Integration-With-KeemenaPreprocessing","page":"Integration","title":"Integration With KeemenaPreprocessing","text":"KeemenaSubwords tokenizers are callable and work with KeemenaPreprocessing's callable tokenizer contract.\n\nusing KeemenaPreprocessing\nusing KeemenaSubwords\n\ntokenizer = load_tokenizer(:core_bpe_en)\n\ncfg = PreprocessConfiguration(tokenizer_name = keemena_callable(tokenizer))\nbundle = preprocess_corpus([\"hello world\", \"hello keemena\"]; config=cfg)\n\n# KeemenaPreprocessing stores callable levels under Symbol(typeof(tokenizer))\nlvl = level_key(tokenizer)\nsubword_corpus = get_corpus(bundle, lvl)\n\nFor the normalization/offsets alignment contract (clean_text -> tokenization_text -> encode_result(...; assume_normalized=true)), see Normalization and Offsets Contract.\n\nAlignment rule: Use tokenization_text = tokenization_view(tokenizer, clean_text), then call encode_result(tokenizer, tokenization_text; assume_normalized=true, ...). Word and subword offsets must both be interpreted in the same tokenization_text coordinate system.","category":"section"},{"location":"formats/#Tokenizer-Formats-and-Required-Files","page":"Formats","title":"Tokenizer Formats and Required Files","text":"detect_tokenizer_format(path) and load_tokenizer(path; format=:auto) use the same detection rules. You can always override detection with format=....\n\nFormat Symbol Accepted Input Required Files Canonical Named Spec Keys Recommended Call\n:hf_tokenizer_json file or directory tokenizer.json path (alias: tokenizer_json) load_hf_tokenizer_json(\"/path/to/tokenizer.json\")\n:bpe_gpt2 file pair or directory vocab.json + merges.txt vocab_json, merges_txt load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n:bpe_encoder file pair or directory encoder.json + vocab.bpe encoder_json, vocab_bpe load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n:bpe directory or vocab.txt file vocab.txt + merges.txt vocab, merges (tuple/spec) load_bpe(\"/path/to/model_dir\")\n:bytebpe directory or file pair vocab.txt + merges.txt vocab, merges (tuple/spec) load_bytebpe(\"/path/to/model_dir\")\n:wordpiece / :wordpiece_vocab file or directory vocab.txt path (alias: vocab_txt) load_wordpiece(\"/path/to/vocab.txt\")\n:sentencepiece_model file or directory Standard SentencePiece binary .model/.model.v3 files, or Keemena text-exported .model files (spm.model, spiece.model, tokenizer.model, tokenizer.model.v3, sentencepiece.bpe.model) path (alias: model_file) load_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\n:tiktoken file or directory *.tiktoken or tiktoken-text tokenizer.model path (alias: encoding_file) load_tiktoken(\"/path/to/o200k_base.tiktoken\")\n:unigram file or directory unigram.tsv path (alias: unigram_tsv) load_unigram(\"/path/to/unigram.tsv\")","category":"section"},{"location":"formats/#Exporting-Hugging-Face-tokenizer.json","page":"Formats","title":"Exporting Hugging Face tokenizer.json","text":"Use the HF export target to write tokenizer.json from any supported KeemenaSubwords tokenizer:\n\nexport_tokenizer(tokenizer, \"out_dir\"; format=:hf_tokenizer_json)\n# equivalent via save_tokenizer\nsave_tokenizer(tokenizer, \"out_dir\"; format=:hf_tokenizer_json)\n\nReload in Julia:\n\nreloaded = load_tokenizer(\"out_dir\"; format=:hf_tokenizer_json)\n\nLoad the same file in Python Fast tokenizers:\n\nfrom transformers import PreTrainedTokenizerFast\ntok = PreTrainedTokenizerFast(tokenizer_file=\"out_dir/tokenizer.json\")\n\nCurrent scope note:\n\ntokenizer_config.json and special_tokens_map.json are not emitted yet.\nTemplateProcessing is emitted in canonical HF JSON shape (single/pair object items and object-map special_tokens) for better external HF compatibility.\nByte-level export writes explicit ByteLevel options (add_prefix_space=false, trim_offsets=false, use_regex=false) for Keemena ByteBPE interoperability, so Python/Rust HF loaders do not silently fall back to different defaults.","category":"section"},{"location":"formats/#BERT-Components-in-tokenizer.json","page":"Formats","title":"BERT Components in tokenizer.json","text":"KeemenaSubwords now supports the common Hugging Face BERT pipeline components:\n\nnormalizer.type = \"BertNormalizer\"\npre_tokenizer.type = \"BertPreTokenizer\"\n\nOffsets for these pipelines follow the same package contract:\n\noffsets are computed against tokenization_view(tokenizer, text) (the tokenizer-normalized text),\ninserted post-processor specials keep sentinel (0,0),\nspanful offsets remain 1-based UTF-8 codeunit half-open spans.\n\nKeemenaPreprocessing integration remains: tokenization_text = tokenization_view(tokenizer, clean_text) then encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true).","category":"section"},{"location":"formats/#Detection-Notes","page":"Formats","title":"Detection Notes","text":"Directory preference order:\ntokenizer.json\nvocab.json + merges.txt\nencoder.json + vocab.bpe\nSentencePiece model filenames\n*.tiktoken\n.model files are sniffed:\ntiktoken-like text (<base64> <int>) => :tiktoken\nbinary SentencePiece protobuf payload or Keemena text SentencePiece payload => :sentencepiece_model\nLLaMA3-style files often use tokenizer.model containing tiktoken text. Use explicit override when needed.\n\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)","category":"section"},{"location":"formats/#Byte-level-behavior","page":"Formats","title":"Byte-level behavior","text":"Byte-level tokenizers:\n:bytebpe\n:bpe_gpt2 / :bpe_encoder\n:tiktoken\nsome :hf_tokenizer_json pipelines when ByteLevel is configured.\nNon-byte-level tokenizers:\n:wordpiece\n:sentencepiece_model\n:unigram.\n\nRound-trip expectations:\n\nByte-level families are generally robust for arbitrary UTF-8 input and usually satisfy stable decode(encode(text)).\nWordPiece/SentencePiece/Unigram operate on learned subword vocabularies; they are deterministic, but unknown-token fallback can reduce exact round-trip fidelity depending on vocab coverage.","category":"section"},{"location":"gated_models/#Installable-Gated-Models","page":"Gated Models","title":"Installable Gated Models","text":"KeemenaSubwords supports gated tokenizers (for example some LLaMA variants) through opt-in installation into your local cache.","category":"section"},{"location":"gated_models/#What-this-does","page":"Gated Models","title":"What this does","text":"install_model!(key; token=...):\n\ndownloads only tokenizer files from upstream with your credentials,\nstores them under your cache (KEEMENA_SUBWORDS_CACHE_DIR override supported),\nregisters them locally so load_tokenizer(:key) works.","category":"section"},{"location":"gated_models/#What-this-does-not-do","page":"Gated Models","title":"What this does not do","text":"No gated tokenizer files are redistributed in this repository.\nNo gated files are published in Artifacts.toml.\nNo silent background downloads happen during load_tokenizer(:key).","category":"section"},{"location":"gated_models/#Install-flow","page":"Gated Models","title":"Install flow","text":"# LLaMA 2\ninstall_model!(:llama2_tokenizer; token=ENV[\"HF_TOKEN\"])\n\n# LLaMA 3 8B\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])\n\n# then load by key\nllama3 = load_tokenizer(:llama3_8b_tokenizer)\n\nIf you already downloaded tokenizer files elsewhere, you can skip install_model! and load/register directly:\n\nllama2 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)\nllama3 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\n\nregister_local_model!(:llama3_local, \"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"gated_models/#Discover-gated-keys","page":"Gated Models","title":"Discover gated keys","text":"available_models(distribution=:installable_gated)\ndescribe_model(:llama3_8b_tokenizer)","category":"section"},{"location":"#KeemenaSubwords.jl","page":"Home","title":"KeemenaSubwords.jl","text":"Downstream of KeemenaPreprocessing.jl.\n\nKeemenaSubwords provides Julia-native loaders and tokenization primitives for:\n\nclassic BPE,\nbyte-level BPE,\nWordPiece,\nSentencePiece,\ntiktoken,\nHugging Face tokenizer.json.","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\npieces = tokenize(tok, \"hello world\")\nids = encode(tok, \"hello world\"; add_special_tokens=true)\ntext = decode(tok, ids)","category":"section"},{"location":"#Model-Discovery","page":"Home","title":"Model Discovery","text":"available_models()\navailable_models(distribution=:artifact_public)\navailable_models(distribution=:installable_gated)\ndescribe_model(:qwen2_5_bpe)\nrecommended_defaults_for_llms()","category":"section"},{"location":"#Key-Workflows","page":"Home","title":"Key Workflows","text":"# local path auto-detection\nload_tokenizer(\"/path/to/model_dir\")\n\n# explicit loaders\nload_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\nload_sentencepiece(\"/path/to/tokenizer.model\")\nload_tiktoken(\"/path/to/tokenizer.model\")\n\n# gated install flow\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])","category":"section"},{"location":"#Documentation-Map","page":"Home","title":"Documentation Map","text":"Built-in model inventory\nNormalization and offsets contract\nTraining (experimental)\nFormat contracts\nLocal path recipes\nLLM cookbook and tokenizer.json roadmap\nInstallable gated models\nTroubleshooting\nAPI reference","category":"section"},{"location":"#KeemenaPreprocessing-Integration","page":"Home","title":"KeemenaPreprocessing Integration","text":"using KeemenaPreprocessing\nusing KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\ncfg = PreprocessConfiguration(tokenizer_name = keemena_callable(tok))\nbundle = preprocess_corpus([\"hello world\"]; config=cfg)\n\nSee API reference for explicit loader APIs and the full exported reference.","category":"section"},{"location":"llm_cookbook/#LLM-Cookbook","page":"LLM Cookbook","title":"LLM Cookbook","text":"","category":"section"},{"location":"llm_cookbook/#OpenAI-tiktoken-encodings","page":"LLM Cookbook","title":"OpenAI tiktoken encodings","text":"prefetch_models([:tiktoken_cl100k_base, :tiktoken_o200k_base])\ntt = load_tokenizer(:tiktoken_cl100k_base)\nencode(tt, \"hello world\")","category":"section"},{"location":"llm_cookbook/#Mistral-SentencePiece","page":"LLM Cookbook","title":"Mistral SentencePiece","text":"prefetch_models([:mistral_v3_sentencepiece])\nmistral = load_tokenizer(:mistral_v3_sentencepiece)","category":"section"},{"location":"llm_cookbook/#Qwen-tokenizer.json-first-loading","page":"LLM Cookbook","title":"Qwen tokenizer.json-first loading","text":"prefetch_models([:qwen2_5_bpe])\nqwen = load_tokenizer(:qwen2_5_bpe)","category":"section"},{"location":"llm_cookbook/#LLaMA-workflow-A:-install-(gated)","page":"LLM Cookbook","title":"LLaMA workflow A: install (gated)","text":"install_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])\nllama = load_tokenizer(:llama3_8b_tokenizer)","category":"section"},{"location":"llm_cookbook/#LLaMA-workflow-B:-manual-local-path","page":"LLM Cookbook","title":"LLaMA workflow B: manual local path","text":"# SentencePiece-style\nllama2 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)\n\n# LLaMA3-style tokenizer.model with tiktoken text\nllama3 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"llm_cookbook/#Pick-practical-defaults","page":"LLM Cookbook","title":"Pick practical defaults","text":"for key in recommended_defaults_for_llms()\n    println(key)\nend","category":"section"},{"location":"llm_cookbook/#Tokenizer.json-roadmap-(no-Python-needed)","page":"LLM Cookbook","title":"Tokenizer.json roadmap (no Python needed)","text":"Near-term roadmap items (in scope for this package):\n\nexpand Hugging Face component coverage incrementally (normalizers, pre-tokenizers, post-processors, decoders),\nadd optional richer encode outputs (offsets, attention_mask, token_type_ids) in a structured return type,\nadd a small number of additional curated flagship tokenizers where redistribution/license terms are clear,\ncontinue performance hardening for BPE merge caching, WordPiece trie lookup, Unigram DP, and added-token matching.","category":"section"}]
}
