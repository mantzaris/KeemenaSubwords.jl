var documenterSearchIndex = {"docs":
[{"location":"structured_outputs_and_batching/#Structured-Outputs-and-Batching","page":"Structured Outputs and Batching","title":"Structured Outputs and Batching","text":"This page shows practical recipes for turning tokenizer outputs into training-ready batch tensors and label alignments.","category":"section"},{"location":"structured_outputs_and_batching/#TokenizationResult-Fields-And-Invariants","page":"Structured Outputs and Batching","title":"TokenizationResult Fields And Invariants","text":"encode_result and encode_batch_result return TokenizationResult values with:\n\nids: token ids (Vector{Int}), using KeemenaSubwords 1-based id space.\ntokens: token strings (Vector{String}) aligned position-by-position with ids.\noffsets: Union{Nothing, Vector{Tuple{Int,Int}}} when return_offsets=true. Offsets are 1-based UTF-8 codeunit half-open spans [start, stop). Sentinel (0, 0) means no source-text span.\nattention_mask: Union{Nothing, Vector{Int}} when return_masks=true. For a single unpadded encode_result call this is all ones.\ntoken_type_ids: Union{Nothing, Vector{Int}} when return_masks=true. Current default is zeros for single-sequence examples.\nspecial_tokens_mask: Union{Nothing, Vector{Int}} when return_masks=true. Marks special-token identity. A token can be special and still have a real span (for example [UNK] in WordPiece).\nmetadata: named tuple with model and offsets metadata. metadata.offsets_reference is:\n:input_text when assume_normalized=true\n:tokenizer_normalized_text when assume_normalized=false","category":"section"},{"location":"structured_outputs_and_batching/#Example-1:-Inspect-encode_result-Output","page":"Structured Outputs and Batching","title":"Example 1: Inspect encode_result Output","text":"using KeemenaSubwords\n\ntokenizer = load_tokenizer(:core_wordpiece_en)\nclean_text = \"hello world\"\ntokenization_text = tokenization_view(tokenizer, clean_text)\n\nresult = encode_result(\n    tokenizer,\n    tokenization_text;\n    assume_normalized = true,\n    add_special_tokens = true,\n    return_offsets = true,\n    return_masks = true,\n)\n\nresult_default_reference = encode_result(\n    tokenizer,\n    clean_text;\n    assume_normalized = false,\n    add_special_tokens = true,\n    return_offsets = true,\n    return_masks = true,\n)\n\n@assert result.offsets !== nothing\n@assert result.attention_mask !== nothing\n@assert result.special_tokens_mask !== nothing\n\nrows = [\n    (\n        token_index = i,\n        token_id = result.ids[i],\n        token = result.tokens[i],\n        offset = result.offsets[i],\n        special = result.special_tokens_mask[i],\n        span_text = try_span_substring(tokenization_text, result.offsets[i]),\n    )\n    for i in eachindex(result.ids)\n]\n\n(\n    ids = result.ids,\n    tokens = result.tokens,\n    attention_mask_unique_values = unique(result.attention_mask),\n    offsets_reference = (\n        assume_normalized_true = result.metadata.offsets_reference,\n        assume_normalized_false = result_default_reference.metadata.offsets_reference,\n    ),\n    token_rows = rows,\n)\n\nWhat this shows:\n\nInserted specials are typically sentinel spans (0, 0) with special == 1.\nspecial_tokens_mask marks token identity, not span participation.\nIn this example, [UNK] is special but still has a spanful offset from source text.","category":"section"},{"location":"structured_outputs_and_batching/#Example-2:-encode_batch_result-Returns-Per-Sequence-Outputs","page":"Structured Outputs and Batching","title":"Example 2: encode_batch_result Returns Per-Sequence Outputs","text":"clean_texts = [\n    \"hello world\",\n    \"world hello world\",\n    \"offset demo\",\n]\ntokenization_texts = [tokenization_view(tokenizer, text) for text in clean_texts]\n\nbatch_results = encode_batch_result(\n    tokenizer,\n    tokenization_texts;\n    assume_normalized = true,\n    add_special_tokens = true,\n    return_offsets = true,\n    return_masks = true,\n)\n\nbatch_summary = [\n    (\n        sequence_index = i,\n        n_ids = length(batch_results[i].ids),\n        n_offsets = batch_results[i].offsets === nothing ? 0 : length(batch_results[i].offsets),\n        attention_mask = batch_results[i].attention_mask,\n    )\n    for i in eachindex(batch_results)\n]\n\nbatch_summary\n\nencode_batch_result returns Vector{TokenizationResult}. Padding is not applied automatically. Each sequence keeps its own length.","category":"section"},{"location":"structured_outputs_and_batching/#Example-3:-Padding-Collator-Recipe","page":"Structured Outputs and Batching","title":"Example 3: Padding Collator Recipe","text":"Policy in this example:\n\noutput layout is (seq_len, batch) (column-major friendly in Julia),\nids pad with pad_id,\npadded attention_mask positions are 0,\npadded special_tokens_mask positions are marked as 1 (mark_pad_as_special=true).\n\nfunction pad_batch(\n    results::Vector{TokenizationResult};\n    pad_id::Int,\n    mark_pad_as_special::Bool = true,\n)\n    batch_size = length(results)\n    max_len = maximum(length(r.ids) for r in results)\n\n    ids = fill(pad_id, max_len, batch_size)\n    attention_mask = fill(0, max_len, batch_size)\n    special_tokens_mask = fill(mark_pad_as_special ? 1 : 0, max_len, batch_size)\n\n    for (col, r) in pairs(results)\n        seq_len = length(r.ids)\n        ids[1:seq_len, col] = r.ids\n        attention_mask[1:seq_len, col] .= 1\n        if r.special_tokens_mask === nothing\n            special_tokens_mask[1:seq_len, col] .= 0\n        else\n            special_tokens_mask[1:seq_len, col] = r.special_tokens_mask\n        end\n    end\n\n    return (\n        ids = ids,\n        attention_mask = attention_mask,\n        special_tokens_mask = special_tokens_mask,\n    )\nend\n\ncollated = pad_batch(batch_results; pad_id = pad_id(tokenizer))\n\n(\n    ids_size = size(collated.ids),\n    attention_mask_size = size(collated.attention_mask),\n    special_tokens_mask_size = size(collated.special_tokens_mask),\n    ids = collated.ids,\n    attention_mask = collated.attention_mask,\n    special_tokens_mask = collated.special_tokens_mask,\n)","category":"section"},{"location":"normalization_offsets_contract/","page":"Normalization & Offsets","title":"Normalization & Offsets","text":"<!‚Äì Source of truth: notes/OffsetContract.md. Synced to docs/src/normalizationoffsetscontract.md by tools/syncoffsetcontract.jl. Edit source then run the sync tool. ‚Äì>","category":"section"},{"location":"normalization_offsets_contract/#Normalization-and-Offsets-Contract","page":"Normalization & Offsets","title":"Normalization and Offsets Contract","text":"This document is the canonical contract for normalization and offsets behavior in KeemenaSubwords when integrated with KeemenaPreprocessing.\n\nFor step-by-step usage patterns, see Offsets Alignment Examples.","category":"section"},{"location":"normalization_offsets_contract/#Normalization-Ownership","page":"Normalization & Offsets","title":"Normalization Ownership","text":"Pipeline normalization (KeemenaPreprocessing): produces clean_text.\nTokenizer intrinsic normalization (KeemenaSubwords): produces tokenization_text.\n\nUse:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\n\nThen call:\n\nresult = encode_result(\n    tokenizer,\n    tokenization_text;\n    assume_normalized=true,\n    return_offsets=true,\n    return_masks=true,\n    add_special_tokens=true,\n)\n\nWhen assume_normalized=true, KeemenaSubwords must not re-run tokenizer intrinsic normalization.","category":"section"},{"location":"normalization_offsets_contract/#Offset-Convention","page":"Normalization & Offsets","title":"Offset Convention","text":"coordinate unit: UTF-8 codeunits\nindex base: 1-based\nspan style: half-open [start, stop)\nvalid bounds for spanful offsets:\n1 <= start <= stop <= ncodeunits(text) + 1\n\nProgrammatic helpers:\n\noffsets_coordinate_system() == :utf8_codeunits\noffsets_index_base() == 1\noffsets_span_style() == :half_open\noffsets_sentinel() == (0, 0)\nhas_span(offset) is true iff offset != (0, 0)\nhas_nonempty_span(offset) is true iff the offset is spanful and stop > start\nspan_ncodeunits(offset) returns span length in codeunits (0 for sentinel/empty)","category":"section"},{"location":"normalization_offsets_contract/#Sentinel-and-Special-Tokens","page":"Normalization & Offsets","title":"Sentinel and Special Tokens","text":"Sentinel:\n\n(0, 0) means \"no source-text span\".\nIn a 1-based scheme, (0, 0) is out-of-range and unambiguous.\n\nSpecial token semantics:\n\nInserted special tokens (TemplateProcessing/post-processor inserted):\nspecial_tokens_mask[i] == 1\noffsets[i] == (0, 0)\nSpecial tokens matched from user text as added tokens:\nspecial_tokens_mask[i] == 1\noffsets[i] is a real span into the input text (offsets[i] != (0, 0))\n\nImportant:\n\nspecial_tokens_mask marks special-token identity.\nSpan participation is determined by offsets/sentinel, not by mask alone.","category":"section"},{"location":"normalization_offsets_contract/#Alignment-Rule-for-KeemenaPreprocessing","page":"Normalization & Offsets","title":"Alignment Rule for KeemenaPreprocessing","text":"Canonical coordinate system is tokenization_text.\n\nKeemenaPreprocessing should:\n\nProduce clean_text via pipeline normalization.\nProduce tokenization_text = tokenization_view(tokenizer, clean_text).\nCall encode_result(tokenizer, tokenization_text; assume_normalized=true, ...).\nCompute both word offsets and subword offsets on tokenization_text.\nIgnore sentinel offsets (0, 0) during span alignment.\nDo not drop all mask==1 tokens blindly; present-in-text special tokens may have real spans.\n\nRecommended span participation policy:\n\nParticipate in span alignment iff has_nonempty_span(offset).","category":"section"},{"location":"normalization_offsets_contract/#Downstream-Safe-Span-Inspection","page":"Normalization & Offsets","title":"Downstream-Safe Span Inspection","text":"Offsets are codeunit spans. Do not assume they are always valid Julia string slicing boundaries, especially for byte-level tokenizers on multibyte text.\n\nUse these helpers for robust downstream handling:\n\nspan_codeunits(text, offset):\nreturns UInt8[] for sentinel/empty spans,\nreturns the exact byte slice for non-empty spans.\ntry_span_substring(text, offset):\nreturns \"\" for sentinel/empty spans,\nreturns String only when both boundaries are valid Julia string boundaries,\nreturns nothing otherwise.\nis_valid_string_boundary(text, idx) can be used to inspect boundary validity.\noffsets_are_nonoverlapping(offsets; ignore_sentinel=true, ignore_empty=true) validates a downstream non-overlap invariant.","category":"section"},{"location":"normalization_offsets_contract/#Boundary-Validity-Expectations-By-Tokenizer-Family","page":"Normalization & Offsets","title":"Boundary Validity Expectations By Tokenizer Family","text":"Non-byte-level tokenizers (for example WordPiece, SentencePiece, Unigram TSV, and non-byte-level HF tokenizer.json pipelines) are expected to produce spanful offsets that land on valid Julia string boundaries.\nByte-level tokenizers (for example ByteBPE and HF ByteLevel pretokenizers) may produce non-boundary offsets on multibyte Unicode inputs.\n\nDownstream interpretation:\n\nWhen try_span_substring(text, offset) returns nothing for a byte-level multibyte case, treat this as expected \"unsafe to slice\" behavior.\nUse span_codeunits(text, offset) for byte-accurate span inspection regardless of string-boundary validity.","category":"section"},{"location":"normalization_offsets_contract/#Strict-Validator-Helpers-(Maintainer-Debugging)","page":"Normalization & Offsets","title":"Strict Validator Helpers (Maintainer Debugging)","text":"For tokenizer development and regression debugging, KeemenaSubwords also exposes strict contract validators:\n\nvalidate_offsets_contract(text, offsets; require_string_boundaries=false): returns Bool without throwing.\nassert_offsets_contract(text, offsets; require_string_boundaries=false): throws ArgumentError on first violation with a targeted message.\n\nUse require_string_boundaries=true when validating string-level tokenizers where spanful offsets are expected to land on valid Julia string boundaries.","category":"section"},{"location":"concepts/#Concepts","page":"Concepts","title":"Concepts","text":"This page is a first-hour guide to the concepts you need for reliable tokenization and alignment workflows in KeemenaSubwords.","category":"section"},{"location":"concepts/#Where-This-Fits","page":"Concepts","title":"Where This Fits","text":"Typical Julia LLM preprocessing split:\n\nKeemenaPreprocessing: produces normalized clean_text.\nKeemenaSubwords: turns text into token pieces and 1-based token ids.\n\nRecommended integration flow:\n\nclean_text = ... from your preprocessing pipeline.\ntokenization_text = tokenization_view(tokenizer, clean_text).\nencode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true, ...).","category":"section"},{"location":"concepts/#Token-Pieces-Vs-Token-Ids","page":"Concepts","title":"Token Pieces Vs Token Ids","text":"tokenize(tok, text) returns token pieces (Vector{String}).\nencode(tok, text; ...) returns token ids (Vector{Int}).\ndecode(tok, ids) maps ids back to text.\n\nusing KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\ntext = \"Hello world\"\n\npieces = tokenize(tok, text)\nids = encode(tok, text; add_special_tokens=true)\ndecoded = decode(tok, ids)\n\n(; pieces, ids, decoded)\n\nKeemenaSubwords uses 1-based token ids.\n\nConvert to 0-based ids only when you need parity with external tooling:\n\nids_zero_based = ids .- 1\nids_julia = ids_zero_based .+ 1","category":"section"},{"location":"concepts/#Tokenizer-Families-Supported","page":"Concepts","title":"Tokenizer Families Supported","text":"Family Typical format symbols Typically byte-level Offset implication\nBPE (classic) :bpe No Spanful offsets are expected to be string-safe in normal usage.\nByteBPE :bytebpe Yes Offsets are valid codeunit spans, but may not always be safe string slice boundaries on multibyte text.\nWordPiece :wordpiece, :wordpiece_vocab No Spanful offsets are expected to be string-safe in normal usage.\nUnigram TSV :unigram, :unigram_tsv No Spanful offsets are expected to be string-safe in normal usage.\nSentencePiece model :sentencepiece_model Usually no Spanful offsets are expected to be string-safe for standard SentencePiece pipelines.\ntiktoken :tiktoken Yes Same byte-level caveat as ByteBPE.\nHF tokenizer.json :hf_tokenizer_json Depends on pipeline If ByteLevel components are present, use byte-level caveats for offsets.","category":"section"},{"location":"concepts/#Special-Tokens-And-add_special_tokens","page":"Concepts","title":"Special Tokens And add_special_tokens","text":"Inspect special token mappings and common ids:\n\nusing KeemenaSubwords\n\ntok = load_tokenizer(:core_wordpiece_en)\n\nspecials = special_tokens(tok)\nids = (\n    bos = try bos_id(tok) catch; nothing end,\n    eos = try eos_id(tok) catch; nothing end,\n    pad = try pad_id(tok) catch; nothing end,\n    unk = try unk_id(tok) catch; nothing end,\n)\n\n(; specials, ids)\n\nadd_special_tokens=true asks the tokenizer/post-processor to insert framework specials (for example BOS/EOS or CLS/SEP).\n\nOffset behavior:\n\nInserted specials: special_tokens_mask[i] == 1 and offsets[i] == (0, 0).\nSpecials that appear in the input text as matched added tokens: special_tokens_mask[i] == 1, but offsets can still be real spans into the input text.","category":"section"},{"location":"concepts/#Structured-Encoding-And-Offsets","page":"Concepts","title":"Structured Encoding And Offsets","text":"Use encode_result when you need ids plus offsets and masks in one object (TokenizationResult).\n\nusing KeemenaSubwords\n\ntok = load_tokenizer(:core_sentencepiece_unigram_en)\nclean_text = \"Hello world\"\ntokenization_text = tokenization_view(tok, clean_text)\n\nresult = encode_result(\n    tok,\n    tokenization_text;\n    assume_normalized=true,\n    add_special_tokens=true,\n    return_offsets=true,\n    return_masks=true,\n)\n\n(\n    ids = result.ids,\n    tokens = result.tokens,\n    offsets = result.offsets,\n    attention_mask = result.attention_mask,\n    special_tokens_mask = result.special_tokens_mask,\n    metadata = result.metadata,\n)\n\nHigh-level offset contract:\n\nCoordinate system: UTF-8 codeunits.\nIndex base: 1.\nSpan style: half-open [start, stop).\nSentinel for no-span tokens: (0, 0).\n\nFor the full contract and helper APIs, see Normalization and Offsets Contract. For worked alignment walkthroughs, see Offsets Alignment Examples. For batching and padding recipes, see Structured Outputs and Batching.\n\nRecommended KeemenaPreprocessing alignment pattern:\n\ntokenization_text = tokenization_view(tok, clean_text)\nresult = encode_result(\n    tok,\n    tokenization_text;\n    assume_normalized=true,\n    return_offsets=true,\n    return_masks=true,\n    add_special_tokens=true,\n)","category":"section"},{"location":"concepts/#Model-Registry-And-Caching","page":"Concepts","title":"Model Registry And Caching","text":"Use the registry APIs to discover models and the cache APIs to avoid reloading tokenizers repeatedly:\n\navailable_models(shipped=true)\ndescribe_model(:core_bpe_en)\nprefetch_models([:core_bpe_en, :core_wordpiece_en, :core_sentencepiece_unigram_en])\n\ntok = get_tokenizer_cached(:core_bpe_en)\n\n# Clear long-lived cached tokenizer instances when needed\n# (for example to release memory or force a fresh reload).\nclear_tokenizer_cache!()","category":"section"},{"location":"concepts/#Loading-And-Exporting","page":"Concepts","title":"Loading And Exporting","text":"Pointers:\n\nLoading Tokenizers\nLoading Tokenizers From Local Paths\nTokenizer Formats and Required Files\n\nExport APIs:\n\nexport_tokenizer(tokenizer, out_dir; format=...)\nsave_tokenizer(tokenizer, out_dir; format=...)\n\nIf you export with format=:hf_tokenizer_json, KeemenaSubwords writes tokenizer.json for HF-compatible fast tokenizer loading. Current scope details (for example companion config files) are documented in Tokenizer Formats and Required Files.\n\nPlaceholder examples below require local paths or gated access and are intentionally non-executable in docs:\n\n# local path placeholder (non-executable)\ntok = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\n\n# gated install placeholder (non-executable)\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])","category":"section"},{"location":"models/#Built-In-Models","page":"Built-In Models","title":"Built-In Models","text":"using KeemenaSubwords\n\navailable_models()\navailable_models(format=:tiktoken)\navailable_models(format=:bpe_gpt2)\navailable_models(format=:hf_tokenizer_json)\navailable_models(family=:qwen)\navailable_models(family=:mistral)\navailable_models(distribution=:artifact_public)\navailable_models(distribution=:installable_gated)\navailable_models(shipped=true)\n\ndescribe_model(:core_bpe_en)\ndescribe_model(:core_wordpiece_en)\ndescribe_model(:core_sentencepiece_unigram_en)\ndescribe_model(:tiktoken_o200k_base)\ndescribe_model(:openai_gpt2_bpe)\ndescribe_model(:bert_base_uncased_wordpiece)\ndescribe_model(:bert_base_multilingual_cased_wordpiece)\ndescribe_model(:t5_small_sentencepiece_unigram)\ndescribe_model(:mistral_v1_sentencepiece)\ndescribe_model(:mistral_v3_sentencepiece)\ndescribe_model(:phi2_bpe)\ndescribe_model(:qwen2_5_bpe)\ndescribe_model(:roberta_base_bpe)\ndescribe_model(:xlm_roberta_base_sentencepiece_bpe)\ndescribe_model(:llama3_8b_tokenizer)\nrecommended_defaults_for_llms()\n\nmodel_path(:core_bpe_en)\n\nThe table below is generated from the same registry used by available_models() and describe_model(...).\n\nGenerated from the registry by `tools/syncreadmemodels.jl(excluding:userlocal` entries)._","category":"section"},{"location":"models/#bpe-/-core","page":"Built-In Models","title":"bpe / core","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:core_bpe_en shipped MIT in-repo/core in-repo:core vocab.txt, merges.txt Tiny built-in English classic BPE model (vocab.txt + merges.txt).","category":"section"},{"location":"models/#bpe_gpt2-/-openai","page":"Built-In Models","title":"bpe_gpt2 / openai","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:openai_gpt2_bpe artifact_public MIT openaipublic/gpt-2 openaipublic:gpt-2/encodings/main vocab.json + merges.txt, encoder.json + vocab.bpe OpenAI GPT-2 byte-level BPE assets (encoder.json + vocab.bpe).","category":"section"},{"location":"models/#bpe_gpt2-/-phi","page":"Built-In Models","title":"bpe_gpt2 / phi","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:phi2_bpe artifact_public MIT microsoft/phi-2 huggingface:microsoft/phi-2@810d367871c1d460086d9f82db8696f2e0a0fcd0 vocab.json + merges.txt, encoder.json + vocab.bpe Microsoft Phi-2 GPT2-style tokenizer files (vocab.json + merges.txt).","category":"section"},{"location":"models/#bpe_gpt2-/-roberta","page":"Built-In Models","title":"bpe_gpt2 / roberta","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:roberta_base_bpe artifact_public MIT FacebookAI/roberta-base huggingface:FacebookAI/roberta-base@e2da8e2f811d1448a5b465c236feacd80ffbac7b vocab.json + merges.txt, encoder.json + vocab.bpe RoBERTa-base byte-level BPE tokenizer files (vocab.json + merges.txt).","category":"section"},{"location":"models/#hf_tokenizer_json-/-llama","page":"Built-In Models","title":"hf_tokenizer_json / llama","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:llama3_8b_tokenizer installable_gated Llama-3.1-Community-License meta-llama/Meta-Llama-3-8B-Instruct huggingface:meta-llama/Meta-Llama-3-8B-Instruct@main tokenizer.json (preferred), vocab.json + merges.txt (fallback) Meta Llama 3 8B tokenizer (gated; install with install_model!).","category":"section"},{"location":"models/#hf_tokenizer_json-/-qwen","page":"Built-In Models","title":"hf_tokenizer_json / qwen","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:qwen2_5_bpe artifact_public Apache-2.0 Qwen/Qwen2.5-7B huggingface:Qwen/Qwen2.5-7B@d149729398750b98c0af14eb82c78cfe92750796 tokenizer.json (preferred), vocab.json + merges.txt (fallback) Qwen2.5 BPE tokenizer assets (tokenizer.json with vocab/merges fallback).","category":"section"},{"location":"models/#sentencepiece_model-/-core","page":"Built-In Models","title":"sentencepiece_model / core","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:core_sentencepiece_unigram_en shipped MIT in-repo/core in-repo:core spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Tiny built-in SentencePiece Unigram model (.model).","category":"section"},{"location":"models/#sentencepiece_model-/-llama","page":"Built-In Models","title":"sentencepiece_model / llama","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:llama2_tokenizer installable_gated Llama-2-Community-License meta-llama/Llama-2-7b-hf huggingface:meta-llama/Llama-2-7b-hf@main spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Meta Llama 2 tokenizer (gated; install with install_model!).","category":"section"},{"location":"models/#sentencepiece_model-/-mistral","page":"Built-In Models","title":"sentencepiece_model / mistral","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:mistral_v1_sentencepiece artifact_public Apache-2.0 mistralai/Mixtral-8x7B-Instruct-v0.1 huggingface:mistralai/Mixtral-8x7B-Instruct-v0.1@eba92302a2861cdc0098cc54bc9f17cb2c47eb61 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Mistral/Mixtral tokenizer.model SentencePiece model.\n:mistral_v3_sentencepiece artifact_public Apache-2.0 mistralai/Mistral-7B-Instruct-v0.3 huggingface:mistralai/Mistral-7B-Instruct-v0.3@c170c708c41dac9275d15a8fff4eca08d52bab71 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Mistral-7B-Instruct-v0.3 tokenizer.model.v3 SentencePiece model.","category":"section"},{"location":"models/#sentencepiece_model-/-t5","page":"Built-In Models","title":"sentencepiece_model / t5","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:t5_small_sentencepiece_unigram artifact_public Apache-2.0 google-t5/t5-small huggingface:google-t5/t5-small@df1b051c49625cf57a3d0d8d3863ed4d13564fe4 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model Hugging Face google-t5/t5-small SentencePiece model (Unigram).","category":"section"},{"location":"models/#sentencepiece_model-/-xlm_roberta","page":"Built-In Models","title":"sentencepiece_model / xlm_roberta","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:xlm_roberta_base_sentencepiece_bpe artifact_public MIT FacebookAI/xlm-roberta-base huggingface:FacebookAI/xlm-roberta-base@e73636d4f797dec63c3081bb6ed5c7b0bb3f2089 spm.model / tokenizer.model / tokenizer.model.v3 / sentencepiece.bpe.model XLM-RoBERTa-base sentencepiece.bpe.model file.","category":"section"},{"location":"models/#tiktoken-/-openai","page":"Built-In Models","title":"tiktoken / openai","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:tiktoken_cl100k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/cl100k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken cl100k_base encoding.\n:tiktoken_o200k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/o200k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken o200k_base encoding.\n:tiktoken_p50k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/p50k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken p50k_base encoding.\n:tiktoken_r50k_base artifact_public MIT openaipublic/encodings openaipublic:encodings/r50k_base.tiktoken *.tiktoken or tokenizer.model (tiktoken text) OpenAI tiktoken r50k_base encoding.","category":"section"},{"location":"models/#wordpiece_vocab-/-bert","page":"Built-In Models","title":"wordpiece_vocab / bert","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:bert_base_multilingual_cased_wordpiece artifact_public Apache-2.0 google-bert/bert-base-multilingual-cased huggingface:google-bert/bert-base-multilingual-cased@3f076fdb1ab68d5b2880cb87a0886f315b8146f8 vocab.txt Hugging Face bert-base-multilingual-cased WordPiece vocabulary.\n:bert_base_uncased_wordpiece artifact_public Apache-2.0 bert-base-uncased huggingface:bert-base-uncased@86b5e0934494bd15c9632b12f734a8a67f723594 vocab.txt Hugging Face bert-base-uncased WordPiece vocabulary.","category":"section"},{"location":"models/#wordpiece_vocab-/-core","page":"Built-In Models","title":"wordpiece_vocab / core","text":"Key Distribution License Upstream Repo Upstream Ref Expected Files Description\n:core_wordpiece_en shipped MIT in-repo/core in-repo:core vocab.txt Tiny built-in English WordPiece model.\n\ndescribe_model(key) includes provenance metadata such as license, family, distribution, upstream_repo, upstream_ref, and upstream_files.\n\nBuilt-ins resolve from artifact paths when present, with in-repo fallback model files only for tiny :core_* assets.\n\nprefetch_models(recommended_defaults_for_llms())","category":"section"},{"location":"training/#Training-(Experimental)","page":"Training","title":"Training (Experimental)","text":"Training support is currently experimental and intentionally separated from the pretrained tokenizer loading/encoding workflows.\n\nAvailable now:\n\ntrain_bpe(...)\ntrain_bytebpe(...)\ntrain_unigram(...)\ntrain_wordpiece(...)\ntrain_wordpiece_result(...)\ntrain_sentencepiece(...)\ntrain_sentencepiece_result(...)\ntrain_hf_bert_wordpiece(...)\ntrain_hf_bert_wordpiece_result(...)\ntrain_hf_roberta_bytebpe(...)\ntrain_hf_roberta_bytebpe_result(...)\ntrain_hf_gpt2_bytebpe(...)\ntrain_hf_gpt2_bytebpe_result(...)\nsave_training_bundle(result, out_dir; ...)\nload_training_bundle(out_dir)","category":"section"},{"location":"training/#Training-API","page":"Training","title":"Training API","text":"","category":"section"},{"location":"training/#HF-BERT-WordPiece-Preset","page":"Training","title":"HF BERT WordPiece Preset","text":"using KeemenaSubwords\n\ncorpus = [\n    \"Hello, world!\",\n    \"Caf√© na√Øve fa√ßade\",\n    \"‰Ω†Â•Ω ‰∏ñÁïå\",\n]\n\ntok = train_hf_bert_wordpiece(\n    corpus;\n    vocab_size=128,\n    min_frequency=1,\n    lowercase=true,\n    strip_accents=nothing,\n    handle_chinese_chars=true,\n    clean_text=true,\n)\n\nexport_tokenizer(tok, \"out_hf_bert\"; format=:hf_tokenizer_json)\nreloaded = load_hf_tokenizer_json(\"out_hf_bert/tokenizer.json\")","category":"section"},{"location":"training/#HF-RoBERTa-ByteBPE-Preset","page":"Training","title":"HF RoBERTa ByteBPE Preset","text":"using KeemenaSubwords\n\ncorpus = [\n    \"hello world\",\n    \"hello, world!\",\n    \"caf√© costs 5 euros\",\n]\n\ntok = train_hf_roberta_bytebpe(\n    corpus;\n    vocab_size=384,\n    min_frequency=1,\n)\n\nexport_tokenizer(tok, \"out_hf_roberta\"; format=:hf_tokenizer_json)\nreloaded = load_hf_tokenizer_json(\"out_hf_roberta/tokenizer.json\")\n\nRoBERTa preset defaults are chosen for HF-style ByteLevel behavior:\n\nuse_regex=true applies GPT-2 ByteLevel regex splitting.\nadd_prefix_space=true matches RoBERTa-style leading-space handling.\ntrim_offsets=true trims span edges for whitespace while preserving the offsets contract: non-span specials use sentinel (0,0), while trimmed real tokens may become empty but remain in-bounds spans like (k,k) (never sentinel).","category":"section"},{"location":"training/#HF-GPT-2-ByteBPE-Preset","page":"Training","title":"HF GPT-2 ByteBPE Preset","text":"using KeemenaSubwords\n\ncorpus = [\n    \"Hello my friend, how is your day going?\",\n    \"caf√© üôÇ\",\n]\n\ntok = train_hf_gpt2_bytebpe(\n    corpus;\n    vocab_size=384,\n    min_frequency=1,\n)\n\nexport_tokenizer(tok, \"out_hf_gpt2\"; format=:hf_tokenizer_json)\nreloaded = load_hf_tokenizer_json(\"out_hf_gpt2/tokenizer.json\")","category":"section"},{"location":"training/#Note-on-pretokenizer","page":"Training","title":"Note on pretokenizer","text":"pretokenizer is used only during training to split input text into units for frequency counts.\nTrained tokenizers do not persist or apply the training pretokenizer at runtime.\nFor consistent behavior, apply equivalent preprocessing upstream (for example via KeemenaPreprocessing) before calling encode/encode_result.\nByteBPE exports as vocab.txt + merges.txt; when reloading exported files, use format=:bytebpe if format auto-detection is ambiguous.","category":"section"},{"location":"training/#Training-Bundles","page":"Training","title":"Training Bundles","text":"using KeemenaSubwords\n\ncorpus = [\"hello world\", \"caf√© costs 5\"]\nresult = train_wordpiece_result(corpus; vocab_size=96, min_frequency=1)\n\nsave_training_bundle(result, \"out_bundle\")\nreloaded = load_training_bundle(\"out_bundle\")\n\nencode(reloaded, \"hello caf√©\"; add_special_tokens=false)\n\nsave_training_bundle writes exported tokenizer files plus keemena_training_manifest.json, so reload does not require remembering loader kwargs. Offsets behavior remains unchanged and compatible with tokenization_view(...) + encode_result(...; assume_normalized=true).\n\nCurrent behavior:\n\nSentencePiece training supports both model_type=:unigram and model_type=:bpe.\nUnigram training defaults to SentencePiece-style whitespace_marker=\"‚ñÅ\" so multi-word text can round-trip through decode(encode(...)).\nIf whitespace_marker=\"\", runtime Unigram tokenization is still word-split, so decoding may collapse spaces in multi-word text (for example \"hello world\" -> \"helloworld\").\n\nThe pretrained-tokenizer APIs (load_tokenizer, tokenize, encode, encode_result, decode) remain stable and independent from training codepaths.","category":"section"},{"location":"training/#KeemenaSubwords.Training.train_bpe","page":"Training","title":"KeemenaSubwords.Training.train_bpe","text":"Train a character-level BPE tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_bpe_result","page":"Training","title":"KeemenaSubwords.Training.train_bpe_result","text":"Train a character-level BPE tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_bytebpe","page":"Training","title":"KeemenaSubwords.Training.train_bytebpe","text":"Train a byte-level BPE tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_bytebpe_result","page":"Training","title":"KeemenaSubwords.Training.train_bytebpe_result","text":"Train a byte-level BPE tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_unigram","page":"Training","title":"KeemenaSubwords.Training.train_unigram","text":"High-level Unigram training entry point.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_unigram_result","page":"Training","title":"KeemenaSubwords.Training.train_unigram_result","text":"Train a Unigram tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_wordpiece","page":"Training","title":"KeemenaSubwords.Training.train_wordpiece","text":"Train a WordPiece tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_wordpiece_result","page":"Training","title":"KeemenaSubwords.Training.train_wordpiece_result","text":"Train a WordPiece tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_sentencepiece","page":"Training","title":"KeemenaSubwords.Training.train_sentencepiece","text":"Train a SentencePiece tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_sentencepiece_result","page":"Training","title":"KeemenaSubwords.Training.train_sentencepiece_result","text":"Train a SentencePiece tokenizer and return model artifacts.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_bert_wordpiece","page":"Training","title":"KeemenaSubwords.Training.train_hf_bert_wordpiece","text":"train_hf_bert_wordpiece(corpus; kwargs...) -> HuggingFaceJSONTokenizer\n\nTrain a BERT-style WordPiece tokenizer and return a HuggingFaceJSONTokenizer pipeline composed of:\n\nBertNormalizer\nBertPreTokenizer\nBertProcessing (CLS/SEP insertion)\nWordPiece decoder\n\nSpecial token behavior:\n\nadd_special_tokens=true inserts [CLS] and [SEP] via post-processing.\nSpecial tokens present verbatim in input text can also be matched via HF added_tokens patterns.\n\nKeemenaPreprocessing integration:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\nencode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)\n\nExport/reload flow:\n\nexport_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)\nload_hf_tokenizer_json(joinpath(out_dir, \"tokenizer.json\"))\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_bert_wordpiece_result","page":"Training","title":"KeemenaSubwords.Training.train_hf_bert_wordpiece_result","text":"train_hf_bert_wordpiece_result(corpus; kwargs...) ->\n    TrainingResult{HuggingFaceJSONTokenizer,BertWordPieceTrainingConfig,BertWordPieceTrainingArtifacts}\n\nTrain a BERT-style WordPiece tokenizer and return:\n\ntokenizer::HuggingFaceJSONTokenizer\nconfig::BertWordPieceTrainingConfig\nartifacts::BertWordPieceTrainingArtifacts\n\nThe returned tokenizer includes BertNormalizer, BertPreTokenizer, BertProcessing, and WordPiece decoding, with special tokens exported as HF added_tokens for deterministic save/reload parity.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_roberta_bytebpe","page":"Training","title":"KeemenaSubwords.Training.train_hf_roberta_bytebpe","text":"train_hf_roberta_bytebpe(corpus; kwargs...) -> HuggingFaceJSONTokenizer\n\nTrain a RoBERTa-style ByteLevel BPE tokenizer and return a HuggingFaceJSONTokenizer pipeline composed of:\n\nByteLevel pre-tokenization\nRobertaProcessing (<s> ... </s> insertion)\nByteLevel decoding\n\nSpecial token behavior:\n\nadd_special_tokens=true inserts BOS/EOS via RobertaProcessing.\nSpecial tokens present verbatim in input text can be matched via HF added_tokens patterns.\nBy default the preset enables HF-style ByteLevel settings: use_regex=true, add_prefix_space=true, and trim_offsets=true.\n\nKeemenaPreprocessing integration:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\nencode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)\n\nExport/reload flow:\n\nexport_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)\nload_hf_tokenizer_json(joinpath(out_dir, \"tokenizer.json\"))\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_roberta_bytebpe_result","page":"Training","title":"KeemenaSubwords.Training.train_hf_roberta_bytebpe_result","text":"train_hf_roberta_bytebpe_result(corpus; kwargs...) ->\n    TrainingResult{HuggingFaceJSONTokenizer,RobertaByteBPETrainingConfig,RobertaByteBPETrainingArtifacts}\n\nTrain a RoBERTa-style ByteLevel BPE tokenizer and return:\n\ntokenizer::HuggingFaceJSONTokenizer\nconfig::RobertaByteBPETrainingConfig\nartifacts::RobertaByteBPETrainingArtifacts\n\nThe returned tokenizer wraps an inner trained ByteBPETokenizer and preserves a HF-native pipeline (ByteLevel pre-tokenizer/decoder + RobertaProcessing).\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_gpt2_bytebpe","page":"Training","title":"KeemenaSubwords.Training.train_hf_gpt2_bytebpe","text":"train_hf_gpt2_bytebpe(corpus; kwargs...) -> HuggingFaceJSONTokenizer\n\nTrain a GPT-2 style ByteLevel BPE tokenizer and return a HuggingFaceJSONTokenizer pipeline composed of:\n\nNo-op normalizer\nByteLevel pre-tokenization\nByteLevel post-processing (no BOS/EOS insertion)\nByteLevel decoding\n\nSpecial token behavior:\n\nBy default, this preset uses a single special token: special_tokens=Dict(:unk => \"<|endoftext|>\").\nadd_special_tokens=true does not change ids by default because GPT-2 style ByteLevel pipelines do not auto-insert BOS/EOS.\nSpecial tokens present verbatim in input text can still be matched through HF added_tokens patterns.\n\nKeemenaPreprocessing integration:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\nencode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true)\n\nExport/reload flow:\n\nexport_tokenizer(tokenizer, out_dir; format=:hf_tokenizer_json)\nload_hf_tokenizer_json(joinpath(out_dir, \"tokenizer.json\"))\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.train_hf_gpt2_bytebpe_result","page":"Training","title":"KeemenaSubwords.Training.train_hf_gpt2_bytebpe_result","text":"train_hf_gpt2_bytebpe_result(corpus; kwargs...) ->\n    TrainingResult{HuggingFaceJSONTokenizer,GPT2ByteBPETrainingConfig,GPT2ByteBPETrainingArtifacts}\n\nTrain a GPT-2 style ByteLevel BPE tokenizer and return:\n\ntokenizer::HuggingFaceJSONTokenizer\nconfig::GPT2ByteBPETrainingConfig\nartifacts::GPT2ByteBPETrainingArtifacts\n\nThe returned tokenizer wraps an inner trained ByteBPETokenizer and preserves a HF-native ByteLevel pipeline. By default, exported HF JSON uses model.unk_token = null for GPT-2 compatibility while the internal Julia base tokenizer still uses a concrete unknown token string.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.write_training_manifest","page":"Training","title":"KeemenaSubwords.Training.write_training_manifest","text":"write_training_manifest(outdir, manifest)\n\nWrite a TrainingManifestV1 to outdir/keemena_training_manifest.json.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.read_training_manifest","page":"Training","title":"KeemenaSubwords.Training.read_training_manifest","text":"read_training_manifest(outdir) -> TrainingManifestV1\n\nRead outdir/keemena_training_manifest.json.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.save_training_bundle","page":"Training","title":"KeemenaSubwords.Training.save_training_bundle","text":"save_training_bundle(result, outdir; export_format=:auto, overwrite=false)\n\nExport a trained tokenizer result and write a deterministic v1 manifest into outdir so the bundle can be reloaded later with load_training_bundle.\n\n\n\n\n\n","category":"function"},{"location":"training/#KeemenaSubwords.Training.load_training_bundle","page":"Training","title":"KeemenaSubwords.Training.load_training_bundle","text":"load_training_bundle(outdir) -> AbstractSubwordTokenizer\n\nLoad a tokenizer bundle previously written by save_training_bundle.\n\n\n\n\n\n","category":"function"},{"location":"loading_local/#Loading-Tokenizers-From-Local-Paths","page":"Loading Local","title":"Loading Tokenizers From Local Paths","text":"Use explicit loader functions when you know the file contract. Use load_tokenizer(path; format=:auto) only when auto-detection is preferred.\n\nNamed-spec convention:\n\nuse path as the canonical key for single-file formats,\nkeep format-specific pair keys for multi-file formats (vocab_json + merges_txt, encoder_json + vocab_bpe).\nbackward-compatible aliases (vocab_txt, model_file, encoding_file, tokenizer_json) are still accepted.","category":"section"},{"location":"loading_local/#1)-GPT-2-/-RoBERTa-style-BPE-(vocab.json-merges.txt)","page":"Loading Local","title":"1) GPT-2 / RoBERTa style BPE (vocab.json + merges.txt)","text":"tok = load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n\n# equivalent named spec\nload_tokenizer((format=:bpe_gpt2, vocab_json=\"/path/to/vocab.json\", merges_txt=\"/path/to/merges.txt\"))","category":"section"},{"location":"loading_local/#2)-OpenAI-encoder-variant-(encoder.json-vocab.bpe)","page":"Loading Local","title":"2) OpenAI encoder variant (encoder.json + vocab.bpe)","text":"tok = load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n\n# equivalent named spec\nload_tokenizer((format=:bpe_encoder, encoder_json=\"/path/to/encoder.json\", vocab_bpe=\"/path/to/vocab.bpe\"))","category":"section"},{"location":"loading_local/#3)-Classic-BPE-/-Byte-level-BPE-(vocab.txt-merges.txt)","page":"Loading Local","title":"3) Classic BPE / Byte-level BPE (vocab.txt + merges.txt)","text":"classic = load_bpe(\"/path/to/model_dir\")\nbyte_level = load_bytebpe(\"/path/to/model_dir\")","category":"section"},{"location":"loading_local/#4)-WordPiece-(vocab.txt)","page":"Loading Local","title":"4) WordPiece (vocab.txt)","text":"wp = load_wordpiece(\"/path/to/vocab.txt\"; continuation_prefix=\"##\")\n\n# register via canonical key\nregister_local_model!(\n    :my_wordpiece,\n    (format=:wordpiece_vocab, path=\"/path/to/vocab.txt\");\n    description=\"local WordPiece\",\n)","category":"section"},{"location":"loading_local/#5)-SentencePiece-(.model,-.model.v3,-sentencepiece.bpe.model)","page":"Loading Local","title":"5) SentencePiece (.model, .model.v3, sentencepiece.bpe.model)","text":"load_sentencepiece accepts either:\n\nstandard SentencePiece binary model files,\nor Keemena text-exported SentencePiece files (same filename patterns).\n\nsp_auto = load_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nsp_uni = load_sentencepiece(\"/path/to/spm.model\"; kind=:unigram)\nsp_bpe = load_sentencepiece(\"/path/to/tokenizer.model.v3\"; kind=:bpe)\n\nregister_local_model!(:my_sp, (format=:sentencepiece_model, path=\"/path/to/tokenizer.model\"))","category":"section"},{"location":"loading_local/#6)-tiktoken-(*.tiktoken-or-text-tokenizer.model)","page":"Loading Local","title":"6) tiktoken (*.tiktoken or text tokenizer.model)","text":"tt = load_tiktoken(\"/path/to/o200k_base.tiktoken\")\nllama3_style = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\n\nregister_local_model!(:my_tiktoken, (format=:tiktoken, path=\"/path/to/tokenizer.model\"))","category":"section"},{"location":"loading_local/#7)-Hugging-Face-tokenizer.json","page":"Loading Local","title":"7) Hugging Face tokenizer.json","text":"hf = load_hf_tokenizer_json(\"/path/to/tokenizer.json\")\n\nregister_local_model!(:my_hf, (format=:hf_tokenizer_json, path=\"/path/to/tokenizer.json\"))","category":"section"},{"location":"loading_local/#8)-Generic-auto-detect-override","page":"Loading Local","title":"8) Generic auto-detect + override","text":"auto_tok = load_tokenizer(\"/path/to/model_dir\")\nforced = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"loading_local/#9)-Explicit-FilesSpec-objects","page":"Loading Local","title":"9) Explicit FilesSpec objects","text":"spec = FilesSpec(\n    format=:bpe_gpt2,\n    vocab_json=\"/path/to/vocab.json\",\n    merges_txt=\"/path/to/merges.txt\",\n)\ntok = load_tokenizer(spec)\nregister_local_model!(:my_bpe, spec; description=\"explicit file spec\")","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Explicit-Loader-APIs","page":"API Reference","title":"Explicit Loader APIs","text":"Structured encoding and file-spec APIs are also part of the public surface: TokenizationResult, FilesSpec, normalize, tokenization_view, requires_tokenizer_normalization, offsets_coordinate_system, offsets_index_base, offsets_span_style, offsets_sentinel, has_span, has_nonempty_span, span_ncodeunits, span_codeunits, is_valid_string_boundary, try_span_substring, offsets_are_nonoverlapping, validate_offsets_contract, assert_offsets_contract, encode_result, encode_batch_result.","category":"section"},{"location":"api/#Registry-and-Installation-APIs","page":"API Reference","title":"Registry and Installation APIs","text":"register_external_model! remains available as a deprecated compatibility alias; prefer register_local_model! in new code.","category":"section"},{"location":"api/#Full-Exported-API","page":"API Reference","title":"Full Exported API","text":"","category":"section"},{"location":"api/#KeemenaSubwords.load_bpe","page":"API Reference","title":"KeemenaSubwords.load_bpe","text":"Load a BPE tokenizer from either a directory (vocab.txt + merges.txt) or a vocab file path.\n\n\n\n\n\nLoad a BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_bytebpe","page":"API Reference","title":"KeemenaSubwords.load_bytebpe","text":"Load a byte-level BPE tokenizer from a directory (vocab.txt + merges.txt) or vocab path.\n\n\n\n\n\nLoad a byte-level BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_bpe_gpt2","page":"API Reference","title":"KeemenaSubwords.load_bpe_gpt2","text":"Load GPT-2 / RoBERTa style BPE from vocab.json + merges.txt.\n\nExample: load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_bpe_encoder","page":"API Reference","title":"KeemenaSubwords.load_bpe_encoder","text":"Load GPT-2 encoder variant from encoder.json + vocab.bpe.\n\nExample: load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_unigram","page":"API Reference","title":"KeemenaSubwords.load_unigram","text":"Load a Unigram tokenizer from unigram.tsv (file or directory).\n\nExpected format (tab-separated): token<TAB>score[<TAB>special_symbol]\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_wordpiece","page":"API Reference","title":"KeemenaSubwords.load_wordpiece","text":"Load a WordPiece tokenizer from a vocab file path or a directory containing vocab.txt.\n\nExamples:\n\nload_wordpiece(\"/path/to/vocab.txt\")\nload_wordpiece(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_sentencepiece","page":"API Reference","title":"KeemenaSubwords.load_sentencepiece","text":"Load a SentencePiece .model file.\n\nSupported inputs:\n\nstandard SentencePiece binary protobuf .model/.model.v3 payloads\nKeemena text-exported model files:\nkey/value lines (type=unigram|bpe, whitespace_marker=‚ñÅ, unk_token=<unk>)\npiece rows: piece<TAB>token<TAB>score[<TAB>special_symbol]\nbpe merge rows (for type=bpe): merge<TAB>left<TAB>right\n\nExamples:\n\nload_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nload_sentencepiece(\"/path/to/tokenizer.model.v3\"; kind=:bpe)\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_tiktoken","page":"API Reference","title":"KeemenaSubwords.load_tiktoken","text":"Load a tiktoken encoding file (*.tiktoken).\n\nThe expected format is line-based: <base64_token_bytes><space><rank> where ranks are non-negative integers.\n\nExamples:\n\nload_tiktoken(\"/path/to/o200k_base.tiktoken\")\nload_tiktoken(\"/path/to/tokenizer.model\") (when file contains tiktoken text lines)\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_hf_tokenizer_json","page":"API Reference","title":"KeemenaSubwords.load_hf_tokenizer_json","text":"Load a Hugging Face tokenizer.json tokenizer in pure Julia.\n\nExpected files:\n\ntokenizer.json directly, or\na directory containing tokenizer.json.\n\nExamples:\n\nload_hf_tokenizer_json(\"/path/to/tokenizer.json\")\nload_hf_tokenizer_json(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.load_tokenizer","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer by built-in model name.\n\n\n\n\n\nLoad tokenizer from file system path.\n\nCommon format contracts:\n\n:hf_tokenizer_json -> tokenizer.json\n:bpe_gpt2 -> vocab.json + merges.txt\n:bpe_encoder -> encoder.json + vocab.bpe\n:wordpiece / :wordpiece_vocab -> vocab.txt\n:sentencepiece_model -> *.model / *.model.v3 / sentencepiece.bpe.model\n:tiktoken -> *.tiktoken or tiktoken-text tokenizer.model\n\nExamples:\n\nload_tokenizer(\"/path/to/model_dir\")\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.json\"; format=:hf_tokenizer_json)\n\n\n\n\n\nLoad tokenizer from explicit (vocab_path, merges_path) tuple.\n\nThis tuple form is for classic BPE/byte-level BPE (vocab.txt + merges.txt) or explicit JSON-pair loaders (vocab.json + merges.txt, encoder.json + vocab.bpe) when accompanied by format.\n\n\n\n\n\nLoad tokenizer from a named specification.\n\nExamples:\n\n(format=:wordpiece, path=\"/.../vocab.txt\")\n(format=:hf_tokenizer_json, path=\"/.../tokenizer.json\")\n(format=:unigram, path=\"/.../unigram.tsv\")\n(format=:bpe_gpt2, vocab_json=\"/.../vocab.json\", merges_txt=\"/.../merges.txt\")\n(format=:bpe_encoder, encoder_json=\"/.../encoder.json\", vocab_bpe=\"/.../vocab.bpe\")\n(format=:wordpiece, vocab_txt=\"/.../vocab.txt\") (alias)\n(format=:sentencepiece_model, model_file=\"/.../tokenizer.model\") (alias)\n(format=:tiktoken, encoding_file=\"/.../o200k_base.tiktoken\") (alias)\n(format=:hf_tokenizer_json, tokenizer_json=\"/.../tokenizer.json\") (alias)\n(format=:unigram, unigram_tsv=\"/.../unigram.tsv\") (alias)\n\n\n\n\n\nLoad tokenizer from a FilesSpec.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.detect_tokenizer_format","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_format","text":"Detect tokenizer format from a local file or directory.\n\nReturns one of symbols such as :hf_tokenizer_json, :bpe_gpt2, :bpe_encoder, :sentencepiece_model, :tiktoken, :wordpiece, :bpe, or :unigram.\n\nExamples:\n\ndetect_tokenizer_format(\"/path/to/model_dir\")\ndetect_tokenizer_format(\"/path/to/tokenizer.model\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.detect_tokenizer_files","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_files","text":"Inspect a tokenizer directory and return detected candidate files.\n\nExample: detect_tokenizer_files(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.available_models","page":"API Reference","title":"KeemenaSubwords.available_models","text":"List available built-in model names.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.describe_model","page":"API Reference","title":"KeemenaSubwords.describe_model","text":"Describe a built-in model.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.model_path","page":"API Reference","title":"KeemenaSubwords.model_path","text":"Resolve built-in model name to on-disk path.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.prefetch_models","page":"API Reference","title":"KeemenaSubwords.prefetch_models","text":"Ensure artifact-backed built-in models are present on disk.\n\nReturns a dictionary of key => is_available.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.register_local_model!","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register a local tokenizer path under a symbolic key and persist it in the cache registry.\n\n\n\n\n\nRegister local model files by explicit specification.\n\n\n\n\n\nRegister local model files from a FilesSpec.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.install_model!","page":"API Reference","title":"KeemenaSubwords.install_model!","text":"Install an installable-gated tokenizer into the user cache and register it by key.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.install_llama2_tokenizer!","page":"API Reference","title":"KeemenaSubwords.install_llama2_tokenizer!","text":"Install the gated LLaMA 2 tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama2_tokenizer; ...).\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.install_llama3_8b_tokenizer!","page":"API Reference","title":"KeemenaSubwords.install_llama3_8b_tokenizer!","text":"Install the gated LLaMA 3 8B tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama3_8b_tokenizer; ...).\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.download_hf_files","page":"API Reference","title":"KeemenaSubwords.download_hf_files","text":"Download selected files from a Hugging Face repository revision into cache.\n\nThis helper is opt-in and useful for user-managed / gated tokenizers.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.recommended_defaults_for_llms","page":"API Reference","title":"KeemenaSubwords.recommended_defaults_for_llms","text":"Recommended built-in keys for LLM-oriented default prefetching.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.AbstractSubwordTokenizer","page":"API Reference","title":"KeemenaSubwords.AbstractSubwordTokenizer","text":"Abstract parent type for all subword tokenizers.\n\nTokenizers are callable and support: tokenizer(text::AbstractString) -> Vector{String}.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.FilesSpec","page":"API Reference","title":"KeemenaSubwords.FilesSpec","text":"Structured file specification for local tokenizer loading/registration.\n\nUse path for single-file formats and explicit pairs for multi-file formats.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.SubwordVocabulary","page":"API Reference","title":"KeemenaSubwords.SubwordVocabulary","text":"Vocabulary container with forward/reverse lookup and special token IDs.\n\nIDs are 1-based.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.TokenizationResult","page":"API Reference","title":"KeemenaSubwords.TokenizationResult","text":"Structured tokenization output for downstream pipelines.\n\nOffset contract:\n\ncoordinate unit: UTF-8 codeunits.\nindex base: 1.\nspan style: half-open [start, stop).\nvalid bounds for spanful tokens: 1 <= start <= stop <= ncodeunits(text) + 1.\nsentinel for tokens without source-text spans: (0, 0).\ninserted post-processor specials use sentinel offsets.\npresent-in-text special added tokens keep real spans, and may still have special_tokens_mask[i] == 1.\nspecial_tokens_mask marks special-token identity; offsets determine span participation.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.TokenizerMetadata","page":"API Reference","title":"KeemenaSubwords.TokenizerMetadata","text":"Common metadata for tokenizer instances.\n\n\n\n\n\n","category":"type"},{"location":"api/#KeemenaSubwords.assert_offsets_contract-Tuple{AbstractString, Vector{Tuple{Int64, Int64}}}","page":"API Reference","title":"KeemenaSubwords.assert_offsets_contract","text":"Assert offsets satisfy the package offset contract.\n\nThrows ArgumentError on first contract violation. With require_string_boundaries=true, non-empty spans must start/end on valid Julia string boundaries.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.asset_status-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.asset_status","text":"Return prefetch status for a single model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.available_models-Tuple{}","page":"API Reference","title":"KeemenaSubwords.available_models","text":"List available built-in model names.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"Return beginning-of-sequence token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.bos_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.bos_id","text":"BOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.cached_tokenizers-Tuple{}","page":"API Reference","title":"KeemenaSubwords.cached_tokenizers","text":"List cache keys for in-session cached tokenizers.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.clear_tokenizer_cache!-Tuple{}","page":"API Reference","title":"KeemenaSubwords.clear_tokenizer_cache!","text":"Clear the in-session tokenizer cache used by one-call convenience APIs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{AbstractString, AbstractVector{<:Integer}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"One-call decode by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{AbstractSubwordTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode token IDs into text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{BPETokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode token IDs to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{ByteBPETokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode byte-level BPE IDs back to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{SentencePieceTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode SentencePiece IDs back to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{Symbol, AbstractVector{<:Integer}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"One-call decode by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{TiktokenTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode tiktoken rank IDs to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{UnigramTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode unigram token IDs back to text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.decode-Tuple{WordPieceTokenizer, AbstractVector{Int64}}","page":"API Reference","title":"KeemenaSubwords.decode","text":"Decode WordPiece token IDs back into text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.describe_model-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.describe_model","text":"Describe a built-in model.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.detect_tokenizer_files-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_files","text":"Inspect a tokenizer directory and return detected candidate files.\n\nExample: detect_tokenizer_files(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.detect_tokenizer_format-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.detect_tokenizer_format","text":"Detect tokenizer format from a local file or directory.\n\nReturns one of symbols such as :hf_tokenizer_json, :bpe_gpt2, :bpe_encoder, :sentencepiece_model, :tiktoken, :wordpiece, :bpe, or :unigram.\n\nExamples:\n\ndetect_tokenizer_format(\"/path/to/model_dir\")\ndetect_tokenizer_format(\"/path/to/tokenizer.model\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.download_hf_files-Tuple{AbstractString, AbstractVector{<:AbstractString}}","page":"API Reference","title":"KeemenaSubwords.download_hf_files","text":"Download selected files from a Hugging Face repository revision into cache.\n\nThis helper is opt-in and useful for user-managed / gated tokenizers.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"One-call encode by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text into token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{BPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{ByteBPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to byte-level BPE IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{SentencePieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to SentencePiece IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"One-call encode by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{TiktokenTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text into tiktoken rank IDs (1-based in this package).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{UnigramTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to unigram token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode-Tuple{WordPieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode","text":"Encode text to WordPiece token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode_batch_result-Tuple{AbstractSubwordTokenizer, AbstractVector{<:AbstractString}}","page":"API Reference","title":"KeemenaSubwords.encode_batch_result","text":"Batch variant of encode_result.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode_result","page":"API Reference","title":"KeemenaSubwords.encode_result","text":"Encode text and return a structured TokenizationResult.\n\nKey keyword arguments:\n\nassume_normalized::Bool=false: when true, tokenizer intrinsic normalization is skipped and offsets are computed against the exact provided text.\nreturn_offsets::Bool=false: include token-level offsets when available.\nreturn_masks::Bool=false: include attention/token-type/special-token masks.\n\nOffset note:\n\nOffsets use the package-wide 1-based UTF-8 codeunit half-open convention.\nassume_normalized changes whether intrinsic normalization runs; it does not change the offset coordinate system.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.encode_result-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode_result","text":"One-call structured encode by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.encode_result-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.encode_result","text":"One-call structured encode by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"Return end-of-sequence token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.eos_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.eos_id","text":"EOS token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.export_tokenizer-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.export_tokenizer","text":"Export tokenizer to external formats.\n\nSupported format values:\n\n:internal\n:bpe / :bpe_gpt2\n:wordpiece_vocab\n:unigram_tsv\n:sentencepiece_model\n:hf_tokenizer_json\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.get_tokenizer_cached-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.get_tokenizer_cached","text":"Return a cached tokenizer for a model key or path, loading and caching on first use.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.has_nonempty_span-Tuple{Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.has_nonempty_span","text":"Return true when an offset carries a non-empty source-text span.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.has_span-Tuple{Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.has_span","text":"Return true when an offset carries a real source-text span.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{AbstractSubwordTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{BPETokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{ByteBPETokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{SentencePieceTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{SubwordVocabulary, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Map ID to token string.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{TiktokenTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{UnigramTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.id_to_token-Tuple{WordPieceTokenizer, Int64}","page":"API Reference","title":"KeemenaSubwords.id_to_token","text":"Reverse token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.install_llama2_tokenizer!-Tuple{}","page":"API Reference","title":"KeemenaSubwords.install_llama2_tokenizer!","text":"Install the gated LLaMA 2 tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama2_tokenizer; ...).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.install_llama3_8b_tokenizer!-Tuple{}","page":"API Reference","title":"KeemenaSubwords.install_llama3_8b_tokenizer!","text":"Install the gated LLaMA 3 8B tokenizer files into local cache and register them.\n\nThis is a convenience wrapper over install_model!(:llama3_8b_tokenizer; ...).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.install_model!-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.install_model!","text":"Install an installable-gated tokenizer into the user cache and register it by key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.is_valid_string_boundary-Tuple{AbstractString, Int64}","page":"API Reference","title":"KeemenaSubwords.is_valid_string_boundary","text":"Return whether idx is a valid Julia string boundary for text.\n\nThis includes the exclusive end boundary ncodeunits(text) + 1.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.keemena_callable-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.keemena_callable","text":"Return a function compatible with KeemenaPreprocessing's callable tokenizer contract.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.level_key-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.level_key","text":"Level key used by KeemenaPreprocessing for callable tokenizers.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe","text":"Load a BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe","text":"Load a BPE tokenizer from either a directory (vocab.txt + merges.txt) or a vocab file path.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe_encoder-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe_encoder","text":"Load GPT-2 encoder variant from encoder.json + vocab.bpe.\n\nExample: load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bpe_gpt2-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bpe_gpt2","text":"Load GPT-2 / RoBERTa style BPE from vocab.json + merges.txt.\n\nExample: load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bytebpe-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bytebpe","text":"Load a byte-level BPE tokenizer from explicit vocab + merges paths.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_bytebpe-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_bytebpe","text":"Load a byte-level BPE tokenizer from a directory (vocab.txt + merges.txt) or vocab path.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_hf_tokenizer_json-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_hf_tokenizer_json","text":"Load a Hugging Face tokenizer.json tokenizer in pure Julia.\n\nExpected files:\n\ntokenizer.json directly, or\na directory containing tokenizer.json.\n\nExamples:\n\nload_hf_tokenizer_json(\"/path/to/tokenizer.json\")\nload_hf_tokenizer_json(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_sentencepiece-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_sentencepiece","text":"Load a SentencePiece .model file.\n\nSupported inputs:\n\nstandard SentencePiece binary protobuf .model/.model.v3 payloads\nKeemena text-exported model files:\nkey/value lines (type=unigram|bpe, whitespace_marker=‚ñÅ, unk_token=<unk>)\npiece rows: piece<TAB>token<TAB>score[<TAB>special_symbol]\nbpe merge rows (for type=bpe): merge<TAB>left<TAB>right\n\nExamples:\n\nload_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nload_sentencepiece(\"/path/to/tokenizer.model.v3\"; kind=:bpe)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tiktoken-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_tiktoken","text":"Load a tiktoken encoding file (*.tiktoken).\n\nThe expected format is line-based: <base64_token_bytes><space><rank> where ranks are non-negative integers.\n\nExamples:\n\nload_tiktoken(\"/path/to/o200k_base.tiktoken\")\nload_tiktoken(\"/path/to/tokenizer.model\") (when file contains tiktoken text lines)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from file system path.\n\nCommon format contracts:\n\n:hf_tokenizer_json -> tokenizer.json\n:bpe_gpt2 -> vocab.json + merges.txt\n:bpe_encoder -> encoder.json + vocab.bpe\n:wordpiece / :wordpiece_vocab -> vocab.txt\n:sentencepiece_model -> *.model / *.model.v3 / sentencepiece.bpe.model\n:tiktoken -> *.tiktoken or tiktoken-text tokenizer.model\n\nExamples:\n\nload_tokenizer(\"/path/to/model_dir\")\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.json\"; format=:hf_tokenizer_json)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{FilesSpec}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from a FilesSpec.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{NamedTuple}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from a named specification.\n\nExamples:\n\n(format=:wordpiece, path=\"/.../vocab.txt\")\n(format=:hf_tokenizer_json, path=\"/.../tokenizer.json\")\n(format=:unigram, path=\"/.../unigram.tsv\")\n(format=:bpe_gpt2, vocab_json=\"/.../vocab.json\", merges_txt=\"/.../merges.txt\")\n(format=:bpe_encoder, encoder_json=\"/.../encoder.json\", vocab_bpe=\"/.../vocab.bpe\")\n(format=:wordpiece, vocab_txt=\"/.../vocab.txt\") (alias)\n(format=:sentencepiece_model, model_file=\"/.../tokenizer.model\") (alias)\n(format=:tiktoken, encoding_file=\"/.../o200k_base.tiktoken\") (alias)\n(format=:hf_tokenizer_json, tokenizer_json=\"/.../tokenizer.json\") (alias)\n(format=:unigram, unigram_tsv=\"/.../unigram.tsv\") (alias)\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer by built-in model name.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_tokenizer-Tuple{Tuple{AbstractString, AbstractString}}","page":"API Reference","title":"KeemenaSubwords.load_tokenizer","text":"Load tokenizer from explicit (vocab_path, merges_path) tuple.\n\nThis tuple form is for classic BPE/byte-level BPE (vocab.txt + merges.txt) or explicit JSON-pair loaders (vocab.json + merges.txt, encoder.json + vocab.bpe) when accompanied by format.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_unigram-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_unigram","text":"Load a Unigram tokenizer from unigram.tsv (file or directory).\n\nExpected format (tab-separated): token<TAB>score[<TAB>special_symbol]\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.load_wordpiece-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.load_wordpiece","text":"Load a WordPiece tokenizer from a vocab file path or a directory containing vocab.txt.\n\nExamples:\n\nload_wordpiece(\"/path/to/vocab.txt\")\nload_wordpiece(\"/path/to/model_dir\")\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Return model metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_info-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.model_info","text":"Tokenizer metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.model_path-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.model_path","text":"Resolve built-in model name to on-disk path.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.normalize-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.normalize","text":"Return tokenizer intrinsic normalization output.\n\nThis does not perform pipeline-level preprocessing. Tokenizers without intrinsic normalization return text unchanged.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.normalize_text-Tuple{AbstractString}","page":"API Reference","title":"KeemenaSubwords.normalize_text","text":"Normalize text using an optional user-provided callable.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_are_nonoverlapping-Tuple{Vector{Tuple{Int64, Int64}}}","page":"API Reference","title":"KeemenaSubwords.offsets_are_nonoverlapping","text":"Return whether participating offsets are non-overlapping in sequence order.\n\nParticipating offsets satisfy:\n\nnot sentinel when ignore_sentinel=true\nnot empty when ignore_empty=true\n\nFor participating offsets, this enforces next.start >= prev.stop.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_coordinate_system-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_coordinate_system","text":"Offset coordinate system for TokenizationResult.offsets.\n\nOffsets are UTF-8 codeunit indices with half-open span convention [start, stop).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_index_base-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_index_base","text":"Offset index base for TokenizationResult.offsets.\n\nOffsets are 1-based codeunit indices.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_sentinel-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_sentinel","text":"Sentinel used for tokens without a source-text span.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.offsets_span_style-Tuple{}","page":"API Reference","title":"KeemenaSubwords.offsets_span_style","text":"Offset span style.\n\nTokenizationResult.offsets use half-open spans [start, stop).\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Return padding-token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.pad_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.pad_id","text":"Padding token ID if available.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.prefetch_models-2","page":"API Reference","title":"KeemenaSubwords.prefetch_models","text":"Ensure artifact-backed built-in models are present on disk.\n\nReturns a dictionary of key => is_available.\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.prefetch_models_status","page":"API Reference","title":"KeemenaSubwords.prefetch_models_status","text":"Return detailed prefetch status for built-in model keys.\n\nEach value includes:\n\navailable::Bool\nmethod::Symbol (:artifact, :fallback_download, :already_present, or :failed)\npath::Union{Nothing,String}\nerror::Union{Nothing,String}\n\n\n\n\n\n","category":"function"},{"location":"api/#KeemenaSubwords.print_asset_status-Tuple{Symbol}","page":"API Reference","title":"KeemenaSubwords.print_asset_status","text":"Print a compact prefetch status line for one model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.recommended_defaults_for_llms-Tuple{}","page":"API Reference","title":"KeemenaSubwords.recommended_defaults_for_llms","text":"Recommended built-in keys for LLM-oriented default prefetching.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_external_model!-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.register_external_model!","text":"Deprecated alias kept for compatibility. Use register_local_model! instead.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_local_model!-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register a local tokenizer path under a symbolic key and persist it in the cache registry.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_local_model!-Tuple{Symbol, FilesSpec}","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register local model files from a FilesSpec.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.register_local_model!-Tuple{Symbol, NamedTuple}","page":"API Reference","title":"KeemenaSubwords.register_local_model!","text":"Register local model files by explicit specification.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.requires_tokenizer_normalization-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.requires_tokenizer_normalization","text":"Whether this tokenizer defines intrinsic normalization that can change text.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.save_tokenizer-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.save_tokenizer","text":"Save tokenizer to a canonical on-disk format.\n\nformat=:internal chooses a tokenizer-family specific default:\n\nWordPieceTokenizer -> vocab.txt\nBPETokenizer / ByteBPETokenizer -> vocab.txt + merges.txt\nUnigramTokenizer -> unigram.tsv\nSentencePieceTokenizer -> spm.model\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.span_codeunits-Tuple{AbstractString, Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.span_codeunits","text":"Return the offset span as UTF-8 codeunits.\n\nSentinel and empty spans return UInt8[]. Invalid or out-of-bounds spans also return UInt8[] to keep this helper non-throwing for downstream inspection.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.span_ncodeunits-Tuple{Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.span_ncodeunits","text":"Return span length measured in UTF-8 codeunits.\n\nSentinel and empty spans return 0.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Return special token IDs keyed by symbol.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.special_tokens-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.special_tokens","text":"Special token IDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{BPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{ByteBPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{SentencePieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{SubwordVocabulary, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Map token string to ID, falling back to :unk.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{TiktokenTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{UnigramTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.token_to_id-Tuple{WordPieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.token_to_id","text":"Forward token lookup.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenization_view-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenization_view","text":"Canonical tokenizer text view used for subword offsets/alignment.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{AbstractString, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"One-call tokenize by tokenizer path/directory.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{AbstractSubwordTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text into subword pieces.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{BPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize with classic BPE merges.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{ByteBPETokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text by first mapping bytes to unicode symbols, then applying BPE merges.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{SentencePieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text with SentencePiece wrapper behavior.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{Symbol, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"One-call tokenize by model key.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{TiktokenTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text into b64:<...> token pieces.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{UnigramTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Tokenize text using deterministic Viterbi segmentation.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.tokenize-Tuple{WordPieceTokenizer, AbstractString}","page":"API Reference","title":"KeemenaSubwords.tokenize","text":"Greedy longest-match WordPiece tokenization.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.try_span_substring-Tuple{AbstractString, Tuple{Int64, Int64}}","page":"API Reference","title":"KeemenaSubwords.try_span_substring","text":"Attempt to return a substring for a half-open codeunit span [start, stop).\n\nSentinel and empty spans return \"\". If span boundaries are not valid Julia string boundaries, this returns nothing. This helper never throws.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Return unknown-token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.unk_id-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.unk_id","text":"Unknown token ID.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.validate_offsets_contract-Tuple{AbstractString, Vector{Tuple{Int64, Int64}}}","page":"API Reference","title":"KeemenaSubwords.validate_offsets_contract","text":"Validate offsets against the package offset contract.\n\nReturns true when all offsets satisfy bounds/sentinel invariants. With require_string_boundaries=true, non-empty spans must also start/end on valid Julia string boundaries.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{AbstractSubwordTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{BPETokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{ByteBPETokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{SentencePieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{SubwordVocabulary}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{TiktokenTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{UnigramTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"api/#KeemenaSubwords.vocab_size-Tuple{WordPieceTokenizer}","page":"API Reference","title":"KeemenaSubwords.vocab_size","text":"Vocabulary size.\n\n\n\n\n\n","category":"method"},{"location":"loading/#Loading-Tokenizers","page":"Loading","title":"Loading Tokenizers","text":"Use load_tokenizer for key-based and auto-detected loading, and use explicit constructors when you want strict file contracts.","category":"section"},{"location":"loading/#Built-in-keys","page":"Loading","title":"Built-in keys","text":"using KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\nqwen = load_tokenizer(:qwen2_5_bpe)","category":"section"},{"location":"loading/#Auto-detected-local-paths","page":"Loading","title":"Auto-detected local paths","text":"load_tokenizer(\"/path/to/model_dir\")\nload_tokenizer(\"/path/to/tokenizer.model\")\nload_tokenizer(\"/path/to/tokenizer.json\")","category":"section"},{"location":"loading/#Force-format-override","page":"Loading","title":"Force format override","text":"load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)","category":"section"},{"location":"loading/#Explicit-constructors","page":"Loading","title":"Explicit constructors","text":"load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\nload_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\nload_wordpiece(\"/path/to/vocab.txt\")\nload_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\nload_tiktoken(\"/path/to/o200k_base.tiktoken\")\nload_hf_tokenizer_json(\"/path/to/tokenizer.json\")","category":"section"},{"location":"loading/#Structured-encode-outputs","page":"Loading","title":"Structured encode outputs","text":"tok = load_tokenizer(:core_wordpiece_en)\ntokenization_text = normalize(tok, \"hello world\")\nresult = encode_result(\n    tok,\n    tokenization_text;\n    assume_normalized=true,\n    add_special_tokens=false,\n    return_offsets=true,\n    return_masks=true,\n)\n\nresult.ids\nresult.tokens\nresult.offsets  # UTF-8 codeunit spans on tokenization_text\n\nFor complete local path recipes, see Loading Tokenizers From Local Paths. For explicit file contracts and named-spec keys, see Tokenizer Formats and Required Files.","category":"section"},{"location":"troubleshooting/#Troubleshooting","page":"Troubleshooting","title":"Troubleshooting","text":"","category":"section"},{"location":"troubleshooting/#Offset-contract-sync-check-fails","page":"Troubleshooting","title":"Offset contract sync check fails","text":"The check julia --project=. tools/sync_offset_contract.jl --check enforces that notes/OffsetContract.md (source) and docs/src/normalization_offsets_contract.md (generated target) match after newline normalization.\n\nDo not hand-edit docs/src/normalization_offsets_contract.md. Edit notes/OffsetContract.md, then run:\n\njulia --project=. tools/sync_offset_contract.jl\njulia --project=. tools/sync_offset_contract.jl --check","category":"section"},{"location":"troubleshooting/#Auto-detect-picked-the-wrong-format","page":"Troubleshooting","title":"Auto-detect picked the wrong format","text":"Force the format explicitly:\n\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)\n\nUse detection helpers to inspect first:\n\ndetect_tokenizer_format(\"/path/to/model_dir\")\ndetect_tokenizer_files(\"/path/to/model_dir\")","category":"section"},{"location":"troubleshooting/#Missing-merges.txt","page":"Troubleshooting","title":"Missing merges.txt","text":"For GPT-2 style BPE you need both files:\n\nvocab.json + merges.txt\nor encoder.json + vocab.bpe\n\nUse explicit loaders for clearer errors:\n\nload_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\nload_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")","category":"section"},{"location":"troubleshooting/#tokenizer.model-is-not-SentencePiece","page":"Troubleshooting","title":"tokenizer.model is not SentencePiece","text":"Some models (notably LLaMA3-style releases) provide tiktoken text in a file named tokenizer.model.\n\nKeemenaSubwords sniffs .model files:\n\ntiktoken-like text lines => :tiktoken\nbinary / SentencePiece-like payload => :sentencepiece_model\n\nIf needed, override manually:\n\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"troubleshooting/#Gated-model-key-fails-to-load","page":"Troubleshooting","title":"Gated model key fails to load","text":"If load_tokenizer(:llama3_8b_tokenizer) says not installed, install first:\n\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])\n\nYou must have accepted upstream license terms and have valid access.","category":"section"},{"location":"offset_alignment_examples/#Offsets-Alignment-Examples","page":"Offsets Alignment Examples","title":"Offsets Alignment Examples","text":"This page is a practical tutorial for applying the normalization and offsets contract. For the normative specification, see Normalization and Offsets Contract.","category":"section"},{"location":"offset_alignment_examples/#Mental-Model-And-Coordinate-System","page":"Offsets Alignment Examples","title":"Mental Model And Coordinate System","text":"Offsets are always interpreted in the coordinate system of tokenization_text. tokenization_text may differ from clean_text when tokenizer intrinsic normalization is active.\n\nSafe pattern with KeemenaPreprocessing output:\n\ntokenization_text = tokenization_view(tokenizer, clean_text)\nencode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, ...)\n\nOffset contract reminders:\n\nOffsets are 1-based UTF-8 codeunit half-open spans (start, stop).\nstop is exclusive.\nSentinel (0, 0) means \"no span\" and should be treated as non-aligning.","category":"section"},{"location":"offset_alignment_examples/#Example-1:-Inspect-encode_result-Output","page":"Offsets Alignment Examples","title":"Example 1: Inspect encode_result Output","text":"using KeemenaSubwords\n\ntokenizer = load_tokenizer(:core_sentencepiece_unigram_en)\nclean_text = \"Hello, world! This is an offsets demo.\"\ntokenization_text = tokenization_view(tokenizer, clean_text)\n\nresult = encode_result(\n    tokenizer,\n    tokenization_text;\n    assume_normalized = true,\n    add_special_tokens = true,\n    return_offsets = true,\n    return_masks = true,\n)\n\n@assert result.offsets !== nothing\n@assert result.special_tokens_mask !== nothing\n\ntoken_offsets = result.offsets\nspecial_tokens_mask = result.special_tokens_mask\n\nrows = [\n    (\n        token_index = i,\n        token_id = result.ids[i],\n        token_string = result.tokens[i],\n        offset = token_offsets[i],\n        substring_or_nothing = try_span_substring(tokenization_text, token_offsets[i]),\n        is_special = special_tokens_mask[i] == 1,\n        has_span = has_nonempty_span(token_offsets[i]),\n    )\n    for i in eachindex(result.ids)\n]\n\nrows[1:min(end, 30)]\n\nHow to interpret these rows:\n\nTokens with offset (0, 0) are no-span tokens. They are usually inserted specials.\nis_special and has_span are related but not identical concepts. Align by span, not by mask alone.\nsubstring_or_nothing helps verify offsets quickly against tokenization_text.\nUse tokenization_text for offset-based slicing. Do not assume clean_text uses the same coordinates.","category":"section"},{"location":"offset_alignment_examples/#Example-2:-Word-Offsets-And-Subword-To-Word-Mapping","page":"Offsets Alignment Examples","title":"Example 2: Word Offsets And Subword-To-Word Mapping","text":"function whitespace_word_offsets(text)::Vector{Tuple{Int,Int}}\n    offsets = Tuple{Int,Int}[]\n    stop_exclusive = ncodeunits(text) + 1\n    i = firstindex(text)\n\n    while i < stop_exclusive\n        while i < stop_exclusive && isspace(text[i])\n            i = nextind(text, i)\n        end\n        i < stop_exclusive || break\n\n        word_start = i\n        while i < stop_exclusive && !isspace(text[i])\n            i = nextind(text, i)\n        end\n        word_stop = i\n        push!(offsets, (word_start, word_stop))\n    end\n\n    return offsets\nend\n\noverlap_len(a_start, a_stop, b_start, b_stop)::Int =\n    max(0, min(a_stop, b_stop) - max(a_start, b_start))\n\nfunction subword_to_word_index(\n    word_offsets::Vector{Tuple{Int,Int}},\n    subword_offset::Tuple{Int,Int},\n)::Union{Nothing,Int}\n    has_nonempty_span(subword_offset) || return nothing\n    sub_start, sub_stop = subword_offset\n\n    for (word_index, (word_start, word_stop)) in pairs(word_offsets)\n        if sub_start >= word_start && sub_stop <= word_stop\n            return word_index\n        end\n    end\n\n    best_index = nothing\n    best_overlap = 0\n    for (word_index, (word_start, word_stop)) in pairs(word_offsets)\n        current_overlap = overlap_len(sub_start, sub_stop, word_start, word_stop)\n        # Strict > means equal-overlap ties keep the earliest word index.\n        if current_overlap > best_overlap\n            best_overlap = current_overlap\n            best_index = word_index\n        end\n    end\n\n    return best_overlap > 0 ? best_index : nothing\nend\n\nword_offsets = whitespace_word_offsets(tokenization_text)\ntoken_to_word = map(token_offsets) do off\n    subword_to_word_index(word_offsets, off)\nend\n\nword_rows = [\n    (\n        word_index = i,\n        offset = word_offsets[i],\n        word_substring = try_span_substring(tokenization_text, word_offsets[i]),\n    )\n    for i in eachindex(word_offsets)\n]\n\ntoken_rows = [\n    (\n        token_index = i,\n        token_string = result.tokens[i],\n        offset = token_offsets[i],\n        word_index = token_to_word[i],\n        word_substring = token_to_word[i] === nothing ? nothing :\n            try_span_substring(tokenization_text, word_offsets[token_to_word[i]]),\n    )\n    for i in eachindex(result.tokens)\n]\n\n(\n    words = word_rows,\n    first_tokens = token_rows[1:min(end, 30)],\n)\n\nLimitations of this tutorial mapping:\n\nSubword spans can overlap multiple words in some normalization and punctuation situations.\nThis example returns one best word index (full containment first, else maximum overlap).\nEqual-overlap ties are resolved to the earliest word index.\nIf you need multi-word mapping, return all overlapping word indices instead of one index.","category":"section"},{"location":"offset_alignment_examples/#Example-3:-Special-Tokens-Policy-For-Alignment","page":"Offsets Alignment Examples","title":"Example 3: Special Tokens Policy For Alignment","text":"participates_in_alignment(offset, is_special)::Bool = has_nonempty_span(offset)\n\nalignment_rows = [\n    (\n        token_index = i,\n        token_string = result.tokens[i],\n        offset = token_offsets[i],\n        is_special = special_tokens_mask[i] == 1,\n        participates = participates_in_alignment(token_offsets[i], special_tokens_mask[i] == 1),\n    )\n    for i in eachindex(result.tokens)\n]\n\n(\n    skipped = [row for row in alignment_rows if !row.participates][1:min(end, 10)],\n    participating = [row for row in alignment_rows if row.participates][1:min(end, 10)],\n)\n\nPolicy summary:\n\nPragmatic default: participate in alignment iff has_nonempty_span(offset).\nInserted special tokens usually have (0, 0) and are skipped automatically.","category":"section"},{"location":"offset_alignment_examples/#Example-4:-Byte-Level-Caveat-And-Safe-Extraction","page":"Offsets Alignment Examples","title":"Example 4: Byte-Level Caveat And Safe Extraction","text":"Byte-level tokenizers can produce offsets that are valid codeunit spans but are not always safe Julia string slicing boundaries on multibyte text.\n\nWhen you consume offsets, use this safe pattern:\n\n# non-executable byte-level pattern\nsubstring = try_span_substring(tokenization_text, offset)\n\nif substring === nothing && has_nonempty_span(offset)\n    bytes = span_codeunits(tokenization_text, offset)\n    # Use bytes in a byte-aware path when boundaries are not string-safe.\nend\n\nThis fallback keeps alignment pipelines robust across both string-safe and byte-level offset cases.","category":"section"},{"location":"offset_alignment_examples/#Example-5:-Map-A-Labeled-Span-To-Token-Indices","page":"Offsets Alignment Examples","title":"Example 5: Map A Labeled Span To Token Indices","text":"function token_indices_overlapping_span(\n    offsets::Vector{Tuple{Int,Int}},\n    span::Tuple{Int,Int},\n)::Vector{Int}\n    span_start, span_stop = span\n    span_stop > span_start || return Int[]\n\n    overlaps = Int[]\n    for (token_index, offset) in pairs(offsets)\n        has_nonempty_span(offset) || continue\n        token_start, token_stop = offset\n        if min(token_stop, span_stop) > max(token_start, span_start)\n            push!(overlaps, token_index)\n        end\n    end\n    return overlaps\nend\n\nlabeled_range = findfirst(\"offsets\", tokenization_text)\n@assert labeled_range !== nothing\n\nlabeled_span = (\n    first(labeled_range),\n    nextind(tokenization_text, last(labeled_range)),\n)\noverlapping_token_indices = token_indices_overlapping_span(token_offsets, labeled_span)\n\noverlap_rows = [\n    (\n        token_index = i,\n        token_string = result.tokens[i],\n        token_offset = token_offsets[i],\n        token_substring = try_span_substring(tokenization_text, token_offsets[i]),\n    )\n    for i in overlapping_token_indices\n]\n\n(\n    labeled_span = labeled_span,\n    labeled_substring = try_span_substring(tokenization_text, labeled_span),\n    overlapping_token_indices = overlapping_token_indices,\n    overlapping_tokens = overlap_rows,\n)\n\nThis pattern is useful for projecting character/codeunit span labels onto token indices for training targets.","category":"section"},{"location":"integration/#Integration-With-KeemenaPreprocessing","page":"Integration","title":"Integration With KeemenaPreprocessing","text":"KeemenaSubwords tokenizers are callable and work with KeemenaPreprocessing's callable tokenizer contract.\n\nusing KeemenaPreprocessing\nusing KeemenaSubwords\n\ntokenizer = load_tokenizer(:core_bpe_en)\n\ncfg = PreprocessConfiguration(tokenizer_name = keemena_callable(tokenizer))\nbundle = preprocess_corpus([\"hello world\", \"hello keemena\"]; config=cfg)\n\n# KeemenaPreprocessing stores callable levels under Symbol(typeof(tokenizer))\nlvl = level_key(tokenizer)\nsubword_corpus = get_corpus(bundle, lvl)\n\nFor the normalization/offsets alignment contract (clean_text -> tokenization_text -> encode_result(...; assume_normalized=true)), see Normalization and Offsets Contract.\n\nFor onboarding context and token/id semantics, see Concepts.\n\nAlignment rule: Use tokenization_text = tokenization_view(tokenizer, clean_text), then call encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true, ...). Word and subword offsets must both be interpreted in the same tokenization_text coordinate system.\n\nSee Offsets Alignment Examples for a worked subword-to-word mapping tutorial.","category":"section"},{"location":"formats/#Tokenizer-Formats-and-Required-Files","page":"Formats","title":"Tokenizer Formats and Required Files","text":"detect_tokenizer_format(path) and load_tokenizer(path; format=:auto) use the same detection rules. You can always override detection with format=....\n\nFormat Symbol Accepted Input Required Files Canonical Named Spec Keys Recommended Call\n:hf_tokenizer_json file or directory tokenizer.json path (alias: tokenizer_json) load_hf_tokenizer_json(\"/path/to/tokenizer.json\")\n:bpe_gpt2 file pair or directory vocab.json + merges.txt vocab_json, merges_txt load_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\n:bpe_encoder file pair or directory encoder.json + vocab.bpe encoder_json, vocab_bpe load_bpe_encoder(\"/path/to/encoder.json\", \"/path/to/vocab.bpe\")\n:bpe directory or vocab.txt file vocab.txt + merges.txt vocab, merges (tuple/spec) load_bpe(\"/path/to/model_dir\")\n:bytebpe directory or file pair vocab.txt + merges.txt vocab, merges (tuple/spec) load_bytebpe(\"/path/to/model_dir\")\n:wordpiece / :wordpiece_vocab file or directory vocab.txt path (alias: vocab_txt) load_wordpiece(\"/path/to/vocab.txt\")\n:sentencepiece_model file or directory Standard SentencePiece binary .model/.model.v3 files, or Keemena text-exported .model files (spm.model, spiece.model, tokenizer.model, tokenizer.model.v3, sentencepiece.bpe.model) path (alias: model_file) load_sentencepiece(\"/path/to/tokenizer.model\"; kind=:auto)\n:tiktoken file or directory *.tiktoken or tiktoken-text tokenizer.model path (alias: encoding_file) load_tiktoken(\"/path/to/o200k_base.tiktoken\")\n:unigram file or directory unigram.tsv path (alias: unigram_tsv) load_unigram(\"/path/to/unigram.tsv\")","category":"section"},{"location":"formats/#Exporting-Hugging-Face-tokenizer.json","page":"Formats","title":"Exporting Hugging Face tokenizer.json","text":"Use the HF export target to write tokenizer.json from any supported KeemenaSubwords tokenizer:\n\nexport_tokenizer(tokenizer, \"out_dir\"; format=:hf_tokenizer_json)\n# equivalent via save_tokenizer\nsave_tokenizer(tokenizer, \"out_dir\"; format=:hf_tokenizer_json)\n\nReload in Julia:\n\nreloaded = load_tokenizer(\"out_dir\"; format=:hf_tokenizer_json)\n\nLoad the same file in Python Fast tokenizers:\n\nfrom transformers import PreTrainedTokenizerFast\ntok = PreTrainedTokenizerFast(tokenizer_file=\"out_dir/tokenizer.json\")\n\nCurrent scope note:\n\ntokenizer_config.json and special_tokens_map.json are not emitted yet.\nTemplateProcessing is emitted in canonical HF JSON shape (single/pair object items and object-map special_tokens) for better external HF compatibility.\nByte-level export writes explicit ByteLevel options (add_prefix_space=false, trim_offsets=false, use_regex=false) for Keemena ByteBPE interoperability, so Python/Rust HF loaders do not silently fall back to different defaults.","category":"section"},{"location":"formats/#BERT-Components-in-tokenizer.json","page":"Formats","title":"BERT Components in tokenizer.json","text":"KeemenaSubwords now supports the common Hugging Face BERT pipeline components:\n\nnormalizer.type = \"BertNormalizer\"\npre_tokenizer.type = \"BertPreTokenizer\"\n\nOffsets for these pipelines follow the same package contract:\n\noffsets are computed against tokenization_view(tokenizer, text) (the tokenizer-normalized text),\ninserted post-processor specials keep sentinel (0,0),\nspanful offsets remain 1-based UTF-8 codeunit half-open spans.\n\nKeemenaPreprocessing integration remains: tokenization_text = tokenization_view(tokenizer, clean_text) then encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true).","category":"section"},{"location":"formats/#Detection-Notes","page":"Formats","title":"Detection Notes","text":"Directory preference order:\ntokenizer.json\nvocab.json + merges.txt\nencoder.json + vocab.bpe\nSentencePiece model filenames\n*.tiktoken\n.model files are sniffed:\ntiktoken-like text (<base64> <int>) => :tiktoken\nbinary SentencePiece protobuf payload or Keemena text SentencePiece payload => :sentencepiece_model\nLLaMA3-style files often use tokenizer.model containing tiktoken text. Use explicit override when needed.\n\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\nload_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)","category":"section"},{"location":"formats/#Byte-level-behavior","page":"Formats","title":"Byte-level behavior","text":"Byte-level tokenizers:\n:bytebpe\n:bpe_gpt2 / :bpe_encoder\n:tiktoken\nsome :hf_tokenizer_json pipelines when ByteLevel is configured.\nNon-byte-level tokenizers:\n:wordpiece\n:sentencepiece_model\n:unigram.\n\nRound-trip expectations:\n\nByte-level families are generally robust for arbitrary UTF-8 input and usually satisfy stable decode(encode(text)).\nWordPiece/SentencePiece/Unigram operate on learned subword vocabularies; they are deterministic, but unknown-token fallback can reduce exact round-trip fidelity depending on vocab coverage.","category":"section"},{"location":"gated_models/#Installable-Gated-Models","page":"Gated Models","title":"Installable Gated Models","text":"KeemenaSubwords supports gated tokenizers (for example some LLaMA variants) through opt-in installation into your local cache.","category":"section"},{"location":"gated_models/#What-this-does","page":"Gated Models","title":"What this does","text":"install_model!(key; token=...):\n\ndownloads only tokenizer files from upstream with your credentials,\nstores them under your cache (KEEMENA_SUBWORDS_CACHE_DIR override supported),\nregisters them locally so load_tokenizer(:key) works.","category":"section"},{"location":"gated_models/#What-this-does-not-do","page":"Gated Models","title":"What this does not do","text":"No gated tokenizer files are redistributed in this repository.\nNo gated files are published in Artifacts.toml.\nNo silent background downloads happen during load_tokenizer(:key).","category":"section"},{"location":"gated_models/#Install-flow","page":"Gated Models","title":"Install flow","text":"# LLaMA 2\ninstall_model!(:llama2_tokenizer; token=ENV[\"HF_TOKEN\"])\n\n# LLaMA 3 8B\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])\n\n# then load by key\nllama3 = load_tokenizer(:llama3_8b_tokenizer)\n\nIf you already downloaded tokenizer files elsewhere, you can skip install_model! and load/register directly:\n\nllama2 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)\nllama3 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)\n\nregister_local_model!(:llama3_local, \"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"gated_models/#Discover-gated-keys","page":"Gated Models","title":"Discover gated keys","text":"available_models(distribution=:installable_gated)\ndescribe_model(:llama3_8b_tokenizer)","category":"section"},{"location":"#KeemenaSubwords.jl","page":"Home","title":"KeemenaSubwords.jl","text":"Downstream of KeemenaPreprocessing.jl.\n\nKeemenaSubwords provides Julia-native loaders and tokenization primitives for:\n\nclassic BPE,\nbyte-level BPE,\nWordPiece,\nSentencePiece,\ntiktoken,\nHugging Face tokenizer.json.","category":"section"},{"location":"#Start-Here","page":"Home","title":"Start Here","text":"If you are new to the package, start with Concepts for the core contracts and first-hour workflows.\n\nToken ids are 1-based in KeemenaSubwords.\nOffsets are UTF-8 codeunit half-open spans: [start, stop).\nByte-level tokenizers can emit offsets that are valid codeunit spans but not always safe Julia string slice boundaries on multibyte text.","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\npieces = tokenize(tok, \"hello world\")\nids = encode(tok, \"hello world\"; add_special_tokens=true)\ntext = decode(tok, ids)","category":"section"},{"location":"#Model-Discovery","page":"Home","title":"Model Discovery","text":"available_models()\navailable_models(distribution=:artifact_public)\navailable_models(distribution=:installable_gated)\ndescribe_model(:qwen2_5_bpe)\nrecommended_defaults_for_llms()","category":"section"},{"location":"#Key-Workflows","page":"Home","title":"Key Workflows","text":"# local path auto-detection\nload_tokenizer(\"/path/to/model_dir\")\n\n# explicit loaders\nload_bpe_gpt2(\"/path/to/vocab.json\", \"/path/to/merges.txt\")\nload_sentencepiece(\"/path/to/tokenizer.model\")\nload_tiktoken(\"/path/to/tokenizer.model\")\n\n# gated install flow\ninstall_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])","category":"section"},{"location":"#Documentation-Map","page":"Home","title":"Documentation Map","text":"Concepts\nStructured outputs and batching\nBuilt-in model inventory\nNormalization and offsets contract\nOffsets alignment worked examples\nTraining (experimental)\nFormat contracts\nLocal path recipes\nLLM cookbook and tokenizer.json roadmap\nInstallable gated models\nTroubleshooting\nAPI reference","category":"section"},{"location":"#KeemenaPreprocessing-Integration","page":"Home","title":"KeemenaPreprocessing Integration","text":"using KeemenaPreprocessing\nusing KeemenaSubwords\n\ntok = load_tokenizer(:core_bpe_en)\ncfg = PreprocessConfiguration(tokenizer_name = keemena_callable(tok))\nbundle = preprocess_corpus([\"hello world\"]; config=cfg)\n\nSee API reference for explicit loader APIs and the full exported reference.","category":"section"},{"location":"llm_cookbook/#LLM-Cookbook","page":"LLM Cookbook","title":"LLM Cookbook","text":"","category":"section"},{"location":"llm_cookbook/#OpenAI-tiktoken-encodings","page":"LLM Cookbook","title":"OpenAI tiktoken encodings","text":"prefetch_models([:tiktoken_cl100k_base, :tiktoken_o200k_base])\ntt = load_tokenizer(:tiktoken_cl100k_base)\nencode(tt, \"hello world\")","category":"section"},{"location":"llm_cookbook/#Mistral-SentencePiece","page":"LLM Cookbook","title":"Mistral SentencePiece","text":"prefetch_models([:mistral_v3_sentencepiece])\nmistral = load_tokenizer(:mistral_v3_sentencepiece)","category":"section"},{"location":"llm_cookbook/#Qwen-tokenizer.json-first-loading","page":"LLM Cookbook","title":"Qwen tokenizer.json-first loading","text":"prefetch_models([:qwen2_5_bpe])\nqwen = load_tokenizer(:qwen2_5_bpe)","category":"section"},{"location":"llm_cookbook/#LLaMA-workflow-A:-install-(gated)","page":"LLM Cookbook","title":"LLaMA workflow A: install (gated)","text":"install_model!(:llama3_8b_tokenizer; token=ENV[\"HF_TOKEN\"])\nllama = load_tokenizer(:llama3_8b_tokenizer)","category":"section"},{"location":"llm_cookbook/#LLaMA-workflow-B:-manual-local-path","page":"LLM Cookbook","title":"LLaMA workflow B: manual local path","text":"# SentencePiece-style\nllama2 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:sentencepiece_model)\n\n# LLaMA3-style tokenizer.model with tiktoken text\nllama3 = load_tokenizer(\"/path/to/tokenizer.model\"; format=:tiktoken)","category":"section"},{"location":"llm_cookbook/#Pick-practical-defaults","page":"LLM Cookbook","title":"Pick practical defaults","text":"for key in recommended_defaults_for_llms()\n    println(key)\nend","category":"section"},{"location":"llm_cookbook/#Tokenizer.json-roadmap-(no-Python-needed)","page":"LLM Cookbook","title":"Tokenizer.json roadmap (no Python needed)","text":"Near-term roadmap items (in scope for this package):\n\nexpand Hugging Face component coverage incrementally (normalizers, pre-tokenizers, post-processors, decoders),\nadd optional richer encode outputs (offsets, attention_mask, token_type_ids) in a structured return type,\nadd a small number of additional curated flagship tokenizers where redistribution/license terms are clear,\ncontinue performance hardening for BPE merge caching, WordPiece trie lookup, Unigram DP, and added-token matching.","category":"section"}]
}
