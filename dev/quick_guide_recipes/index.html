<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quick Guide Recipes · KeemenaSubwords.jl</title><meta name="title" content="Quick Guide Recipes · KeemenaSubwords.jl"/><meta property="og:title" content="Quick Guide Recipes · KeemenaSubwords.jl"/><meta property="twitter:title" content="Quick Guide Recipes · KeemenaSubwords.jl"/><meta name="description" content="Documentation for KeemenaSubwords.jl."/><meta property="og:description" content="Documentation for KeemenaSubwords.jl."/><meta property="twitter:description" content="Documentation for KeemenaSubwords.jl."/><meta property="og:url" content="https://mantzaris.github.io/KeemenaSubwords.jl/quick_guide_recipes/"/><meta property="twitter:url" content="https://mantzaris.github.io/KeemenaSubwords.jl/quick_guide_recipes/"/><link rel="canonical" href="https://mantzaris.github.io/KeemenaSubwords.jl/quick_guide_recipes/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">KeemenaSubwords.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../concepts/">Concepts</a></li><li class="is-active"><a class="tocitem" href>Quick Guide Recipes</a><ul class="internal"><li><a class="tocitem" href="#Pretrained-tokenizer-recipes-(common)"><span>Pretrained tokenizer recipes (common)</span></a></li><li><a class="tocitem" href="#Training-recipes-(experimental)"><span>Training recipes (experimental)</span></a></li><li><a class="tocitem" href="#Other-options-(short-list)"><span>Other options (short list)</span></a></li></ul></li><li><a class="tocitem" href="../structured_outputs_and_batching/">Structured Outputs and Batching</a></li><li><a class="tocitem" href="../integration/">Integration</a></li><li><a class="tocitem" href="../normalization_offsets_contract/">Normalization &amp; Offsets</a></li><li><a class="tocitem" href="../offset_alignment_examples/">Offsets Alignment Examples</a></li><li><a class="tocitem" href="../loading/">Loading</a></li><li><a class="tocitem" href="../training/">Training</a></li><li><a class="tocitem" href="../formats/">Formats</a></li><li><a class="tocitem" href="../loading_local/">Loading Local</a></li><li><a class="tocitem" href="../llm_cookbook/">LLM Cookbook</a></li><li><a class="tocitem" href="../gated_models/">Gated Models</a></li><li><a class="tocitem" href="../troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../models/">Built-In Models</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Quick Guide Recipes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quick Guide Recipes</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/mantzaris/KeemenaSubwords.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/main/docs/src/quick_guide_recipes.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Quick-Guide-Recipes"><a class="docs-heading-anchor" href="#Quick-Guide-Recipes">Quick Guide Recipes</a><a id="Quick-Guide-Recipes-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Guide-Recipes" title="Permalink"></a></h1><p>This page is a choose-your-path entry point for the most common KeemenaSubwords workflows. Each recipe is short and points to deeper docs when you need details.</p><p>Core invariants:</p><ul><li>Token ids in KeemenaSubwords are 1-based.</li><li>Offsets are 1-based UTF-8 codeunit half-open spans <code>[start, stop)</code>.</li><li><code>(0, 0)</code> is the no-span sentinel.</li></ul><p>Recommended pipeline contract: <code>clean_text</code> comes from preprocessing, then <code>tokenization_text = tokenization_view(tokenizer, clean_text)</code>, then <code>encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true, ...)</code>.</p><h2 id="Pretrained-tokenizer-recipes-(common)"><a class="docs-heading-anchor" href="#Pretrained-tokenizer-recipes-(common)">Pretrained tokenizer recipes (common)</a><a id="Pretrained-tokenizer-recipes-(common)-1"></a><a class="docs-heading-anchor-permalink" href="#Pretrained-tokenizer-recipes-(common)" title="Permalink"></a></h2><h3 id="P1:-Load-a-shipped-tokenizer-and-encode-or-decode"><a class="docs-heading-anchor" href="#P1:-Load-a-shipped-tokenizer-and-encode-or-decode">P1: Load a shipped tokenizer and encode or decode</a><a id="P1:-Load-a-shipped-tokenizer-and-encode-or-decode-1"></a><a class="docs-heading-anchor-permalink" href="#P1:-Load-a-shipped-tokenizer-and-encode-or-decode" title="Permalink"></a></h3><p>When to use: you want the fastest path to tokenize text with a built-in model and verify round-trip behavior.</p><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_bpe_en)
text = &quot;hello world&quot;

token_pieces = tokenize(tokenizer, text)
token_ids = encode(tokenizer, text; add_special_tokens=true)
decoded_text = decode(tokenizer, token_ids)

(token_pieces=token_pieces, token_ids=token_ids, decoded_text=decoded_text)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(token_pieces = [&quot;hello&lt;/w&gt;&quot;, &quot;world&lt;/w&gt;&quot;], token_ids = [3, 24, 29, 4], decoded_text = &quot;hello world&quot;)</code></pre><p>Go deeper: <a href="../concepts/">Concepts</a>, <a href="../loading/">Loading Tokenizers</a>.</p><h3 id="P2:-Discover-models-and-inspect-metadata"><a class="docs-heading-anchor" href="#P2:-Discover-models-and-inspect-metadata">P2: Discover models and inspect metadata</a><a id="P2:-Discover-models-and-inspect-metadata-1"></a><a class="docs-heading-anchor-permalink" href="#P2:-Discover-models-and-inspect-metadata" title="Permalink"></a></h3><p>When to use: you want to pick a model key, inspect provenance, and list practical defaults.</p><pre><code class="language-julia hljs">using KeemenaSubwords

shipped_model_keys = available_models(shipped=true)
recommended_keys = recommended_defaults_for_llms()
core_wordpiece_info = describe_model(:core_wordpiece_en)

preview_count = min(length(shipped_model_keys), 8)
shipped_preview = shipped_model_keys[1:preview_count]

(
    shipped_preview=shipped_preview,
    recommended_keys=recommended_keys,
    core_wordpiece=(
        format=core_wordpiece_info.format,
        distribution=core_wordpiece_info.distribution,
        description=core_wordpiece_info.description,
    ),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(shipped_preview = [:bert_base_multilingual_cased_wordpiece, :bert_base_uncased_wordpiece, :core_bpe_en, :core_sentencepiece_unigram_en, :core_wordpiece_en, :mistral_v1_sentencepiece, :mistral_v3_sentencepiece, :openai_gpt2_bpe], recommended_keys = [:tiktoken_cl100k_base, :tiktoken_o200k_base, :mistral_v3_sentencepiece, :phi2_bpe, :qwen2_5_bpe, :roberta_base_bpe, :xlm_roberta_base_sentencepiece_bpe], core_wordpiece = (format = :wordpiece_vocab, distribution = :shipped, description = &quot;Tiny built-in English WordPiece model.&quot;))</code></pre><p>Optional prefetch for offline-safe shipped models:</p><pre><code class="language-julia hljs">prefetch_models([:core_bpe_en, :core_wordpiece_en, :core_sentencepiece_unigram_en])</code></pre><p>Go deeper: <a href="../models/">Built-In Models</a>, <a href="../llm_cookbook/">LLM Cookbook</a>.</p><h3 id="P3:-Get-ids-plus-offsets-plus-masks-for-alignment"><a class="docs-heading-anchor" href="#P3:-Get-ids-plus-offsets-plus-masks-for-alignment">P3: Get ids plus offsets plus masks for alignment</a><a id="P3:-Get-ids-plus-offsets-plus-masks-for-alignment-1"></a><a class="docs-heading-anchor-permalink" href="#P3:-Get-ids-plus-offsets-plus-masks-for-alignment" title="Permalink"></a></h3><p>When to use: you need token ids and reliable alignment metadata in one result object.</p><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_sentencepiece_unigram_en)
clean_text = &quot;Hello, world! Offsets demo.&quot;
tokenization_text = tokenization_view(tokenizer, clean_text)

result = encode_result(
    tokenizer,
    tokenization_text;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

@assert result.offsets !== nothing
@assert result.special_tokens_mask !== nothing

preview_rows = [
    (
        token_index=i,
        token=result.tokens[i],
        offset=result.offsets[i],
        is_special=result.special_tokens_mask[i] == 1,
    )
    for i in 1:min(length(result.ids), 12)
]

(
    offsets_reference=result.metadata.offsets_reference,
    token_count=length(result.ids),
    preview_rows=preview_rows,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(offsets_reference = :input_text, token_count = 6, preview_rows = @NamedTuple{token_index::Int64, token::String, offset::Tuple{Int64, Int64}, is_special::Bool}[(token_index = 1, token = &quot;&lt;s&gt;&quot;, offset = (0, 0), is_special = 1), (token_index = 2, token = &quot;&lt;unk&gt;&quot;, offset = (1, 7), is_special = 1), (token_index = 3, token = &quot;&lt;unk&gt;&quot;, offset = (8, 14), is_special = 1), (token_index = 4, token = &quot;&lt;unk&gt;&quot;, offset = (15, 22), is_special = 1), (token_index = 5, token = &quot;&lt;unk&gt;&quot;, offset = (23, 28), is_special = 1), (token_index = 6, token = &quot;&lt;/s&gt;&quot;, offset = (0, 0), is_special = 1)])</code></pre><p>Go deeper: <a href="../normalization_offsets_contract/">Normalization and Offsets Contract</a>, <a href="../offset_alignment_examples/">Offsets Alignment Examples</a>, <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a>.</p><h3 id="P4:-Span-text-extraction-from-offsets-(safe-slicing)"><a class="docs-heading-anchor" href="#P4:-Span-text-extraction-from-offsets-(safe-slicing)">P4: Span text extraction from offsets (safe slicing)</a><a id="P4:-Span-text-extraction-from-offsets-(safe-slicing)-1"></a><a class="docs-heading-anchor-permalink" href="#P4:-Span-text-extraction-from-offsets-(safe-slicing)" title="Permalink"></a></h3><p>When to use: you want substring views for token spans without assuming all offsets are safe string boundaries.</p><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_sentencepiece_unigram_en)
tokenization_text = tokenization_view(tokenizer, &quot;Hello, world! Offsets demo.&quot;)
result = encode_result(
    tokenizer,
    tokenization_text;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

@assert result.offsets !== nothing

span_preview = [
    (
        token_index=i,
        token=result.tokens[i],
        offset=result.offsets[i],
        span_text=try_span_substring(tokenization_text, result.offsets[i]),
    )
    for i in 1:min(length(result.ids), 12)
]

span_preview</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{@NamedTuple{token_index::Int64, token::String, offset::Tuple{Int64, Int64}, span_text::String}}:
 (token_index = 1, token = &quot;&lt;s&gt;&quot;, offset = (0, 0), span_text = &quot;&quot;)
 (token_index = 2, token = &quot;&lt;unk&gt;&quot;, offset = (1, 7), span_text = &quot;Hello,&quot;)
 (token_index = 3, token = &quot;&lt;unk&gt;&quot;, offset = (8, 14), span_text = &quot;world!&quot;)
 (token_index = 4, token = &quot;&lt;unk&gt;&quot;, offset = (15, 22), span_text = &quot;Offsets&quot;)
 (token_index = 5, token = &quot;&lt;unk&gt;&quot;, offset = (23, 28), span_text = &quot;demo.&quot;)
 (token_index = 6, token = &quot;&lt;/s&gt;&quot;, offset = (0, 0), span_text = &quot;&quot;)</code></pre><p>Byte-level caveat: some byte-level tokenizers can emit non-boundary spans on multibyte text. Use <code>try_span_substring</code> first, then <code>span_codeunits</code> fallback when needed.</p><p>Go deeper: <a href="../offset_alignment_examples/">Offsets Alignment Examples</a>, <a href="../normalization_offsets_contract/">Normalization and Offsets Contract</a>.</p><h3 id="P5:-Batch-encode-multiple-sequences-(no-padding-yet)"><a class="docs-heading-anchor" href="#P5:-Batch-encode-multiple-sequences-(no-padding-yet)">P5: Batch encode multiple sequences (no padding yet)</a><a id="P5:-Batch-encode-multiple-sequences-(no-padding-yet)-1"></a><a class="docs-heading-anchor-permalink" href="#P5:-Batch-encode-multiple-sequences-(no-padding-yet)" title="Permalink"></a></h3><p>When to use: you want per-sequence structured outputs before collation.</p><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_wordpiece_en)
clean_texts = [&quot;hello world&quot;, &quot;hello&quot;, &quot;world hello world&quot;]
tokenization_texts = [tokenization_view(tokenizer, clean_text) for clean_text in clean_texts]

batch_results = encode_batch_result(
    tokenizer,
    tokenization_texts;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

sequence_lengths = [length(result.ids) for result in batch_results]

(
    sequence_lengths=sequence_lengths,
    has_variable_lengths=length(unique(sequence_lengths)) &gt; 1,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(sequence_lengths = [4, 3, 5], has_variable_lengths = true)</code></pre><p>Go deeper: <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a>.</p><h3 id="P6:-Padding-plus-labels-for-training-(pointer-recipe)"><a class="docs-heading-anchor" href="#P6:-Padding-plus-labels-for-training-(pointer-recipe)">P6: Padding plus labels for training (pointer recipe)</a><a id="P6:-Padding-plus-labels-for-training-(pointer-recipe)-1"></a><a class="docs-heading-anchor-permalink" href="#P6:-Padding-plus-labels-for-training-(pointer-recipe)" title="Permalink"></a></h3><p>When to use: you want minimal tensors for causal LM training with <code>ignore_index=-100</code> behavior.</p><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_wordpiece_en)
clean_texts = [&quot;hello world&quot;, &quot;hello&quot;]
tokenization_texts = [tokenization_view(tokenizer, clean_text) for clean_text in clean_texts]
batch_results = encode_batch_result(
    tokenizer,
    tokenization_texts;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=false,
    return_masks=true,
)

function tiny_pad_batch(results::Vector{TokenizationResult}; pad_token_id::Int)
    batch_size = length(results)
    max_length = maximum(length(result.ids) for result in results)
    ids = fill(pad_token_id, max_length, batch_size)
    attention_mask = fill(0, max_length, batch_size)
    for (column_index, result) in pairs(results)
        sequence_length = length(result.ids)
        ids[1:sequence_length, column_index] = result.ids
        attention_mask[1:sequence_length, column_index] .= 1
    end
    return (ids=ids, attention_mask=attention_mask)
end

function tiny_causal_labels(ids::Matrix{Int}, attention_mask::Matrix{Int}; ignore_index::Int=-100)
    labels = fill(ignore_index, size(ids))
    for column_index in axes(ids, 2)
        valid_positions = findall(attention_mask[:, column_index] .== 1)
        for i in 1:(length(valid_positions) - 1)
            labels[valid_positions[i], column_index] = ids[valid_positions[i + 1], column_index]
        end
    end
    return labels
end

collated = tiny_pad_batch(batch_results; pad_token_id=pad_id(tokenizer))
labels = tiny_causal_labels(collated.ids, collated.attention_mask; ignore_index=-100)
labels_zero_based = map(label -&gt; label == -100 ? -100 : label - 1, labels)

(
    ids_size=size(collated.ids),
    labels_size=size(labels),
    ignore_index_count=count(==(-100), labels),
    labels_zero_based=labels_zero_based,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(ids_size = (4, 2), labels_size = (4, 2), ignore_index_count = 3, labels_zero_based = [5 5; 6 3; 3 -100; -100 -100])</code></pre><p>Go deeper: <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a> for full padding, causal labels, and block packing recipes.</p><h3 id="P7:-Export-to-Hugging-Face-tokenizer.json-for-Python"><a class="docs-heading-anchor" href="#P7:-Export-to-Hugging-Face-tokenizer.json-for-Python">P7: Export to Hugging Face tokenizer.json for Python</a><a id="P7:-Export-to-Hugging-Face-tokenizer.json-for-Python-1"></a><a class="docs-heading-anchor-permalink" href="#P7:-Export-to-Hugging-Face-tokenizer.json-for-Python" title="Permalink"></a></h3><p>When to use: you need interop with Python fast tokenizers.</p><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_wordpiece_en)
output_directory = mktempdir()
export_tokenizer(tokenizer, output_directory; format=:hf_tokenizer_json)

isfile(joinpath(output_directory, &quot;tokenizer.json&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>Python usage (non-executable in Documenter):</p><pre><code class="language-python hljs">from transformers import PreTrainedTokenizerFast
tokenizer = PreTrainedTokenizerFast(tokenizer_file=&quot;out_tokenizer/tokenizer.json&quot;)</code></pre><p>Go deeper: <a href="../formats/">Tokenizer Formats and Required Files</a>, <a href="../llm_cookbook/">LLM Cookbook</a>.</p><h3 id="P8:-Load-from-a-local-path-(auto-detect-plus-override)"><a class="docs-heading-anchor" href="#P8:-Load-from-a-local-path-(auto-detect-plus-override)">P8: Load from a local path (auto-detect plus override)</a><a id="P8:-Load-from-a-local-path-(auto-detect-plus-override)-1"></a><a class="docs-heading-anchor-permalink" href="#P8:-Load-from-a-local-path-(auto-detect-plus-override)" title="Permalink"></a></h3><p>When to use: model files already exist locally and you want either detection or explicit format control.</p><pre><code class="language-julia hljs"># non-executable path placeholders
tokenizer_auto = load_tokenizer(&quot;/path/to/model_dir&quot;)
tokenizer_tiktoken = load_tokenizer(&quot;/path/to/tokenizer.model&quot;; format=:tiktoken)
tokenizer_sentencepiece = load_tokenizer(&quot;/path/to/tokenizer.model&quot;; format=:sentencepiece_model)</code></pre><p>Go deeper: <a href="../loading_local/">Loading Tokenizers From Local Paths</a>, <a href="../formats/">Tokenizer Formats and Required Files</a>.</p><h3 id="P9:-Install-and-load-a-gated-model"><a class="docs-heading-anchor" href="#P9:-Install-and-load-a-gated-model">P9: Install and load a gated model</a><a id="P9:-Install-and-load-a-gated-model-1"></a><a class="docs-heading-anchor-permalink" href="#P9:-Install-and-load-a-gated-model" title="Permalink"></a></h3><p>When to use: you need a gated upstream tokenizer and have access credentials.</p><pre><code class="language-julia hljs"># non-executable gated workflow
install_model!(:llama3_8b_tokenizer; token=ENV[&quot;HF_TOKEN&quot;])
tokenizer = load_tokenizer(:llama3_8b_tokenizer)</code></pre><p>You must accept upstream license terms and have valid access before install.</p><p>Go deeper: <a href="../gated_models/">Installable Gated Models</a>, <a href="../llm_cookbook/">LLM Cookbook</a>.</p><h2 id="Training-recipes-(experimental)"><a class="docs-heading-anchor" href="#Training-recipes-(experimental)">Training recipes (experimental)</a><a id="Training-recipes-(experimental)-1"></a><a class="docs-heading-anchor-permalink" href="#Training-recipes-(experimental)" title="Permalink"></a></h2><p>Training APIs are experimental and may evolve faster than pretrained loading and encoding APIs.</p><h3 id="T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode"><a class="docs-heading-anchor" href="#T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode">T1: Train a tiny WordPiece tokenizer, save, reload, and encode</a><a id="T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode-1"></a><a class="docs-heading-anchor-permalink" href="#T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode" title="Permalink"></a></h3><p>When to use: you want a self-contained training round trip without network access.</p><pre><code class="language-julia hljs">using KeemenaSubwords

training_corpus = [
    &quot;hello world&quot;,
    &quot;hello tokenizer&quot;,
    &quot;world of subwords&quot;,
]

training_result = train_wordpiece_result(
    training_corpus;
    vocab_size=64,
    min_frequency=1,
)

bundle_directory = mktempdir()
save_training_bundle(training_result, bundle_directory; overwrite=true)
reloaded_tokenizer = load_training_bundle(bundle_directory)

encoded_ids = encode(reloaded_tokenizer, &quot;hello world&quot;; add_special_tokens=false)
decoded_text = decode(reloaded_tokenizer, encoded_ids)
bundle_files = sort(readdir(bundle_directory))

(
    encoded_ids=encoded_ids,
    decoded_text=decoded_text,
    bundle_files=bundle_files,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(encoded_ids = [57, 56], decoded_text = &quot;hello world&quot;, bundle_files = [&quot;keemena_training_manifest.json&quot;, &quot;vocab.txt&quot;])</code></pre><h3 id="T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json"><a class="docs-heading-anchor" href="#T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json">T2: Train HF BERT WordPiece preset and export tokenizer.json</a><a id="T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json-1"></a><a class="docs-heading-anchor-permalink" href="#T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json" title="Permalink"></a></h3><p>When to use: you want a BERT-style preset with direct HF <code>tokenizer.json</code> export.</p><pre><code class="language-julia hljs"># non-executable training preset sketch
training_corpus = [&quot;Hello, world!&quot;, &quot;Tokenizer training example&quot;]
tokenizer = train_hf_bert_wordpiece(training_corpus; vocab_size=128, min_frequency=1)
export_tokenizer(tokenizer, &quot;out_hf_bert&quot;; format=:hf_tokenizer_json)</code></pre><h3 id="T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset"><a class="docs-heading-anchor" href="#T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset">T3: Train HF RoBERTa or GPT-2 ByteBPE preset</a><a id="T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset-1"></a><a class="docs-heading-anchor-permalink" href="#T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset" title="Permalink"></a></h3><p>When to use: you want byte-level preset behavior for GPT-2 or RoBERTa style pipelines.</p><pre><code class="language-julia hljs"># non-executable training preset sketch
training_corpus = [&quot;hello world&quot;, &quot;cafe costs 5&quot;]
tokenizer = train_hf_roberta_bytebpe(training_corpus; vocab_size=384, min_frequency=1)
export_tokenizer(tokenizer, &quot;out_hf_roberta&quot;; format=:hf_tokenizer_json)</code></pre><p>Byte-level reminder: offsets still follow the same contract, but span boundaries may not always be safe Julia string boundaries on multibyte text.</p><p>Go deeper:</p><ul><li><a href="../training/">Training (experimental)</a></li><li><a href="../formats/">Tokenizer Formats and Required Files</a></li><li><a href="../normalization_offsets_contract/">Normalization and Offsets Contract</a></li></ul><h2 id="Other-options-(short-list)"><a class="docs-heading-anchor" href="#Other-options-(short-list)">Other options (short list)</a><a id="Other-options-(short-list)-1"></a><a class="docs-heading-anchor-permalink" href="#Other-options-(short-list)" title="Permalink"></a></h2><ul><li>Cache tokenizers for repeated use with <code>get_tokenizer_cached(...)</code> and clear cache with <code>clear_tokenizer_cache!()</code>.</li><li>Use explicit loaders when file contracts are known, for example <code>load_bpe_gpt2</code>, <code>load_sentencepiece</code>, and <code>load_tiktoken</code>.</li><li>Convert to 0-based ids only when an external consumer requires it: <code>ids_zero_based = token_ids .- 1</code>.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../concepts/">« Concepts</a><a class="docs-footer-nextpage" href="../structured_outputs_and_batching/">Structured Outputs and Batching »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 16 February 2026 22:03">Monday 16 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
