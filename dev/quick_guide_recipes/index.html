<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quick Guide Recipes · KeemenaSubwords.jl</title><meta name="title" content="Quick Guide Recipes · KeemenaSubwords.jl"/><meta property="og:title" content="Quick Guide Recipes · KeemenaSubwords.jl"/><meta property="twitter:title" content="Quick Guide Recipes · KeemenaSubwords.jl"/><meta name="description" content="Documentation for KeemenaSubwords.jl."/><meta property="og:description" content="Documentation for KeemenaSubwords.jl."/><meta property="twitter:description" content="Documentation for KeemenaSubwords.jl."/><meta property="og:url" content="https://mantzaris.github.io/KeemenaSubwords.jl/quick_guide_recipes/"/><meta property="twitter:url" content="https://mantzaris.github.io/KeemenaSubwords.jl/quick_guide_recipes/"/><link rel="canonical" href="https://mantzaris.github.io/KeemenaSubwords.jl/quick_guide_recipes/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">KeemenaSubwords.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../concepts/">Concepts</a></li><li class="is-active"><a class="tocitem" href>Quick Guide Recipes</a><ul class="internal"><li><a class="tocitem" href="#Quick-Handlers-(1-2-line-workflows)"><span>Quick Handlers (1-2 line workflows)</span></a></li><li><a class="tocitem" href="#How-to-use-this-page"><span>How to use this page</span></a></li><li><a class="tocitem" href="#Choose-a-recipe-by-goal"><span>Choose a recipe by goal</span></a></li><li><a class="tocitem" href="#Pretrained-tokenizer-recipes-(common)"><span>Pretrained tokenizer recipes (common)</span></a></li><li><a class="tocitem" href="#Training-recipes-(experimental)"><span>Training recipes (experimental)</span></a></li><li><a class="tocitem" href="#Other-options-(short-list)"><span>Other options (short list)</span></a></li></ul></li><li><a class="tocitem" href="../structured_outputs_and_batching/">Structured Outputs and Batching</a></li><li><a class="tocitem" href="../integration/">Integration</a></li><li><a class="tocitem" href="../normalization_offsets_contract/">Normalization &amp; Offsets</a></li><li><a class="tocitem" href="../offset_alignment_examples/">Offsets Alignment Examples</a></li><li><a class="tocitem" href="../loading/">Loading</a></li><li><a class="tocitem" href="../training/">Training</a></li><li><a class="tocitem" href="../formats/">Formats</a></li><li><a class="tocitem" href="../loading_local/">Loading Local</a></li><li><a class="tocitem" href="../llm_cookbook/">LLM Cookbook</a></li><li><a class="tocitem" href="../gated_models/">Gated Models</a></li><li><a class="tocitem" href="../troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../models/">Built-In Models</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Quick Guide Recipes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quick Guide Recipes</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/mantzaris/KeemenaSubwords.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/mantzaris/KeemenaSubwords.jl/blob/main/docs/src/quick_guide_recipes.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Quick-Guide-Recipes"><a class="docs-heading-anchor" href="#Quick-Guide-Recipes">Quick Guide Recipes</a><a id="Quick-Guide-Recipes-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Guide-Recipes" title="Permalink"></a></h1><p>This page is a choose-your-path entry point for the most common KeemenaSubwords workflows. Each recipe is written as a guided walkthrough and links to deeper docs when you need detail.</p><p>Core invariants:</p><ul><li>Token ids in KeemenaSubwords are 1-based.</li><li>Offsets are 1-based UTF-8 codeunit half-open spans <code>[start, stop)</code>.</li><li><code>(0, 0)</code> is the no-span sentinel.</li></ul><p>Recommended pipeline contract:</p><p><code>clean_text</code> comes from preprocessing, then <code>tokenization_text = tokenization_view(tokenizer, clean_text)</code>, then <code>encode_result(tokenizer, tokenization_text; assume_normalized=true, return_offsets=true, return_masks=true, ...)</code>.</p><h2 id="Quick-Handlers-(1-2-line-workflows)"><a class="docs-heading-anchor" href="#Quick-Handlers-(1-2-line-workflows)">Quick Handlers (1-2 line workflows)</a><a id="Quick-Handlers-(1-2-line-workflows)-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Handlers-(1-2-line-workflows)" title="Permalink"></a></h2><div class="admonition is-success" id="In-a-hurry?-Start-here-fdb0cd9b8002f3a"><header class="admonition-header">In a hurry? Start here<a class="admonition-anchor" href="#In-a-hurry?-Start-here-fdb0cd9b8002f3a" title="Permalink"></a></header><div class="admonition-body"><p>Pick one quick handler and run it. You do not need all of them. Each handler returns extra outputs on purpose, so you can ignore fields you do not need now. If you want to customize behavior, the detailed recipes below walk through each step.</p></div></div><p>Pick one:</p><ul><li>One string -&gt; token ids (numbers) and token pieces (readable chunks) with <code>quick_tokenize</code></li><li>Many strings -&gt; per-string results (no padding) with <code>quick_encode_batch</code></li><li>Many strings -&gt; padded training tensors and causal labels with <code>quick_causal_lm_batch</code></li><li>Train -&gt; save bundle -&gt; reload -&gt; sanity check with <code>quick_train_bundle</code></li></ul><h3 id="One-string-to-tokens-with-quick_tokenize"><a class="docs-heading-anchor" href="#One-string-to-tokens-with-quick_tokenize">One string to tokens with <code>quick_tokenize</code></a><a id="One-string-to-tokens-with-quick_tokenize-1"></a><a class="docs-heading-anchor-permalink" href="#One-string-to-tokens-with-quick_tokenize" title="Permalink"></a></h3><ul><li><strong>You have:</strong> one input <code>String</code>, for example <code>&quot;hello world&quot;</code>.</li><li><strong>You get:</strong> token ids (<code>Int</code> numbers), token pieces (readable chunks), and decoded text as a round-trip sanity check.</li><li><strong>Use it for:</strong> quick model-input prep, inspection, and early alignment checks.</li></ul><p>Super simple:</p><pre><code class="language-julia hljs">using KeemenaSubwords
(let out = quick_tokenize(:core_bpe_en, &quot;hello world&quot;); (pieces=out.token_pieces, ids=out.token_ids, decoded=out.decoded_text) end)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(pieces = [&quot;&lt;s&gt;&quot;, &quot;hello&lt;/w&gt;&quot;, &quot;world&lt;/w&gt;&quot;, &quot;&lt;/s&gt;&quot;], ids = [3, 24, 29, 4], decoded = &quot;hello world&quot;)</code></pre><p>Peek inside:</p><pre><code class="language-julia hljs">using KeemenaSubwords
(let out = quick_tokenize(:core_bpe_en, &quot;hello world hello&quot;); (offsets_reference=out.metadata.offsets_reference, offsets=out.offsets) end)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(offsets_reference = :input_text, offsets = [(0, 0), (1, 6), (7, 12), (13, 18), (0, 0)])</code></pre><p>Returns: <code>token_pieces</code>, <code>token_ids</code>, <code>decoded_text</code>, <code>tokenization_text</code>, <code>offsets</code>, <code>attention_mask</code>, <code>token_type_ids</code>, <code>special_tokens_mask</code>, and <code>metadata</code>. It returns offsets and masks by default, and you can ignore them unless you are doing alignment or training.</p><p>Common knobs:</p><ul><li><code>add_special_tokens</code>: include start and end tokens used by many models.</li><li><code>return_offsets</code>: include where each token came from in the string.</li><li><code>return_masks</code>: include 0/1 masks that are useful for training.</li><li><code>apply_tokenization_view</code>: run tokenizer-specific normalization view before encoding.</li></ul><p>Go deeper: <a href="#p1-load-a-shipped-tokenizer-and-encode-or-decode">P1</a>, <a href="../concepts/">Concepts</a>.</p><h3 id="Many-strings-to-per-sequence-results-with-quick_encode_batch"><a class="docs-heading-anchor" href="#Many-strings-to-per-sequence-results-with-quick_encode_batch">Many strings to per-sequence results with <code>quick_encode_batch</code></a><a id="Many-strings-to-per-sequence-results-with-quick_encode_batch-1"></a><a class="docs-heading-anchor-permalink" href="#Many-strings-to-per-sequence-results-with-quick_encode_batch" title="Permalink"></a></h3><ul><li><strong>You have:</strong> <code>Vector{String}</code>, for example <code>[&quot;hello world&quot;, &quot;hello&quot;]</code>.</li><li><strong>You get:</strong> one structured result per string, plus per-string sequence lengths.</li><li><strong>Use it for:</strong> batched preprocessing before you decide how to pad or collate.</li></ul><p>Super simple:</p><pre><code class="language-julia hljs">using KeemenaSubwords
quick_encode_batch(:core_wordpiece_en, [&quot;hello world&quot;, &quot;hello&quot;]).sequence_lengths</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Int64}:
 4
 3</code></pre><p>Peek inside:</p><pre><code class="language-julia hljs">using KeemenaSubwords
(let out = quick_encode_batch(:core_wordpiece_en, [&quot;hello world hello&quot;, &quot;hello world&quot;]); (tokens=first(out.results[1].tokens, 6), ids=first(out.results[1].ids, 6)) end)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(tokens = [&quot;[CLS]&quot;, &quot;hello&quot;, &quot;world&quot;, &quot;hello&quot;, &quot;[SEP]&quot;], ids = [3, 6, 7, 6, 4])</code></pre><p>Returns: <code>tokenization_texts</code>, <code>results</code>, and <code>sequence_lengths</code>. Full structured per-string outputs live at <code>.results</code>.</p><p>Common knobs:</p><ul><li><code>add_special_tokens</code>: include model-specific start and end tokens.</li><li><code>return_offsets</code>: keep alignment spans for each token.</li><li><code>return_masks</code>: keep 0/1 masks for each sequence.</li><li><code>apply_tokenization_view</code>: normalize each input into tokenizer coordinates first.</li></ul><p>Go deeper: <a href="#p5-batch-encode-multiple-sequences-no-padding-yet">P5</a>, <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a>.</p><h3 id="Many-strings-to-training-tensors-with-quick_causal_lm_batch"><a class="docs-heading-anchor" href="#Many-strings-to-training-tensors-with-quick_causal_lm_batch">Many strings to training tensors with <code>quick_causal_lm_batch</code></a><a id="Many-strings-to-training-tensors-with-quick_causal_lm_batch-1"></a><a class="docs-heading-anchor-permalink" href="#Many-strings-to-training-tensors-with-quick_causal_lm_batch" title="Permalink"></a></h3><ul><li><strong>You have:</strong> <code>Vector{String}</code> for a training batch.</li><li><strong>You get:</strong> padded <code>ids</code>, <code>attention_mask</code>, and <code>labels</code> matrices shaped <code>(seq_len, batch)</code>.</li><li><strong>Use it for:</strong> next-token training pipelines that need dense tensors.</li></ul><p>Super simple:</p><pre><code class="language-julia hljs">using KeemenaSubwords
(let out = quick_causal_lm_batch(:core_wordpiece_en, [&quot;hello world&quot;, &quot;hello&quot;]); (ids_size=size(out.ids), labels_size=size(out.labels), pad_token_id=out.pad_token_id) end)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(ids_size = (4, 2), labels_size = (4, 2), pad_token_id = 1)</code></pre><p>Peek inside:</p><pre><code class="language-julia hljs">using KeemenaSubwords
(let out = quick_causal_lm_batch(:core_wordpiece_en, [&quot;hello world hello&quot;, &quot;hello world&quot;]); (ids_col_1=out.ids[:, 1], labels_col_1=out.labels[:, 1], ignore_index=out.ignore_index) end)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(ids_col_1 = [3, 6, 7, 6, 4], labels_col_1 = [6, 7, 6, 4, -100], ignore_index = -100)</code></pre><p>Returns: <code>ids</code>, <code>attention_mask</code>, <code>labels</code>, <code>token_type_ids</code>, <code>special_tokens_mask</code>, <code>tokenization_texts</code>, <code>sequence_lengths</code>, <code>pad_token_id</code>, <code>ignore_index</code>, and <code>zero_based</code>. <code>labels</code> are the next-token targets. Padding and the last real token are set to <code>ignore_index</code>.</p><p>Common knobs:</p><ul><li><code>ignore_index</code>: value used where loss should be ignored.</li><li><code>zero_based</code>: convert labels to 0-based ids for external consumers.</li><li><code>pad_to_multiple_of</code>: round sequence length up for kernel-friendly shapes.</li><li><code>add_special_tokens</code>: include model boundary tokens before collation.</li></ul><p>Go deeper: <a href="#p6-padding-plus-labels-for-training-pointer-recipe">P6</a>, <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a>.</p><h3 id="Train-a-tokenizer-bundle-with-quick_train_bundle"><a class="docs-heading-anchor" href="#Train-a-tokenizer-bundle-with-quick_train_bundle">Train a tokenizer bundle with <code>quick_train_bundle</code></a><a id="Train-a-tokenizer-bundle-with-quick_train_bundle-1"></a><a class="docs-heading-anchor-permalink" href="#Train-a-tokenizer-bundle-with-quick_train_bundle" title="Permalink"></a></h3><ul><li><strong>You have:</strong> a small local corpus (<code>Vector{String}</code>).</li><li><strong>You get:</strong> saved bundle files and a reloadable tokenizer with sanity encode/decode outputs.</li><li><strong>Use it for:</strong> local tokenizer training and reproducible reload for experiments.</li></ul><p>Super simple:</p><pre><code class="language-julia hljs">using KeemenaSubwords
quick_train_bundle([&quot;hello world&quot;, &quot;hello tokenizer&quot;, &quot;world tokenizer&quot;]; vocab_size=48, min_frequency=1).bundle_files</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{String}:
 &quot;keemena_training_manifest.json&quot;
 &quot;vocab.txt&quot;</code></pre><p>Peek inside:</p><pre><code class="language-julia hljs">using KeemenaSubwords
(let out = quick_train_bundle([&quot;alpha beta&quot;, &quot;beta gamma&quot;, &quot;alpha gamma&quot;]; vocab_size=40, min_frequency=1); (sanity_ids=out.sanity_encoded_ids, sanity_decoded=out.sanity_decoded_text) end)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(sanity_ids = [1, 1], sanity_decoded = &quot;[UNK] [UNK]&quot;)</code></pre><p>Returns: <code>bundle_directory</code>, <code>bundle_files</code>, <code>tokenizer</code>, <code>training_summary</code>, <code>sanity_encoded_ids</code>, and <code>sanity_decoded_text</code>. This writes a small tokenizer bundle to disk so you can reload it later without remembering training settings.</p><p>Common knobs:</p><ul><li><code>vocab_size</code>: target vocabulary size.</li><li><code>min_frequency</code>: minimum token frequency to keep.</li><li><code>sanity_text</code>: string used for encode/decode sanity check.</li><li><code>overwrite</code>: allow reusing an existing bundle directory.</li></ul><p>Go deeper: <a href="../training/">Training (experimental)</a>, <a href="../formats/">Formats</a>.</p><h2 id="How-to-use-this-page"><a class="docs-heading-anchor" href="#How-to-use-this-page">How to use this page</a><a id="How-to-use-this-page-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-use-this-page" title="Permalink"></a></h2><p>Use this 3-step mental model:</p><ol><li>Choose a tokenizer source: shipped registry key, local files, or gated install.</li><li>Choose output shape: single input with <code>encode_result</code>, or many inputs with <code>encode_batch_result</code> plus collation.</li><li>Choose metadata: offsets for alignment tasks, masks for training tasks, or both.</li></ol><h2 id="Choose-a-recipe-by-goal"><a class="docs-heading-anchor" href="#Choose-a-recipe-by-goal">Choose a recipe by goal</a><a id="Choose-a-recipe-by-goal-1"></a><a class="docs-heading-anchor-permalink" href="#Choose-a-recipe-by-goal" title="Permalink"></a></h2><ul><li>I just want token ids from text -&gt; <a href="#p1-load-a-shipped-tokenizer-and-encode-or-decode">P1</a></li><li>I need offsets for alignment -&gt; <a href="#p3-get-ids-plus-offsets-plus-masks-for-alignment">P3</a></li><li>I have many texts and want a batch -&gt; <a href="#p5-batch-encode-multiple-sequences-no-padding-yet">P5</a></li><li>I need training-ready tensors and causal labels -&gt; <a href="#p6-padding-plus-labels-for-training-pointer-recipe">P6</a></li><li>I need Python interop -&gt; <a href="#p7-export-to-hugging-face-tokenizerjson-for-python">P7</a></li><li>I want to load local files -&gt; <a href="#p8-load-from-a-local-path-auto-detect-plus-override">P8</a></li><li>I need a gated tokenizer -&gt; <a href="#p9-install-and-load-a-gated-model">P9</a></li><li>I want to train a tokenizer -&gt; <a href="#t1-train-a-tiny-wordpiece-tokenizer-save-reload-and-encode">T1</a></li></ul><h2 id="Pretrained-tokenizer-recipes-(common)"><a class="docs-heading-anchor" href="#Pretrained-tokenizer-recipes-(common)">Pretrained tokenizer recipes (common)</a><a id="Pretrained-tokenizer-recipes-(common)-1"></a><a class="docs-heading-anchor-permalink" href="#Pretrained-tokenizer-recipes-(common)" title="Permalink"></a></h2><h3 id="P1:-Load-a-shipped-tokenizer-and-encode-or-decode"><a class="docs-heading-anchor" href="#P1:-Load-a-shipped-tokenizer-and-encode-or-decode">P1: Load a shipped tokenizer and encode or decode</a><a id="P1:-Load-a-shipped-tokenizer-and-encode-or-decode-1"></a><a class="docs-heading-anchor-permalink" href="#P1:-Load-a-shipped-tokenizer-and-encode-or-decode" title="Permalink"></a></h3><ul><li><strong>You have:</strong> <code>text::String</code>, for example <code>&quot;hello world&quot;</code>.</li><li><strong>You want:</strong> token pieces (<code>Vector{String}</code>), token ids (<code>Vector{Int}</code>), and a decoded string.</li><li><strong>Objective:</strong> quickly verify end-to-end tokenization behavior on a built-in model, with the option to inspect offsets and masks in the same call.</li><li><strong>Steps:</strong><ol><li>Call <code>quick_tokenize(:core_bpe_en, text)</code> for a one-call output bundle.</li><li>Read <code>token_pieces</code>, <code>token_ids</code>, and <code>decoded_text</code>.</li><li>If needed, pass options like <code>add_special_tokens=false</code> or <code>return_offsets=false</code>.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

text = &quot;hello world&quot;
quick_output = quick_tokenize(
    :core_bpe_en,
    text;
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

(
    token_pieces=quick_output.token_pieces,
    token_ids=quick_output.token_ids,
    decoded_text=quick_output.decoded_text,
    offsets_reference=quick_output.metadata.offsets_reference,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(token_pieces = [&quot;&lt;s&gt;&quot;, &quot;hello&lt;/w&gt;&quot;, &quot;world&lt;/w&gt;&quot;, &quot;&lt;/s&gt;&quot;], token_ids = [3, 24, 29, 4], decoded_text = &quot;hello world&quot;, offsets_reference = :input_text)</code></pre><ul><li><strong>What you should see:</strong><ul><li><code>token_pieces</code> is a vector of strings.</li><li><code>token_ids</code> is a vector of integers.</li><li><code>decoded_text</code> is a string and should be close to input text for covered vocabulary.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li><code>tokenize</code> returns readable pieces, <code>encode</code> returns integer ids, and <code>decode</code> maps ids back to text.</li><li><code>quick_tokenize</code> uses <code>encode_result</code> internally, so offsets and masks are available by default.</li><li><code>add_special_tokens=true</code> includes model specials (useful for model input); set <code>false</code> for raw spans.</li><li>Ids are always 1-based in this package.</li></ul></li><li><strong>Next:</strong> if you need model selection, go to <a href="#p2-discover-models-and-inspect-metadata">P2</a>. If you need offsets, go to <a href="#p3-get-ids-plus-offsets-plus-masks-for-alignment">P3</a>.</li></ul><h4 id="What-quick_tokenize-does-under-the-hood"><a class="docs-heading-anchor" href="#What-quick_tokenize-does-under-the-hood">What <code>quick_tokenize</code> does under the hood</a><a id="What-quick_tokenize-does-under-the-hood-1"></a><a class="docs-heading-anchor-permalink" href="#What-quick_tokenize-does-under-the-hood" title="Permalink"></a></h4><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_bpe_en)
text = &quot;hello world&quot;

tokenization_text = tokenization_view(tokenizer, text)
result = encode_result(
    tokenizer,
    tokenization_text;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

(
    token_pieces=result.tokens,
    token_ids=result.ids,
    decoded_text=decode(tokenizer, result.ids),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(token_pieces = [&quot;&lt;s&gt;&quot;, &quot;hello&lt;/w&gt;&quot;, &quot;world&lt;/w&gt;&quot;, &quot;&lt;/s&gt;&quot;], token_ids = [3, 24, 29, 4], decoded_text = &quot;hello world&quot;)</code></pre><h3 id="P2:-Discover-models-and-inspect-metadata"><a class="docs-heading-anchor" href="#P2:-Discover-models-and-inspect-metadata">P2: Discover models and inspect metadata</a><a id="P2:-Discover-models-and-inspect-metadata-1"></a><a class="docs-heading-anchor-permalink" href="#P2:-Discover-models-and-inspect-metadata" title="Permalink"></a></h3><ul><li><strong>You have:</strong> no tokenizer picked yet.</li><li><strong>You want:</strong> a shortlist of candidates with provenance and defaults.</li><li><strong>Objective:</strong> choose a safe model key and understand where it comes from.</li><li><strong>Steps:</strong><ol><li>Call <code>available_models(shipped=true)</code> for built-in keys.</li><li>Call <code>recommended_defaults_for_llms()</code> for practical default candidates.</li><li>Call <code>describe_model(key)</code> for provenance, distribution, and file expectations.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

shipped_model_keys = available_models(shipped=true)
recommended_keys = recommended_defaults_for_llms()
core_wordpiece_info = describe_model(:core_wordpiece_en)

preview_count = min(length(shipped_model_keys), 8)
shipped_preview = shipped_model_keys[1:preview_count]

(
    shipped_preview=shipped_preview,
    recommended_keys=recommended_keys,
    core_wordpiece=(
        format=core_wordpiece_info.format,
        distribution=core_wordpiece_info.distribution,
        description=core_wordpiece_info.description,
    ),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(shipped_preview = [:bert_base_multilingual_cased_wordpiece, :bert_base_uncased_wordpiece, :core_bpe_en, :core_sentencepiece_unigram_en, :core_wordpiece_en, :mistral_v1_sentencepiece, :mistral_v3_sentencepiece, :openai_gpt2_bpe], recommended_keys = [:tiktoken_cl100k_base, :tiktoken_o200k_base, :mistral_v3_sentencepiece, :phi2_bpe, :qwen2_5_bpe, :roberta_base_bpe, :xlm_roberta_base_sentencepiece_bpe], core_wordpiece = (format = :wordpiece_vocab, distribution = :shipped, description = &quot;Tiny built-in English WordPiece model.&quot;))</code></pre><ul><li><strong>What you should see:</strong><ul><li><code>shipped_preview</code> contains local-ready model keys.</li><li><code>recommended_keys</code> contains practical LLM defaults.</li><li><code>describe_model</code> returns structured metadata (format, distribution, description, upstream info).</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li><code>shipped</code> means tiny models included in the repository.</li><li><code>artifact_public</code> means downloadable public artifacts.</li><li><code>installable_gated</code> means install flow requires credentials and license acceptance.</li><li>This recipe is offline-safe because it does not download.</li></ul></li><li><strong>Next:</strong> model inventory details are in <a href="../models/">Built-In Models</a>. For install flows, go to <a href="#p9-install-and-load-a-gated-model">P9</a>.</li></ul><h3 id="P3:-Get-ids-plus-offsets-plus-masks-for-alignment"><a class="docs-heading-anchor" href="#P3:-Get-ids-plus-offsets-plus-masks-for-alignment">P3: Get ids plus offsets plus masks for alignment</a><a id="P3:-Get-ids-plus-offsets-plus-masks-for-alignment-1"></a><a class="docs-heading-anchor-permalink" href="#P3:-Get-ids-plus-offsets-plus-masks-for-alignment" title="Permalink"></a></h3><ul><li><strong>You have:</strong> <code>clean_text::String</code> from preprocessing.</li><li><strong>You want:</strong> a <code>TokenizationResult</code> with ids, offsets, and masks.</li><li><strong>Objective:</strong> get alignment-ready metadata in one call.</li><li><strong>Steps:</strong><ol><li>Call <code>load_tokenizer(...)</code>.</li><li>Call <code>tokenization_view(tokenizer, clean_text)</code> to get tokenizer-coordinate text.</li><li>Call <code>encode_result(...; assume_normalized=true, return_offsets=true, return_masks=true)</code>.</li><li>Read <code>result.metadata.offsets_reference</code> to confirm what string offsets are relative to.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_sentencepiece_unigram_en)
clean_text = &quot;Hello, world! Offsets demo.&quot;
tokenization_text = tokenization_view(tokenizer, clean_text)

result = encode_result(
    tokenizer,
    tokenization_text;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

@assert result.offsets !== nothing
@assert result.special_tokens_mask !== nothing

preview_rows = [
    (
        token_index=i,
        token=result.tokens[i],
        offset=result.offsets[i],
        is_special=result.special_tokens_mask[i] == 1,
    )
    for i in 1:min(length(result.ids), 12)
]

(
    offsets_reference=result.metadata.offsets_reference,
    token_count=length(result.ids),
    preview_rows=preview_rows,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(offsets_reference = :input_text, token_count = 6, preview_rows = @NamedTuple{token_index::Int64, token::String, offset::Tuple{Int64, Int64}, is_special::Bool}[(token_index = 1, token = &quot;&lt;s&gt;&quot;, offset = (0, 0), is_special = 1), (token_index = 2, token = &quot;&lt;unk&gt;&quot;, offset = (1, 7), is_special = 1), (token_index = 3, token = &quot;&lt;unk&gt;&quot;, offset = (8, 14), is_special = 1), (token_index = 4, token = &quot;&lt;unk&gt;&quot;, offset = (15, 22), is_special = 1), (token_index = 5, token = &quot;&lt;unk&gt;&quot;, offset = (23, 28), is_special = 1), (token_index = 6, token = &quot;&lt;/s&gt;&quot;, offset = (0, 0), is_special = 1)])</code></pre><ul><li><strong>What you should see:</strong><ul><li><code>result.offsets</code> and <code>result.special_tokens_mask</code> are present.</li><li><code>offsets_reference</code> is <code>:input_text</code> because <code>assume_normalized=true</code> and you passed <code>tokenization_text</code>.</li><li>Preview rows include offsets and special-token flags per token.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>Use <code>assume_normalized=true</code> only when the input text is already <code>tokenization_view(...)</code> output.</li><li>Offsets are relative to whatever <code>offsets_reference</code> says.</li><li>Inserted specials often have <code>(0, 0)</code> sentinel offsets.</li></ul></li><li><strong>Next:</strong> for offset semantics go to <a href="../normalization_offsets_contract/">Normalization and Offsets Contract</a>. For alignment algorithms go to <a href="../offset_alignment_examples/">Offsets Alignment Examples</a>.</li></ul><h3 id="P4:-Span-text-extraction-from-offsets-(safe-slicing)"><a class="docs-heading-anchor" href="#P4:-Span-text-extraction-from-offsets-(safe-slicing)">P4: Span text extraction from offsets (safe slicing)</a><a id="P4:-Span-text-extraction-from-offsets-(safe-slicing)-1"></a><a class="docs-heading-anchor-permalink" href="#P4:-Span-text-extraction-from-offsets-(safe-slicing)" title="Permalink"></a></h3><ul><li><strong>You have:</strong> token offsets from <code>TokenizationResult</code>.</li><li><strong>You want:</strong> readable text snippets per token span.</li><li><strong>Objective:</strong> debug and inspect token alignment quickly.</li><li><strong>Steps:</strong><ol><li>Build <code>tokenization_text</code> with <code>tokenization_view</code>.</li><li>Produce offsets with <code>encode_result(...; return_offsets=true)</code>.</li><li>Call <code>try_span_substring(tokenization_text, offset)</code> for each offset.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_sentencepiece_unigram_en)
tokenization_text = tokenization_view(tokenizer, &quot;Hello, world! Offsets demo.&quot;)
result = encode_result(
    tokenizer,
    tokenization_text;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

@assert result.offsets !== nothing

span_preview = [
    (
        token_index=i,
        token=result.tokens[i],
        offset=result.offsets[i],
        span_text=try_span_substring(tokenization_text, result.offsets[i]),
    )
    for i in 1:min(length(result.ids), 12)
]

span_preview</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{@NamedTuple{token_index::Int64, token::String, offset::Tuple{Int64, Int64}, span_text::String}}:
 (token_index = 1, token = &quot;&lt;s&gt;&quot;, offset = (0, 0), span_text = &quot;&quot;)
 (token_index = 2, token = &quot;&lt;unk&gt;&quot;, offset = (1, 7), span_text = &quot;Hello,&quot;)
 (token_index = 3, token = &quot;&lt;unk&gt;&quot;, offset = (8, 14), span_text = &quot;world!&quot;)
 (token_index = 4, token = &quot;&lt;unk&gt;&quot;, offset = (15, 22), span_text = &quot;Offsets&quot;)
 (token_index = 5, token = &quot;&lt;unk&gt;&quot;, offset = (23, 28), span_text = &quot;demo.&quot;)
 (token_index = 6, token = &quot;&lt;/s&gt;&quot;, offset = (0, 0), span_text = &quot;&quot;)</code></pre><ul><li><strong>What you should see:</strong><ul><li>Most spanful offsets produce <code>span_text::String</code>.</li><li>Sentinel or empty spans produce <code>&quot;&quot;</code>.</li><li>In byte-level cases, <code>try_span_substring</code> may return <code>nothing</code> for non-boundary spans.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li><code>try_span_substring</code> returns <code>nothing</code> when boundaries are not valid Julia string boundaries.</li><li>If you need bytes regardless of boundaries, use <code>span_codeunits(tokenization_text, offset)</code>.</li><li>Keep extraction text and offset coordinate text consistent (<code>tokenization_text</code>).</li></ul></li><li><strong>Next:</strong> go to <a href="../offset_alignment_examples/">Offsets Alignment Examples</a> for overlap mapping and span-label workflows.</li></ul><h3 id="P5:-Batch-encode-multiple-sequences-(no-padding-yet)"><a class="docs-heading-anchor" href="#P5:-Batch-encode-multiple-sequences-(no-padding-yet)">P5: Batch encode multiple sequences (no padding yet)</a><a id="P5:-Batch-encode-multiple-sequences-(no-padding-yet)-1"></a><a class="docs-heading-anchor-permalink" href="#P5:-Batch-encode-multiple-sequences-(no-padding-yet)" title="Permalink"></a></h3><ul><li><strong>You have:</strong> many texts (<code>Vector{String}</code>).</li><li><strong>You want:</strong> <code>Vector{TokenizationResult}</code> with one structured output per input sequence.</li><li><strong>Objective:</strong> prepare data for later collation while preserving per-sequence metadata.</li><li><strong>Steps:</strong><ol><li>Normalize each input with <code>tokenization_view</code>.</li><li>Call <code>encode_batch_result(...)</code> with offsets and masks enabled.</li><li>Inspect per-sequence lengths before padding.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_wordpiece_en)
clean_texts = [&quot;hello world&quot;, &quot;hello&quot;, &quot;world hello world&quot;]
tokenization_texts = [tokenization_view(tokenizer, clean_text) for clean_text in clean_texts]

batch_results = encode_batch_result(
    tokenizer,
    tokenization_texts;
    assume_normalized=true,
    add_special_tokens=true,
    return_offsets=true,
    return_masks=true,
)

sequence_lengths = [length(result.ids) for result in batch_results]

(
    sequence_lengths=sequence_lengths,
    has_variable_lengths=length(unique(sequence_lengths)) &gt; 1,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(sequence_lengths = [4, 3, 5], has_variable_lengths = true)</code></pre><ul><li><strong>What you should see:</strong><ul><li><code>batch_results</code> is a vector, not a matrix.</li><li>Sequence lengths can differ.</li><li>Each element still has its own ids, masks, and optional offsets.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>No padding is applied automatically.</li><li>This is intentional: you can choose task-specific collation later.</li></ul></li><li><strong>Next:</strong> for padding and training tensors, go to <a href="#p6-padding-plus-labels-for-training-pointer-recipe">P6</a> and <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a>.</li></ul><h3 id="P6:-Padding-plus-labels-for-training-(pointer-recipe)"><a class="docs-heading-anchor" href="#P6:-Padding-plus-labels-for-training-(pointer-recipe)">P6: Padding plus labels for training (pointer recipe)</a><a id="P6:-Padding-plus-labels-for-training-(pointer-recipe)-1"></a><a class="docs-heading-anchor-permalink" href="#P6:-Padding-plus-labels-for-training-(pointer-recipe)" title="Permalink"></a></h3><ul><li><strong>You have:</strong> many input texts (<code>Vector{String}</code>), or precomputed <code>Vector{TokenizationResult}</code> if you split steps yourself.</li><li><strong>You want:</strong> padded <code>(seq_len, batch)</code> matrices and causal LM labels.</li><li><strong>Objective:</strong> build training-ready tensors with explicit padding and masking behavior.</li><li><strong>Steps:</strong><ol><li>Call <code>quick_causal_lm_batch(...)</code> to run encode, collate, and label-shift in one call.</li><li>Read <code>ids</code>, <code>attention_mask</code>, and <code>labels</code>.</li><li>Use <code>zero_based=true</code> only for external consumers that expect 0-based labels.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

training_batch = quick_causal_lm_batch(
    :core_wordpiece_en,
    [&quot;hello world&quot;, &quot;hello&quot;];
    add_special_tokens=true,
    return_offsets=false,
    ignore_index=-100,
    zero_based=false,
)

@assert size(training_batch.ids) == size(training_batch.attention_mask)
@assert size(training_batch.labels) == size(training_batch.ids)

(
    ids_size=size(training_batch.ids),
    labels_size=size(training_batch.labels),
    ignore_index_count=count(==(-100), training_batch.labels),
    pad_token_id=training_batch.pad_token_id,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(ids_size = (4, 2), labels_size = (4, 2), ignore_index_count = 3, pad_token_id = 1)</code></pre><ul><li><strong>What you should see:</strong><ul><li><code>ids</code>, <code>attention_mask</code>, and <code>labels</code> all share the same matrix shape.</li><li><code>ignore_index_count</code> is positive (padding and final-token masking).</li><li><code>pad_token_id</code> is inferred from tokenizer <code>pad_id</code> or <code>eos_id</code>.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>KeemenaSubwords ids are 1-based.</li><li><code>ignore_index=-100</code> is the common causal LM training convention.</li><li>Final valid token in each sequence should remain ignored.</li></ul></li><li><strong>Next:</strong> go to <a href="../structured_outputs_and_batching/">Structured Outputs and Batching</a> for fuller collation, causal labels, and block packing.</li></ul><h4 id="What-quick_causal_lm_batch-does-under-the-hood"><a class="docs-heading-anchor" href="#What-quick_causal_lm_batch-does-under-the-hood">What <code>quick_causal_lm_batch</code> does under the hood</a><a id="What-quick_causal_lm_batch-does-under-the-hood-1"></a><a class="docs-heading-anchor-permalink" href="#What-quick_causal_lm_batch-does-under-the-hood" title="Permalink"></a></h4><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_wordpiece_en)
input_texts = [&quot;hello world&quot;, &quot;hello&quot;]

batch_encoding = quick_encode_batch(
    tokenizer,
    input_texts;
    add_special_tokens=true,
    return_offsets=false,
    return_masks=true,
)

collated = collate_padded_batch(batch_encoding.results; tokenizer=tokenizer)
labels = causal_lm_labels(collated.ids, collated.attention_mask; ignore_index=-100, zero_based=false)

(
    sequence_lengths=batch_encoding.sequence_lengths,
    ids_size=size(collated.ids),
    labels_size=size(labels),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(sequence_lengths = [4, 3], ids_size = (4, 2), labels_size = (4, 2))</code></pre><h3 id="P7:-Export-to-Hugging-Face-tokenizer.json-for-Python"><a class="docs-heading-anchor" href="#P7:-Export-to-Hugging-Face-tokenizer.json-for-Python">P7: Export to Hugging Face tokenizer.json for Python</a><a id="P7:-Export-to-Hugging-Face-tokenizer.json-for-Python-1"></a><a class="docs-heading-anchor-permalink" href="#P7:-Export-to-Hugging-Face-tokenizer.json-for-Python" title="Permalink"></a></h3><ul><li><strong>You have:</strong> a tokenizer loaded in Julia.</li><li><strong>You want:</strong> a <code>tokenizer.json</code> file that Python can load.</li><li><strong>Objective:</strong> share identical tokenization rules across Julia and Python.</li><li><strong>Steps:</strong><ol><li>Load or train a tokenizer in Julia.</li><li>Call <code>export_tokenizer(...; format=:hf_tokenizer_json)</code>.</li><li>Load the emitted <code>tokenizer.json</code> in Python using <code>PreTrainedTokenizerFast</code>.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

tokenizer = load_tokenizer(:core_wordpiece_en)
output_directory = mktempdir()
export_tokenizer(tokenizer, output_directory; format=:hf_tokenizer_json)

isfile(joinpath(output_directory, &quot;tokenizer.json&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>Python usage (non-executable in Documenter):</p><pre><code class="language-python hljs">from transformers import PreTrainedTokenizerFast
tokenizer = PreTrainedTokenizerFast(tokenizer_file=&quot;out_tokenizer/tokenizer.json&quot;)</code></pre><ul><li><strong>What you should see:</strong><ul><li>Julia writes <code>tokenizer.json</code> to the output directory.</li><li>Python loads that same file with <code>PreTrainedTokenizerFast</code>.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>Exported file captures tokenizer pipeline behavior supported by the package.</li><li>Keep format contracts in mind when sharing across runtimes.</li></ul></li><li><strong>Next:</strong> details are in <a href="../formats/">Tokenizer Formats and Required Files</a> and <a href="../llm_cookbook/">LLM Cookbook</a>.</li></ul><h3 id="P8:-Load-from-a-local-path-(auto-detect-plus-override)"><a class="docs-heading-anchor" href="#P8:-Load-from-a-local-path-(auto-detect-plus-override)">P8: Load from a local path (auto-detect plus override)</a><a id="P8:-Load-from-a-local-path-(auto-detect-plus-override)-1"></a><a class="docs-heading-anchor-permalink" href="#P8:-Load-from-a-local-path-(auto-detect-plus-override)" title="Permalink"></a></h3><ul><li><strong>You have:</strong> local tokenizer files on disk.</li><li><strong>You want:</strong> a loaded tokenizer without guessing format details manually.</li><li><strong>Objective:</strong> use auto-detection when it works, and explicit overrides when needed.</li><li><strong>Steps:</strong><ol><li>Try <code>load_tokenizer(&quot;/path/to/model_dir&quot;)</code> first.</li><li>If format is ambiguous, set <code>format=:...</code> explicitly.</li><li>Validate with a tiny <code>encode</code> or <code>tokenize</code> call.</li></ol></li></ul><p>Decision tree:</p><ul><li>If auto-detect works, use <code>load_tokenizer(path)</code>.</li><li>If ambiguous or incorrect, use <code>load_tokenizer(path; format=:...)</code>.</li></ul><pre><code class="language-julia hljs"># non-executable path placeholders
tokenizer_auto = load_tokenizer(&quot;/path/to/model_dir&quot;)
tokenizer_tiktoken = load_tokenizer(&quot;/path/to/tokenizer.model&quot;; format=:tiktoken)
tokenizer_sentencepiece = load_tokenizer(&quot;/path/to/tokenizer.model&quot;; format=:sentencepiece_model)</code></pre><ul><li><strong>What you should see:</strong><ul><li>Auto-detect succeeds for common directory layouts.</li><li>Explicit override resolves ambiguous <code>.model</code> cases.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li><code>format</code> selects a file contract (required filenames and parsing rules).</li><li>Placeholder paths here are non-executable in docs.</li></ul></li><li><strong>Next:</strong> go to <a href="../loading_local/">Loading Tokenizers From Local Paths</a> and <a href="../formats/">Tokenizer Formats and Required Files</a>.</li></ul><h3 id="P9:-Install-and-load-a-gated-model"><a class="docs-heading-anchor" href="#P9:-Install-and-load-a-gated-model">P9: Install and load a gated model</a><a id="P9:-Install-and-load-a-gated-model-1"></a><a class="docs-heading-anchor-permalink" href="#P9:-Install-and-load-a-gated-model" title="Permalink"></a></h3><ul><li><strong>You have:</strong> model access credentials and accepted upstream license terms.</li><li><strong>You want:</strong> installed local assets for a gated model key.</li><li><strong>Objective:</strong> run a reproducible install-then-load workflow.</li><li><strong>Steps:</strong><ol><li>Set an HF token (for example in <code>ENV[&quot;HF_TOKEN&quot;]</code>).</li><li>Call <code>install_model!(...; token=ENV[&quot;HF_TOKEN&quot;])</code>.</li><li>Call <code>load_tokenizer(:model_key)</code> after install.</li></ol></li></ul><pre><code class="language-julia hljs"># non-executable gated workflow
install_model!(:llama3_8b_tokenizer; token=ENV[&quot;HF_TOKEN&quot;])
tokenizer = load_tokenizer(:llama3_8b_tokenizer)</code></pre><ul><li><strong>What you should see:</strong><ul><li>Install step fetches and stores assets for the key.</li><li>Load step then resolves locally by model key.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>You must accept upstream license terms before access is granted.</li><li>Keep secrets in environment variables, not source files.</li></ul></li><li><strong>Next:</strong> go to <a href="../gated_models/">Installable Gated Models</a> and <a href="../llm_cookbook/">LLM Cookbook</a>.</li></ul><h2 id="Training-recipes-(experimental)"><a class="docs-heading-anchor" href="#Training-recipes-(experimental)">Training recipes (experimental)</a><a id="Training-recipes-(experimental)-1"></a><a class="docs-heading-anchor-permalink" href="#Training-recipes-(experimental)" title="Permalink"></a></h2><p>Training is usually appropriate when you need domain adaptation, research control over vocabulary behavior, or constrained deployments that need custom tokenizer assets. Training APIs are experimental and may evolve faster than pretrained loading and encoding APIs.</p><h3 id="T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode"><a class="docs-heading-anchor" href="#T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode">T1: Train a tiny WordPiece tokenizer, save, reload, and encode</a><a id="T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode-1"></a><a class="docs-heading-anchor-permalink" href="#T1:-Train-a-tiny-WordPiece-tokenizer,-save,-reload,-and-encode" title="Permalink"></a></h3><ul><li><strong>You have:</strong> a small in-memory corpus (<code>Vector{String}</code>).</li><li><strong>You want:</strong> a trained tokenizer bundle you can reload reproducibly.</li><li><strong>Objective:</strong> run a fully local training round trip without network access.</li><li><strong>Steps:</strong><ol><li>Call <code>quick_train_bundle(:wordpiece, corpus; ...)</code>.</li><li>Read <code>bundle_files</code> and <code>tokenizer</code>.</li><li>Confirm sanity ids and decoded text from the returned fields.</li></ol></li></ul><pre><code class="language-julia hljs">using KeemenaSubwords

training_corpus = [
    &quot;hello world&quot;,
    &quot;hello tokenizer&quot;,
    &quot;world of subwords&quot;,
]

training_output = quick_train_bundle(
    :wordpiece,
    training_corpus;
    vocab_size=64,
    min_frequency=1,
    sanity_text=&quot;hello world&quot;,
)

(
    bundle_directory=training_output.bundle_directory,
    bundle_files=training_output.bundle_files,
    sanity_encoded_ids=training_output.sanity_encoded_ids,
    sanity_decoded_text=training_output.sanity_decoded_text,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(bundle_directory = &quot;/tmp/jl_O6KUk4&quot;, bundle_files = [&quot;keemena_training_manifest.json&quot;, &quot;vocab.txt&quot;], sanity_encoded_ids = [57, 56], sanity_decoded_text = &quot;hello world&quot;)</code></pre><ul><li><strong>What you should see:</strong><ul><li><code>bundle_files</code> includes tokenizer exports and the training manifest.</li><li>Returned tokenizer has already been reloaded from bundle.</li><li>Workflow is deterministic for the same corpus and config.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>The bundle gives reproducible reload without remembering loader kwargs.</li><li><code>quick_train_bundle</code> currently supports <code>:wordpiece</code> plus HF preset trainer symbols.</li><li>Avoid asserting exact token ids across different configs.</li></ul></li><li><strong>Next:</strong> full API and preset coverage are in <a href="../training/">Training (experimental)</a>.</li></ul><h4 id="What-quick_train_bundle-does-under-the-hood"><a class="docs-heading-anchor" href="#What-quick_train_bundle-does-under-the-hood">What <code>quick_train_bundle</code> does under the hood</a><a id="What-quick_train_bundle-does-under-the-hood-1"></a><a class="docs-heading-anchor-permalink" href="#What-quick_train_bundle-does-under-the-hood" title="Permalink"></a></h4><pre><code class="language-julia hljs">using KeemenaSubwords

training_corpus = [&quot;hello world&quot;, &quot;hello tokenizer&quot;, &quot;world tokenizer&quot;]
training_result = train_wordpiece_result(training_corpus; vocab_size=48, min_frequency=1)

bundle_directory = mktempdir()
save_training_bundle(training_result, bundle_directory; overwrite=true)
reloaded_tokenizer = load_training_bundle(bundle_directory)

encoded_ids = encode(reloaded_tokenizer, &quot;hello world&quot;; add_special_tokens=false)
decoded_text = decode(reloaded_tokenizer, encoded_ids)

(
    bundle_files=sort(readdir(bundle_directory)),
    encoded_ids=encoded_ids,
    decoded_text=decoded_text,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(bundle_files = [&quot;keemena_training_manifest.json&quot;, &quot;vocab.txt&quot;], encoded_ids = [44, 45], decoded_text = &quot;hello world&quot;)</code></pre><h3 id="T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json"><a class="docs-heading-anchor" href="#T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json">T2: Train HF BERT WordPiece preset and export tokenizer.json</a><a id="T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json-1"></a><a class="docs-heading-anchor-permalink" href="#T2:-Train-HF-BERT-WordPiece-preset-and-export-tokenizer.json" title="Permalink"></a></h3><ul><li><strong>You have:</strong> text suited to BERT-style tokenization behavior.</li><li><strong>You want:</strong> a BERT preset tokenizer and optional HF export.</li><li><strong>Objective:</strong> use familiar BERT normalization and pretokenization defaults.</li><li><strong>Steps:</strong><ol><li>Call <code>train_hf_bert_wordpiece(corpus; ...)</code>.</li><li>Export with <code>export_tokenizer(...; format=:hf_tokenizer_json)</code>.</li></ol></li></ul><pre><code class="language-julia hljs"># non-executable training preset sketch
training_corpus = [&quot;Hello, world!&quot;, &quot;Tokenizer training example&quot;]
tokenizer = train_hf_bert_wordpiece(training_corpus; vocab_size=128, min_frequency=1)
export_tokenizer(tokenizer, &quot;out_hf_bert&quot;; format=:hf_tokenizer_json)</code></pre><ul><li><strong>What you should see:</strong><ul><li>Training returns a WordPiece tokenizer configured for BERT-style behavior.</li><li>Export creates <code>tokenizer.json</code> for HF interop.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>Presets are convenient defaults, not strict replication of every upstream variant.</li></ul></li><li><strong>Next:</strong> see <a href="../training/">Training (experimental)</a> and <a href="../formats/">Tokenizer Formats and Required Files</a>.</li></ul><h3 id="T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset"><a class="docs-heading-anchor" href="#T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset">T3: Train HF RoBERTa or GPT-2 ByteBPE preset</a><a id="T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset-1"></a><a class="docs-heading-anchor-permalink" href="#T3:-Train-HF-RoBERTa-or-GPT-2-ByteBPE-preset" title="Permalink"></a></h3><ul><li><strong>You have:</strong> corpus data for byte-level subword training.</li><li><strong>You want:</strong> a byte-level preset tokenizer in RoBERTa or GPT-2 style.</li><li><strong>Objective:</strong> use byte-level presets for robust coverage of arbitrary UTF-8 input.</li><li><strong>Steps:</strong><ol><li>Call <code>train_hf_roberta_bytebpe(...)</code> or <code>train_hf_gpt2_bytebpe(...)</code>.</li><li>Export with <code>export_tokenizer(...; format=:hf_tokenizer_json)</code> if needed.</li></ol></li></ul><pre><code class="language-julia hljs"># non-executable training preset sketch
training_corpus = [&quot;hello world&quot;, &quot;cafe costs 5&quot;]
tokenizer = train_hf_roberta_bytebpe(training_corpus; vocab_size=384, min_frequency=1)
export_tokenizer(tokenizer, &quot;out_hf_roberta&quot;; format=:hf_tokenizer_json)</code></pre><ul><li><strong>What you should see:</strong><ul><li>Training returns a byte-level tokenizer preset.</li><li>Exported artifacts are reloadable in Julia and usable in compatible external tools.</li></ul></li><li><strong>Concerns and setup notes:</strong><ul><li>Byte-level offsets still follow the package contract.</li><li>Span boundaries may not always be safe Julia string boundaries on multibyte text.</li></ul></li><li><strong>Next:</strong> offset rules are in <a href="../normalization_offsets_contract/">Normalization and Offsets Contract</a> and examples are in <a href="../offset_alignment_examples/">Offsets Alignment Examples</a>.</li></ul><h2 id="Other-options-(short-list)"><a class="docs-heading-anchor" href="#Other-options-(short-list)">Other options (short list)</a><a id="Other-options-(short-list)-1"></a><a class="docs-heading-anchor-permalink" href="#Other-options-(short-list)" title="Permalink"></a></h2><ul><li>Cache tokenizers for repeated use with <code>get_tokenizer_cached(...)</code> and clear cache with <code>clear_tokenizer_cache!()</code>.</li><li>Use explicit loaders when file contracts are known, for example <code>load_bpe_gpt2</code>, <code>load_sentencepiece</code>, and <code>load_tiktoken</code>.</li><li>Convert to 0-based ids only when an external consumer requires it: <code>ids_zero_based = token_ids .- 1</code>.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../concepts/">« Concepts</a><a class="docs-footer-nextpage" href="../structured_outputs_and_batching/">Structured Outputs and Batching »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 19 February 2026 05:33">Thursday 19 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
