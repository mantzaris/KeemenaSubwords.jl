module KeemenaSubwords

include("types.jl")
include("vocab.jl")
include("normalization.jl")
include("models.jl")
include("bpe.jl")
include("bytebpe.jl")
include("wordpiece.jl")
include("unigram.jl")
include("sentencepiece.jl")
include("tiktoken.jl")
include("huggingface_json/hf_json_types.jl")
include("huggingface_json/hf_json_parse.jl")
include("huggingface_json/hf_json_pipeline.jl")
include("huggingface_json/hf_json_loader.jl")
include("training.jl")
include("bpe_train.jl")
include("unigram_train.jl")
include("io.jl")

export AbstractSubwordTokenizer,
       TokenizerMetadata,
       TokenizationResult,
       SubwordVocabulary,
       FilesSpec,
       BPETokenizer,
       ByteBPETokenizer,
       WordPieceTokenizer,
       UnigramTokenizer,
       SentencePieceTokenizer,
       TiktokenTokenizer,
       HuggingFaceJSONTokenizer,
       keemena_callable,
       level_key,
       available_models,
       describe_model,
       model_path,
       prefetch_models,
       prefetch_models_status,
       asset_status,
       print_asset_status,
       download_hf_files,
       install_model!,
       install_llama2_tokenizer!,
       install_llama3_8b_tokenizer!,
       register_external_model!,
       register_local_model!,
       recommended_defaults_for_llms,
       detect_tokenizer_files,
       detect_tokenizer_format,
       load_bpe,
       load_bytebpe,
       load_bpe_gpt2,
       load_bpe_encoder,
       load_unigram,
       load_wordpiece,
       load_sentencepiece,
       load_tiktoken,
       load_hf_tokenizer_json,
       load_tokenizer,
       get_tokenizer_cached,
       clear_tokenizer_cache!,
       cached_tokenizers,
       tokenize,
       encode,
       encode_result,
       encode_batch_result,
       decode,
       token_to_id,
       id_to_token,
       vocab_size,
       special_tokens,
       model_info,
       unk_id,
       pad_id,
       bos_id,
       eos_id,
       normalize,
       normalize_text,
       tokenization_view,
       requires_tokenizer_normalization,
       offsets_coordinate_system,
       offsets_index_base,
       offsets_span_style,
       offsets_sentinel,
       has_span,
       has_nonempty_span,
       span_ncodeunits,
       span_codeunits,
       is_valid_string_boundary,
       try_span_substring,
       offsets_are_nonoverlapping,
       train_bpe,
       train_unigram,
       train_wordpiece,
       save_tokenizer,
       export_tokenizer

end # module
