module KeemenaSubwords

include("types.jl")
include("vocab.jl")
include("normalization.jl")
include("models.jl")
include("bpe.jl")
include("bytebpe.jl")
include("wordpiece.jl")
include("unigram.jl")
include("sentencepiece.jl")
include("tiktoken.jl")
include("huggingface_json/hf_json_types.jl")
include("huggingface_json/hf_json_parse.jl")
include("huggingface_json/hf_json_pipeline.jl")
include("huggingface_json/hf_json_loader.jl")
include("huggingface_json/hf_json_export.jl")
include("training/Training.jl")
using .Training: train_bpe,
    train_bpe_result,
    train_bytebpe,
    train_bytebpe_result,
    train_unigram,
    train_unigram_result,
    train_wordpiece,
    train_wordpiece_result,
    train_sentencepiece,
    train_sentencepiece_result,
    train_hf_bert_wordpiece,
    train_hf_bert_wordpiece_result,
    train_hf_roberta_bytebpe,
    train_hf_roberta_bytebpe_result,
    train_hf_gpt2_bytebpe,
    train_hf_gpt2_bytebpe_result,
    write_training_manifest,
    read_training_manifest,
    save_training_bundle,
    load_training_bundle
include("io.jl")
include("orchestration.jl")

export AbstractSubwordTokenizer,
       TokenizerMetadata,
       TokenizationResult,
       SubwordVocabulary,
       FilesSpec,
       BPETokenizer,
       ByteBPETokenizer,
       WordPieceTokenizer,
       UnigramTokenizer,
       SentencePieceTokenizer,
       TiktokenTokenizer,
       HuggingFaceJSONTokenizer,
       keemena_callable,
       level_key,
       available_models,
       describe_model,
       model_path,
       prefetch_models,
       prefetch_models_status,
       asset_status,
       print_asset_status,
       download_hf_files,
       install_model!,
       install_llama2_tokenizer!,
       install_llama3_8b_tokenizer!,
       register_external_model!,
       register_local_model!,
       recommended_defaults_for_llms,
       detect_tokenizer_files,
       detect_tokenizer_format,
       load_bpe,
       load_bytebpe,
       load_bpe_gpt2,
       load_bpe_encoder,
       load_unigram,
       load_wordpiece,
       load_sentencepiece,
       load_tiktoken,
       load_hf_tokenizer_json,
       load_tokenizer,
       get_tokenizer_cached,
       clear_tokenizer_cache!,
       cached_tokenizers,
       tokenize,
       encode,
       encode_result,
       encode_batch_result,
       quick_tokenize,
       quick_encode_batch,
       collate_padded_batch,
       causal_lm_labels,
       quick_causal_lm_batch,
       quick_train_bundle,
       decode,
       token_to_id,
       id_to_token,
       vocab_size,
       special_tokens,
       model_info,
       unk_id,
       pad_id,
       bos_id,
       eos_id,
       normalize,
       normalize_text,
       tokenization_view,
       requires_tokenizer_normalization,
       offsets_coordinate_system,
       offsets_index_base,
       offsets_span_style,
       offsets_sentinel,
       has_span,
       has_nonempty_span,
       span_ncodeunits,
       span_codeunits,
       is_valid_string_boundary,
       try_span_substring,
       offsets_are_nonoverlapping,
       validate_offsets_contract,
       assert_offsets_contract,
       train_bpe,
       train_bpe_result,
       train_bytebpe,
       train_bytebpe_result,
       train_unigram,
       train_unigram_result,
       train_wordpiece,
       train_wordpiece_result,
       train_sentencepiece,
       train_sentencepiece_result,
       train_hf_bert_wordpiece,
       train_hf_bert_wordpiece_result,
       train_hf_roberta_bytebpe,
       train_hf_roberta_bytebpe_result,
       train_hf_gpt2_bytebpe,
       train_hf_gpt2_bytebpe_result,
       write_training_manifest,
       read_training_manifest,
       save_training_bundle,
       load_training_bundle,
       save_tokenizer,
       export_tokenizer

end # module
