version = 1
description = "Shared edge-case corpus for tokenizer behavior and E2E workflow coverage."

[categories]
ascii = [
  "hello",
  "hello world",
  "The quick brown fox jumps over the lazy dog.",
  "tokenization baseline",
  "numbers 12345",
  "punctuation !?,.;:-",
  "parentheses (a+b)",
  "quotes \"double\" and 'single'",
  "url https://example.com/path?q=1",
  "email test@example.com",
  "snake_case_identifier",
  "kebab-case-example",
  "camelCaseInput",
  "JSON-like {\"a\":1,\"b\":2}",
  "math 1+1=2",
  "C:\\Users\\name\\file.txt",
  "/usr/local/bin/run",
  "version v1.2.3",
  "0xDEADBEEF",
  "EdgeCase42",
]

whitespace = [
  " leading space",
  "trailing space ",
  "  both sides  ",
  "multiple   internal   spaces",
  "\tleading tab",
  "trailing tab\t",
  "tab\tseparated\tvalues",
  "line1\nline2",
  "line1\n\nline3",
  "carriage\rreturn",
  "windows\r\nline",
  "mix \t of \n whitespace",
  "    ",
  "\t\t",
  "\n",
  " \n\t ",
  "word\tword",
  "word\nword",
  "word \n word",
  "a\tb\nc",
]

unicode = [
  "cafÃ©",
  "cafe\u0301",
  "maÃ±ana",
  "naÃ¯ve",
  "coÃ¶perate",
  "rÃ©sumÃ©",
  "Ã©lÃ¨ve",
  "Ã…ngstrÃ¶m",
  "straÃŸe",
  "Ä°stanbul",
  "ÅÃ³dÅº",
  "smÃ¸rrebrÃ¸d",
  "FranÃ§ois",
  "niÃ±o",
  "SÃ£o Paulo",
  "ï¬ ligature",
  "â‘  â‘¡ â‘¢",
  "â…£ vs IV",
  "a\u0301 e\u0301 i\u0301 o\u0301 u\u0301",
  "ZÍ‘aÍ’lÍƒgÍ„oÍ‚ text",
]

emoji_symbols = [
  "emoji ğŸ˜€",
  "emoji ğŸ˜ğŸ˜‚ğŸ¤£",
  "thumbs ğŸ‘",
  "skin tone ğŸ‘ğŸ½",
  "family ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦",
  "flags ğŸ‡ºğŸ‡¸ğŸ‡¬ğŸ‡·",
  "rocket ğŸš€âœ¨",
  "heart â¤ï¸",
  "variation selector âœŠğŸ½",
  "keycap 1ï¸âƒ£ 2ï¸âƒ£",
  "symbols â„¢Â©Â®",
  "math âˆ‘âˆ«â‰ˆâ‰ ",
  "currency $ â‚¬ Â¥ â‚¹",
  "arrows â† â†’ â†”",
  "box drawing â”€â”‚â”Œâ”",
  "music â™ªâ™«",
  "check âœ“âœ—",
  "snowman â˜ƒ",
  "chess â™â™œ",
  "infinity âˆ",
]

mixed_scripts = [
  "English Î•Î»Î»Î·Î½Î¹ÎºÎ¬ Ğ ÑƒÑÑĞºĞ¸Ğ¹",
  "Latin ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğ° æ¼¢å­—",
  "abc ××‘×’ 123",
  "í•œêµ­ì–´ and English",
  "æ—¥æœ¬èª mixed with English",
  "ä¸­æ–‡ mixed with punctuation!",
  "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ world",
  "Î³ÎµÎ¹Î¬ ÏƒÎ¿Ï… world",
  "Ù…Ø±Ø­Ø¨Ø§ world",
  "à¤¹à¤¿à¤¨à¥à¤¦à¥€ English",
  "à®¤à®®à®¿à®´à¯ English",
  "à¦¬à¦¾à¦‚à¦²à¦¾ text",
  "à¹„à¸—à¸¢ text",
  "×¢×‘×¨×™×ª English",
  "emoji ğŸ˜€ ä¸­æ–‡ Ñ€ÑƒÑÑĞºĞ¸Ğ¹",
]

rtl = [
  "Ù…Ø±Ø­Ø¨Ø§",
  "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…",
  "×©×œ×•×",
  "×©×œ×•× ×¢×•×œ×",
  "×¢×‘×¨×™×ª ××™××™×Ÿ ×œ×©×××œ",
  "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±",
  "Ù…Ø±Ø­Ø¨Ø§ hello",
  "hello ×©×œ×•×",
  "123 Ù…Ø±Ø­Ø¨Ø§ 456",
  "[CLS] ×©×œ×•× [SEP]",
]

special_tokens = [
  "[CLS] hello [SEP]",
  "<s> hello </s>",
  "<pad>token</pad>",
  "[MASK] token",
  "[UNK]",
  "<unk>",
  "<CITY>",
  "<city>",
  "<city_center>",
  "[SPECIAL] i",
  "<|assistant|> Hello",
  "<|user|> hi",
  "<|endoftext|>",
  "tokenizer.model",
  "tokenizer.json",
]

[generated]
long_seed = "keemena-subwords-long-input-0123456789 "
long_lengths = [8192, 16384, 32768]
